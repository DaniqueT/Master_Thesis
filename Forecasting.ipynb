{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff80defa",
   "metadata": {},
   "source": [
    "ARIMA model (including tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found. Using zeros as starting parameters.\")\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pmdarima.arima import auto_arima\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import itertools\n",
    "import ast\n",
    "import time\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "\n",
    "    train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "    val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "    test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "    newtrain_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    newtrain_df = newtrain_df.copy()\n",
    "\n",
    "    for lag in [24, 48, 72, 168]:\n",
    "        train_df[f'price_lag_{lag}'] = train_df['Price'].shift(lag)\n",
    "        val_df[f'price_lag_{lag}'] = val_df['Price'].shift(lag)\n",
    "        test_df[f'price_lag_{lag}'] = test_df['Price'].shift(lag)\n",
    "        newtrain_df[f'price_lag_{lag}'] = newtrain_df['Price'].shift(lag)\n",
    "\n",
    "    train_df.dropna(inplace=True)\n",
    "    val_df.dropna(inplace=True)\n",
    "    test_df.dropna(inplace=True)\n",
    "    newtrain_df.dropna(inplace=True)\n",
    "\n",
    "    return train_df, val_df, test_df, newtrain_df\n",
    "\n",
    "def fit_hourly_arima_models_24(train_df, order):\n",
    "    models = {}\n",
    "    for hour in tqdm(range(24), desc=\"Fitting ARIMA models for each hour of the day\"):\n",
    "        df_filtered = train_df[train_df['start_datetime'].dt.hour == hour].copy()\n",
    "        df_filtered = df_filtered.set_index('start_datetime').asfreq('D')\n",
    "        df_filtered.dropna(inplace=True)\n",
    "\n",
    "        model = ARIMA(df_filtered['Price'], order=order, enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "        models[hour] = model\n",
    "    return models\n",
    "\n",
    "def forecast_next_24(test_df, test_days, models, Forecast_horizon):\n",
    "    steps_ahead = Forecast_horizon // 24 \n",
    "    test_df = test_df.copy()\n",
    "    test_df['date'] = test_df['start_datetime'].dt.normalize()\n",
    "    test_df['hour'] = test_df['start_datetime'].dt.hour\n",
    "\n",
    "    forecasts, true_vals, start_times = [], [], []\n",
    "    actual_by_hour = {h: test_df[test_df['hour'] == h].set_index('start_datetime')['Price'] for h in range(24)}\n",
    "\n",
    "    for d in tqdm(test_days, desc=f\"ARIMA Forecasting {Forecast_horizon}h with 24 hourly models\"):\n",
    "        forecast = np.empty(Forecast_horizon)\n",
    "        forecast[:] = np.nan\n",
    "\n",
    "        day_to_append_start = d\n",
    "        day_to_append_end = d + timedelta(days=1)\n",
    "\n",
    "        for hour in range(24):\n",
    "            model_fit = models[hour]\n",
    "\n",
    "            forecast_vals = model_fit.forecast(steps=steps_ahead)\n",
    "\n",
    "            for step in range(steps_ahead):\n",
    "                idx = step * 24 + hour\n",
    "                if idx < Forecast_horizon:\n",
    "                    forecast[idx] = forecast_vals.iloc[step]\n",
    "\n",
    "    \n",
    "            actuals_for_day_d = actual_by_hour[hour].loc[\n",
    "                (actual_by_hour[hour].index >= day_to_append_start) &\n",
    "                (actual_by_hour[hour].index < day_to_append_end)\n",
    "            ]\n",
    "\n",
    "            if not actuals_for_day_d.empty:\n",
    "                model_fit = model_fit.append(actuals_for_day_d.values)\n",
    "                models[hour] = model_fit\n",
    "    \n",
    "        d_start = d\n",
    "        d_end = d + timedelta(hours = Forecast_horizon-1)\n",
    "        mask = (test_df['start_datetime'] >= d_start) & (test_df['start_datetime'] <= d_end)\n",
    "        actual_prices = test_df[mask]['Price'].values\n",
    "\n",
    "        if len(actual_prices) == Forecast_horizon:\n",
    "            forecasts.append(forecast)\n",
    "            true_vals.append(actual_prices)\n",
    "            start_times.append(d)\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in forecast length for day {d}. Expected {Forecast_horizon}, got {len(actual_prices)}\")\n",
    "\n",
    "    return np.array(forecasts), np.array(true_vals), start_times\n",
    "\n",
    "\n",
    "train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "train_dates = train_dates['date'].tolist()\n",
    "val_dates = val_dates['date'].tolist()\n",
    "test_dates = test_dates['date'].tolist()\n",
    "\n",
    "FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "train_df, val_df, test_df, newtrain_df = load_data(FILE_PATH)\n",
    "\n",
    "def tune_arima():\n",
    "    results = []\n",
    "    for forecast_horizon in [48, 144]:\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nTuning ARIMA for forecast horizon: {forecast_horizon} hours\")\n",
    "        p_values = [1, 2, 3, 4]\n",
    "        d_values = [1]\n",
    "        q_values = [1, 2, 3, 4]\n",
    "\n",
    "        best_rmse = float(\"inf\")\n",
    "        best_order = None\n",
    "\n",
    "        i = 0\n",
    "        total_models = len(p_values) * len(d_values) * len(q_values)\n",
    "        for p in p_values:\n",
    "            for d in d_values:\n",
    "                for q in q_values:\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        order = (p, d, q)\n",
    "                        print(f\"\\nTrying ARIMA({p},{d},{q}). Model {i} of {total_models}\")\n",
    "                        models = fit_hourly_arima_models_24(train_df, order)\n",
    "                        forecasts, true_vals, _ = forecast_next_24(val_df, val_dates, models, forecast_horizon)\n",
    "                        rmse = np.sqrt(mean_squared_error(true_vals, forecasts))\n",
    "                        print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "            \n",
    "                        if rmse < best_rmse:\n",
    "                            best_rmse = rmse\n",
    "                            best_order = (p, d, q)\n",
    "                            print(f\"New best ARIMA order: {best_order} RMSE: {best_rmse:.2f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"ARIMA({p},{d},{q}) failed: {e}\")\n",
    "\n",
    "        print(f\"\\n✅ Best ARIMA order: {best_order} with RMSE: {best_rmse:.2f} €/MWh\")\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        results.append((forecast_horizon, best_order, best_rmse, run_time))\n",
    "    return results\n",
    "\n",
    "#To tune the ARIMA models, uncomment the following lines:\n",
    "#results = tune_arima()\n",
    "#results_df = pd.DataFrame(results, columns=['Forecast Horizon', 'Best Order', 'Best RMSE', 'Run Time'])\n",
    "#results_df.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning_new/arima_tuning_results_new.csv', index=False)\n",
    "\n",
    "\n",
    "for Forecast_horizon in [48, 144]:\n",
    "    tuning = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/arima_tuning_results.csv', sep=',')\n",
    "    order = tuning[tuning['Forecast Horizon'] == Forecast_horizon]['Best Order'].values[0]\n",
    "    order = ast.literal_eval(order)\n",
    "    print(f\"Generating final forecasts for Forecast Horizon: {Forecast_horizon} and ARIMA order: {order}\")\n",
    "    models = fit_hourly_arima_models_24(newtrain_df, order)\n",
    "    final_forecasts, final_true_vals, _ = forecast_next_24(test_df, test_dates, models, Forecast_horizon)\n",
    "    test_rmse = np.sqrt(mean_squared_error(final_true_vals, final_forecasts))\n",
    "    print(f\"\\n Final Test RMSE using best ARIMA: {test_rmse:.2f} €/MWh\")\n",
    "    final_forecasts_df = pd.DataFrame(final_forecasts, columns=[f'h+{i}' for i in range(Forecast_horizon)])\n",
    "    final_true_vals_df = pd.DataFrame(final_true_vals, columns=[f'h+{i}' for i in range(Forecast_horizon)])\n",
    "    final_forecasts_df.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/Arima_forecasts_{Forecast_horizon}.csv', index=False)\n",
    "    final_true_vals_df.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Models_true/Arima_true_vals_{Forecast_horizon}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61439d4f",
   "metadata": {},
   "source": [
    "GARCH model (Including tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf42150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Non-invertible starting MA parameters found. Using zeros as starting parameters.\")\n",
    "from arch import arch_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pmdarima.arima import auto_arima\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "import time\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "\n",
    "    train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "    val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "    test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "    newtrain_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    newtrain_df = newtrain_df.copy()\n",
    "\n",
    "    for lag in [24, 48, 72, 168]:\n",
    "        train_df[f'price_lag_{lag}'] = train_df['Price'].shift(lag)\n",
    "        val_df[f'price_lag_{lag}'] = val_df['Price'].shift(lag)\n",
    "        test_df[f'price_lag_{lag}'] = test_df['Price'].shift(lag)\n",
    "        newtrain_df[f'price_lag_{lag}'] = newtrain_df['Price'].shift(lag)\n",
    "\n",
    "    train_df.dropna(inplace=True)\n",
    "    val_df.dropna(inplace=True)\n",
    "    test_df.dropna(inplace=True)\n",
    "    newtrain_df.dropna(inplace=True)\n",
    "\n",
    "    return train_df, val_df, test_df, newtrain_df\n",
    "\n",
    "def fit_hourly_garch_models_24(train_df, order):\n",
    "    models = {}\n",
    "    for hour in tqdm(range(24), desc=\"Fitting GARCH models for each hour of the day\"):\n",
    "        df_filtered = train_df[train_df['start_datetime'].dt.hour == hour].copy()\n",
    "        df_filtered = df_filtered.set_index('start_datetime').asfreq('D')\n",
    "        df_filtered.dropna(inplace=True)\n",
    "        p, o, q = order\n",
    "        #result = adfuller(df_filtered['Price'])\n",
    "        #print(f\"Hour {hour}: ADF p-value = {result[1]:.4f}\")\n",
    "\n",
    "        model = arch_model(df_filtered['Price'], vol='GARCH', p=p, o=o, q=q, mean='AR', lags=1, dist='normal')\n",
    "        res = model.fit(disp='off')\n",
    "        models[hour] = (res, df_filtered['Price'])\n",
    "    return models\n",
    "\n",
    "def forecast_next_24(test_df, test_days, models, order, Forecast_horizon):\n",
    "    p, o, q = order\n",
    "    steps_ahead = Forecast_horizon // 24 \n",
    "    test_df = test_df.copy()\n",
    "    test_df['date'] = test_df['start_datetime'].dt.normalize()\n",
    "    test_df['hour'] = test_df['start_datetime'].dt.hour\n",
    "\n",
    "    forecasts, true_vals, start_times = [], [], []\n",
    "    actual_by_hour = {h: test_df[test_df['hour'] == h].set_index('start_datetime')['Price'] for h in range(24)}\n",
    "\n",
    "    for d in tqdm(test_days, desc=f\"GARCH Forecasting {Forecast_horizon}h with 24 hourly models\"):\n",
    "    \n",
    "        forecast = np.empty(Forecast_horizon)\n",
    "        forecast[:] = np.nan\n",
    "\n",
    "        day_to_append_start = d\n",
    "        day_to_append_end = d + timedelta(days=1)\n",
    "\n",
    "        for hour in range(24):\n",
    "            model_fit, data = models[hour]\n",
    "            forecast_all = model_fit.forecast(horizon=steps_ahead)\n",
    "            forecast_vals = forecast_all.mean.values[-1]\n",
    "\n",
    "            for step in range(steps_ahead):\n",
    "                idx = step * 24 + hour\n",
    "                if idx < Forecast_horizon:\n",
    "                    forecast[idx] = forecast_vals[step]\n",
    "\n",
    "        \n",
    "            actuals_for_day_d = actual_by_hour[hour].loc[\n",
    "                (actual_by_hour[hour].index >= day_to_append_start) &\n",
    "                (actual_by_hour[hour].index < day_to_append_end)\n",
    "            ]\n",
    "\n",
    "            if not actuals_for_day_d.empty:\n",
    "                data = np.concatenate([data, actuals_for_day_d.values])\n",
    "                new_model = arch_model(data, vol='GARCH', p=p, o=o, q=q, mean='AR', lags=1, dist='normal')\n",
    "                model_fit = new_model.fit(disp='off')\n",
    "                models[hour] = (model_fit, data)\n",
    "\n",
    "        d_start = d\n",
    "        d_end = d + timedelta(hours = Forecast_horizon-1)\n",
    "        mask = (test_df['start_datetime'] >= d_start) & (test_df['start_datetime'] <= d_end)\n",
    "        actual_prices = test_df[mask]['Price'].values\n",
    "\n",
    "        if len(actual_prices) == Forecast_horizon:\n",
    "            forecasts.append(forecast)\n",
    "            true_vals.append(actual_prices)\n",
    "            start_times.append(d)\n",
    "        else:\n",
    "            print(f\"Warning: Mismatch in forecast length for day {d}. Expected {Forecast_horizon}, got {len(actual_prices)}\")\n",
    "\n",
    "    return np.array(forecasts), np.array(true_vals), start_times\n",
    "\n",
    "\n",
    "train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "train_dates = train_dates['date'].tolist()\n",
    "val_dates = val_dates['date'].tolist()\n",
    "test_dates = test_dates['date'].tolist()\n",
    "\n",
    "FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined.csv'\n",
    "train_df, val_df, test_df, newtrain_df = load_data(FILE_PATH)\n",
    "\n",
    "def tune_GARCH():\n",
    "    results = []\n",
    "    for forecast_horizon in [48, 144]:\n",
    "        start_time = time.time()\n",
    "        print(f\"\\nTuning GARCH for forecast horizon: {forecast_horizon} hours\")\n",
    "\n",
    "        p_values = [1,2,3]\n",
    "        #For regular GARCH tuning, o = 0\n",
    "        o_values = [1,2]\n",
    "        q_values = [1, 2,3]\n",
    "\n",
    "        best_rmse = float(\"inf\")\n",
    "        best_order = None\n",
    "\n",
    "        i = 0\n",
    "        total_models = len(p_values) * len(o_values) * len(q_values)\n",
    "        for p in p_values:\n",
    "            for o in o_values:\n",
    "                for q in q_values:\n",
    "                    i += 1\n",
    "                    try:\n",
    "                        order = (p, o, q)\n",
    "                        print(f\"\\nTrying GARCH({p},{o},{q}). Model {i} of {total_models}\")\n",
    "                        models = fit_hourly_garch_models_24(train_df, order)\n",
    "                        forecasts, true_vals, _ = forecast_next_24(val_df, val_dates, models, order, forecast_horizon)\n",
    "                        rmse = np.sqrt(mean_squared_error(true_vals, forecasts))\n",
    "                        print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "\n",
    "                        if rmse < best_rmse:\n",
    "                            best_rmse = rmse\n",
    "                            best_order = (p, o, q)\n",
    "                            print(f\"New best GARCH order: {best_order} RMSE: {best_rmse:.2f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"GARCH({p},{o},{q}) failed: {e}\")\n",
    "\n",
    "        print(f\"\\n✅ Best GARCH order: {best_order} with RMSE: {best_rmse:.2f} €/MWh\")\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        results.append((forecast_horizon, best_order, best_rmse, run_time))\n",
    "    return results\n",
    "\n",
    "# To tune the GARCH models, uncomment the following lines:\n",
    "#results = tune_GARCH()\n",
    "#results_df = pd.DataFrame(results, columns=['Forecast Horizon', 'Best Order', 'Best RMSE', 'Run time'])\n",
    "#results_df.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/garch_asymmetric_tuning_results.csv', index=False)\n",
    "\n",
    "for Forecast_horizon in [48, 144]:\n",
    "    tuning = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/garch_regular_tuning_results.csv', sep=',')\n",
    "    order = tuning[tuning['Forecast Horizon'] == Forecast_horizon]['Best Order'].values[0]\n",
    "    order = ast.literal_eval(order)\n",
    "    print(f\"Generating final forecasts for Forecast Horizon: {Forecast_horizon} and GARCH order: {order}\")\n",
    "    models = fit_hourly_garch_models_24(newtrain_df, order)\n",
    "    final_forecasts, final_true_vals, _ = forecast_next_24(test_df, test_dates, models, order, Forecast_horizon)\n",
    "    test_rmse = np.sqrt(mean_squared_error(final_true_vals, final_forecasts))\n",
    "    print(f\"\\n Final Test RMSE using best GARCH: {test_rmse:.2f} €/MWh\")\n",
    "    final_forecasts_df = pd.DataFrame(final_forecasts, columns=[f'h+{i}' for i in range(Forecast_horizon)])\n",
    "    final_true_vals_df = pd.DataFrame(final_true_vals, columns=[f'h+{i}' for i in range(Forecast_horizon)])\n",
    "    final_forecasts_df.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/garch_regular_forecasts_{Forecast_horizon}.csv', index=False)\n",
    "    final_true_vals_df.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Models_true/garch_regular_true_vals_{Forecast_horizon}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa718b82",
   "metadata": {},
   "source": [
    "Tuning LSTM, LGBM & XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import time\n",
    "import random\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "MODEL_TYPE = \"LSTM\"   # \"XGB\" or \"LGBM\" or \"LSTM\"\n",
    "H1 = 24                \n",
    "H2 = 24\n",
    "H3 = 120\n",
    "FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "OUTPUT_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Pred_{MODEL_TYPE}_{Forecast_Horizon}.csv'\n",
    "TRUTH_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/True_{MODEL_TYPE}_{Forecast_Horizon}.csv'\n",
    "load_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/load_error.csv')\n",
    "Solar_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv')\n",
    "WindShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv')\n",
    "WindOffShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv')\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "  \n",
    "    df['hour'] = df['start_datetime'].dt.hour\n",
    "    df['dayofweek'] = df['start_datetime'].dt.dayofweek\n",
    "    df['month'] = df['start_datetime'].dt.month\n",
    "    df['day']= df['start_datetime'].dt.date\n",
    "\n",
    "    train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "    val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "    test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "\n",
    "    categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "    features_to_scale = [col for col in train_df.columns if col not in categorical_features]\n",
    "\n",
    "    if MODEL_TYPE == 'LSTM':\n",
    "                \n",
    "        train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "        val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "        test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "        categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "        features_to_scale = [col for col in train_df.columns if col not in categorical_features]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        train_scaled = pd.DataFrame(scaler.fit_transform(train_df[features_to_scale]), index=train_df.index, columns=features_to_scale)\n",
    "        val_scaled   = pd.DataFrame(scaler.transform(val_df[features_to_scale]), index=val_df.index, columns=features_to_scale)\n",
    "        test_scaled  = pd.DataFrame(scaler.transform(test_df[features_to_scale]), index=test_df.index, columns=features_to_scale)\n",
    "\n",
    "        price_scaler = StandardScaler().fit(train_df[['Price']])\n",
    "        for df_scaled, df_orig in zip([train_scaled, val_scaled, test_scaled], [train_df, val_df, test_df]):\n",
    "            df_scaled['Price'] = price_scaler.transform(df_orig[['Price']])\n",
    "            df_scaled[categorical_features] = df_orig[categorical_features]\n",
    "\n",
    "    else:\n",
    "        train_scaled = df[(df['start_datetime'].dt.year == 2023)]\n",
    "        val_scaled = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "        test_scaled  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "        categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "        \n",
    "        price_scaler = StandardScaler().fit(train_scaled[['Price']])\n",
    "\n",
    "    return train_scaled, val_scaled, test_scaled, price_scaler\n",
    "\n",
    "def make_lookup(df):\n",
    "    lookup = defaultdict(dict)\n",
    "    for _, row in df.iterrows():\n",
    "        key = (row['day'], row['hour'])\n",
    "        lookup[key] = row.to_dict()\n",
    "    return lookup\n",
    "\n",
    "class ElectricityDataset(Dataset):\n",
    "    def __init__(self, lookup, base_dates, horizon, add_noise, first24hours):\n",
    "        self.lookup = lookup\n",
    "        if not first24hours:\n",
    "            self.base_dates = [d + timedelta(days=1) for d in base_dates]\n",
    "        else:\n",
    "            self.base_dates = base_dates\n",
    "        self.horizon = horizon\n",
    "        self.add_noise = add_noise\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_day = self.base_dates[idx]\n",
    "        np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "        feats = []\n",
    "        y = []\n",
    "\n",
    "        for hour in range(self.horizon):\n",
    "            days_offset = hour // 24\n",
    "            h = hour % 24\n",
    "\n",
    "            d = (base_day + timedelta(days=days_offset)).date()\n",
    "            d1 = (base_day - timedelta(days=1)).date()\n",
    "            d7 = (d - timedelta(days=7))\n",
    "\n",
    "            p_d1 = self.lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "            p_d7 = self.lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "       \n",
    "            Z1 = self.lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "            Z2 = self.lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "            Z3 = self.lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "            Z4 = self.lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "            Z5 = self.lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "            Z6 = self.lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "\n",
    "            hour = self.lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "            dayofweek = self.lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "            month = self.lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "            x = [p_d1, p_d7, Z1, Z2, Z3, Z4,Z5, Z6, hour, dayofweek, month]\n",
    "            feats.append(x)\n",
    "\n",
    "            target = self.lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "            y.append(target)\n",
    "\n",
    "        feats = np.array(feats, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        if self.add_noise:\n",
    "            for start in range(0, self.horizon, 24):\n",
    "                i = start // 24\n",
    "                end = start + 24\n",
    "                np.random.seed(42)\n",
    "                feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "       \n",
    "        return torch.from_numpy(feats), torch.from_numpy(y)\n",
    "\n",
    "def build_noisy_windows(lookup, days, horizon, add_noise, first24hours):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    if not first24hours:\n",
    "        days = [d + timedelta(days=1) for d in days]\n",
    "        \n",
    "    for base_day in days:\n",
    "        np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "        feats = []\n",
    "        y = []\n",
    "\n",
    "        for hour in range(horizon):\n",
    "            days_offset = hour // 24\n",
    "            h = hour % 24\n",
    "\n",
    "            d = (base_day + timedelta(days=days_offset)).date()\n",
    "            d1 = (base_day - timedelta(days=1)).date()\n",
    "            d7 = (d - timedelta(days=7))\n",
    "\n",
    "            p_d1 = lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "            p_d7 = lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "\n",
    "            Z1 = lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "            Z2 = lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "            Z3 = lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "            Z4 = lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "            Z5 = lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "            Z6 = lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "\n",
    "            hour_val = lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "            dayofweek = lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "            month = lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "            x = [p_d1, p_d7, Z1, Z2, Z3, Z4, Z5, Z6, hour_val, dayofweek, month]\n",
    "            feats.append(x)\n",
    "\n",
    "            target = lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "            y.append(target)\n",
    "\n",
    "        feats = np.array(feats, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        if add_noise:\n",
    "            for start in range(0, horizon, 24):\n",
    "                i = start // 24\n",
    "                end = start + 24\n",
    "                feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "\n",
    "        Xs.append(feats.flatten()) \n",
    "        Ys.append(y)\n",
    "\n",
    "    return np.vstack(Xs), np.vstack(Ys)\n",
    " \n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, dropout, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout \n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        h = h_n[-1]\n",
    "        h = self.dropout(h)\n",
    "        return self.fc(h)\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_state = None \n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_state = model.state_dict()\n",
    "            return False\n",
    "        \n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_state = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "def train_and_evaluate_gbm(model_type, X_train, y_train, X_val, y_val, horizon, config):\n",
    "    preds = np.zeros((len(X_val), horizon))\n",
    "\n",
    "    for h in range(horizon):\n",
    "        if model_type == \"XGB\":\n",
    "            reg = XGBRegressor(\n",
    "                n_estimators=config['n_estimators'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                max_depth=config['max_depth'],\n",
    "                subsample=config['subsample'],\n",
    "                colsample_bytree=config['colsample_bytree'],\n",
    "                tree_method=\"hist\",\n",
    "                early_stopping_rounds=15,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                eval_metric='rmse'\n",
    "            )\n",
    "            reg.fit(X_train, y_train[:, h], eval_set=[(X_val, y_val[:, h])], verbose=False)\n",
    "        elif model_type == \"LGBM\":\n",
    "            reg = LGBMRegressor(\n",
    "                objective='regression',\n",
    "                n_estimators=config['n_estimators'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                max_depth=config['max_depth'],\n",
    "                subsample=config['subsample'],\n",
    "                colsample_bytree=config['colsample_bytree'],\n",
    "                early_stopping_rounds=15,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1,\n",
    "                eval_metric='rmse'\n",
    "            )\n",
    "            reg.fit(X_train, y_train[:, h].copy(), eval_set=[(X_val, y_val[:, h].copy())])\n",
    "        \n",
    "        preds[:, h] = reg.predict(X_val)\n",
    "\n",
    "    val_rmse = np.sqrt(mean_squared_error(\n",
    "        y_val,\n",
    "        preds\n",
    "    ))\n",
    "    \n",
    "    return val_rmse\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_and_tune(model_type, feature_cols,  horizon, isfirst24, **kwargs):\n",
    "    if model_type == \"LSTM\":\n",
    "        train_loader = kwargs[f'train_loader']\n",
    "        val_loader   = kwargs[f'val_loader']\n",
    "\n",
    "        param_grid = {\n",
    "        'lr': [1e-3, 5e-4, 1e-4],\n",
    "        'weight_decay': [1e-5, 1e-4, 1e-3],\n",
    "        'dropout': [0.1, 0.2, 0.4],\n",
    "        'num_layers': [1, 2, 3]\n",
    "        }\n",
    "\n",
    "        grid = list(product(*param_grid.values()))\n",
    "        param_names = list(param_grid.keys())\n",
    "     \n",
    "        if isfirst24:\n",
    "            print(\"Tuning for the first 24 hours\")\n",
    "        else:\n",
    "            print(f\"Tuning the remaining {horizon} hours\")\n",
    "        best_rmse = float('inf')\n",
    "        best_config = None\n",
    "        best_model_state = None\n",
    "        i = 0\n",
    "        for values in grid:\n",
    "            i += 1\n",
    "            rmses = []\n",
    "            print(f\"Training model {i} of {len(grid)}\")\n",
    "            for seed in tqdm([0, 1, 2], desc=\"Seeds\"):\n",
    "                set_seed(seed)\n",
    "                params = dict(zip(param_names, values))\n",
    "                model = LSTMForecast(\n",
    "                        input_size=len(feature_cols),\n",
    "                        output_size=horizon,\n",
    "                        num_layers=params['num_layers'],\n",
    "                        dropout = params['dropout'] if params['num_layers'] > 1 else 0.0\n",
    "                    )\n",
    "                loss_fn = nn.MSELoss()\n",
    "                opt = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3, min_lr=1e-5)\n",
    "                early_stopper = EarlyStopping(patience=3, min_delta=1e-4)\n",
    "\n",
    "                for epoch in range(1, 30):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            opt.zero_grad()\n",
    "                            y_pred = model(X_batch)\n",
    "                            loss = loss_fn(y_pred, y_batch)\n",
    "                            loss.backward()\n",
    "                            opt.step()\n",
    "                        model.eval()\n",
    "                        val_loss = 0.0\n",
    "                        with torch.no_grad():\n",
    "                            for X_batch, y_batch in val_loader:\n",
    "                                y_pred = model(X_batch)\n",
    "                                val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                        val_loss /= len(val_loader.dataset)\n",
    "                        scheduler.step(val_loss)\n",
    "                        if early_stopper(val_loss, model):\n",
    "                            break\n",
    "                model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "                model.eval()\n",
    "                preds, true = [], []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y in val_loader:\n",
    "                        preds.append(model(X_batch))\n",
    "                        true.append(y)\n",
    "                preds = torch.cat(preds).cpu().numpy()\n",
    "                true = torch.cat(true).cpu().numpy()\n",
    "                rmse = np.sqrt(mean_squared_error(price_scaler.inverse_transform(preds),\n",
    "                                                    price_scaler.inverse_transform(true)))\n",
    "                rmses.append(rmse)\n",
    "            avg_rmse = np.mean(rmses)\n",
    "            print(f\"Config {params} — Average RMSE: {avg_rmse:.4f}\")    \n",
    "\n",
    "            if avg_rmse < best_rmse:\n",
    "                best_rmse = avg_rmse\n",
    "                best_config = params\n",
    "                best_model_state = model.state_dict()\n",
    "                print(f\"New best configuration: {best_config} with RMSE: {best_rmse}\")\n",
    "\n",
    "        print(f\"Finished tuning with best RMSE: {best_rmse}, best_conf: {best_config}\")\n",
    "        print(\"\\n\")\n",
    "        return best_config, best_model_state           \n",
    "    else:\n",
    "        X_train = kwargs['X_train']\n",
    "        y_train = kwargs['y_train']\n",
    "        X_val   = kwargs['X_val']\n",
    "        y_val   = kwargs['y_val']\n",
    "        X_test  = kwargs['X_test']\n",
    "\n",
    "        if isfirst24:\n",
    "            print(\"Tuning for the first 24 hours\")\n",
    "        else:\n",
    "            print(f\"Tuning the remaining {horizon} hours\")\n",
    "\n",
    "        param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 6],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "        param_names = list(param_grid.keys())\n",
    "        grid = list(product(*param_grid.values()))\n",
    "        best_rmse = float(\"inf\")\n",
    "        best_config = None\n",
    "        i = 0\n",
    "        for values in grid:\n",
    "            i += 1\n",
    "            print(f\"Testing model {i} of {len(grid)}\")\n",
    "            config = dict(zip(param_names, values))\n",
    "            rmses = []\n",
    "            for seed in tqdm([0, 1, 2], desc=\"Seeds\"):\n",
    "                set_seed(seed)\n",
    "                rmse = train_and_evaluate_gbm(model_type, X_train, y_train, X_val, y_val, horizon, config)\n",
    "                rmses.append(rmse)\n",
    "            \n",
    "            avg_rmse = np.mean(rmses)\n",
    "            print(f\"Config {config} — RMSE: {avg_rmse:.4f}\")\n",
    "            if avg_rmse < best_rmse:\n",
    "                best_rmse = avg_rmse\n",
    "                best_config = config\n",
    "                print(f\"New best configuration: {best_config} with RMSE: {best_rmse}\")\n",
    "\n",
    "        print(f\"\\n Finished tuning with best config: {best_config} — RMSE: {best_rmse:.4f} \\n\")\n",
    "        return best_config\n",
    "\n",
    "\n",
    "train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "train_dates = train_dates['date'].tolist()\n",
    "val_dates = val_dates['date'].tolist()\n",
    "test_dates = test_dates['date'].tolist()\n",
    "\n",
    "feature_cols = ['price_d1','price_d7', 'Load', 'Solar', 'WindShore', 'WindOffShore', 'Net_Import', 'Coal_Price', 'hour', 'dayofweek', 'month']\n",
    "\n",
    "train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "lookup_train = make_lookup(train_scaled)\n",
    "lookup_val = make_lookup(val_scaled)\n",
    "lookup_test = make_lookup(test_scaled)\n",
    "\n",
    "if MODEL_TYPE == \"LSTM\":\n",
    "    ds_train_H1 = ElectricityDataset(lookup_train, train_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "    ds_val_H1   = ElectricityDataset( lookup_val, val_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "    ds_test_H1  = ElectricityDataset( lookup_test, test_dates,  horizon=H1, add_noise=True, first24hours = True)\n",
    "    ds_train_H2 = ElectricityDataset( lookup_train, train_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "    ds_val_H2   = ElectricityDataset( lookup_val,  val_dates,   horizon=H2, add_noise=True, first24hours = False)\n",
    "    ds_test_H2  = ElectricityDataset( lookup_test, test_dates,  horizon=H2, add_noise=True, first24hours = False)\n",
    "    ds_train_H3 = ElectricityDataset( lookup_train, train_dates, horizon=H3, add_noise=True, first24hours = False)\n",
    "    ds_val_H3   = ElectricityDataset( lookup_val,  val_dates,   horizon=H3, add_noise=True, first24hours = False)\n",
    "    ds_test_H3  = ElectricityDataset( lookup_test, test_dates,  horizon=H3, add_noise=True, first24hours = False)\n",
    "\n",
    "    train_loader_24 = DataLoader(ds_train_H1, batch_size=16, shuffle=True)\n",
    "    val_loader_24   = DataLoader(ds_val_H1,   batch_size=16, shuffle=False)\n",
    "    test_loader_24  = DataLoader(ds_test_H1,  batch_size=16, shuffle=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_config_24, best_model_state_24 = train_and_tune(\"LSTM\", feature_cols= feature_cols, horizon =H1, isfirst24=True,\n",
    "                               train_loader=train_loader_24,\n",
    "                               val_loader=val_loader_24,\n",
    "                               test_loader=test_loader_24)\n",
    "    \n",
    "    end_time_24 = time.time()\n",
    "    time_24 = end_time_24 - start_time\n",
    "\n",
    "    train_loader_H2 = DataLoader(ds_train_H2, batch_size=16, shuffle=True)\n",
    "    val_loader_H2   = DataLoader(ds_val_H2,   batch_size=16, shuffle=False)\n",
    "    test_loader_H2  = DataLoader(ds_test_H2,  batch_size=16, shuffle=False)\n",
    "\n",
    "    best_config_H2, best_model_state_H2= train_and_tune(\"LSTM\", feature_cols=feature_cols, horizon=H2, isfirst24=False,\n",
    "        train_loader= train_loader_H2,\n",
    "        val_loader  = val_loader_H2,\n",
    "        test_loader  = test_loader_H2)\n",
    "    \n",
    "    end_time_H2 = time.time()\n",
    "    time_H2 = end_time_H2 - end_time_24\n",
    "    \n",
    "    train_loader_H3 = DataLoader(ds_train_H3, batch_size=16, shuffle=True)\n",
    "    val_loader_H3   = DataLoader(ds_val_H3,   batch_size=16, shuffle=False)\n",
    "    test_loader_H3  = DataLoader(ds_test_H3,  batch_size=16, shuffle=False)\n",
    "\n",
    "    best_config_H3, best_model_state_H3= train_and_tune(\"LSTM\", feature_cols=feature_cols, horizon=H3, isfirst24=False,\n",
    "        train_loader= train_loader_H3,\n",
    "        val_loader  = val_loader_H3,\n",
    "        test_loader  = test_loader_H3)\n",
    "    \n",
    "    end_time_H3 = time.time()\n",
    "    time_H3 = end_time_H3 - end_time_H2\n",
    "    \n",
    "    configs = [\n",
    "    {\"name\": \"H1_first24\", \"runtime_seconds\": round(time_24, 2), **best_config_24},\n",
    "    {\"name\": \"H2\",         \"runtime_seconds\": round(time_H2, 2), **best_config_H2},\n",
    "    {\"name\": \"H3\",         \"runtime_seconds\": round(time_H3, 2), **best_config_H3},\n",
    "     ]\n",
    "\n",
    "    df = pd.DataFrame(configs)\n",
    "    df.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/LSTM_tuning_results.csv\", index=False) \n",
    "\n",
    "else:\n",
    "    X_train_H1, y_train_H1 = build_noisy_windows(lookup_train, train_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "    X_val_H1, y_val_H1 = build_noisy_windows(lookup_val, val_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "    X_test_H1, y_test_H1 = build_noisy_windows(lookup_test, test_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "\n",
    "    X_train_H2, y_train_H2 = build_noisy_windows(lookup_train, train_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "    X_val_H2, y_val_H2 = build_noisy_windows(lookup_val, val_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "    X_test_H2, y_test_H2 = build_noisy_windows(lookup_test, test_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "\n",
    "    X_train_H3, y_train_H3 = build_noisy_windows(lookup_train, train_dates, horizon=H3, add_noise=True, first24hours = False)\n",
    "    X_val_H3, y_val_H3 = build_noisy_windows(lookup_val, val_dates, horizon=H3, add_noise=True, first24hours = False)\n",
    "    X_test_H3, y_test_H3 = build_noisy_windows(lookup_test, test_dates, horizon=H3, add_noise=True, first24hours = False)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_config_24 = train_and_tune(\n",
    "        MODEL_TYPE, feature_cols= feature_cols, horizon=H1, isfirst24=True,\n",
    "        X_train=X_train_H1, y_train=y_train_H1,\n",
    "        X_val=X_val_H1,     y_val=y_val_H1,\n",
    "        X_test=X_test_H1\n",
    "    )\n",
    "    end_time_24 = time.time()\n",
    "    time_24 = end_time_24 - start_time\n",
    "\n",
    "    best_config_H2 = train_and_tune(\n",
    "        MODEL_TYPE, feature_cols=feature_cols, horizon= H2, isfirst24=False,\n",
    "        X_train=X_train_H2, y_train=y_train_H2,\n",
    "        X_val=X_val_H2,     y_val=y_val_H2,\n",
    "        X_test=X_test_H2\n",
    "    )\n",
    "    end_time_H2 = time.time()\n",
    "    time_H2 = end_time_H2 - end_time_24\n",
    "\n",
    "    best_config_H3 = train_and_tune(\n",
    "        MODEL_TYPE, feature_cols=feature_cols, horizon= H3, isfirst24=False,\n",
    "        X_train=X_train_H3, y_train=y_train_H3,\n",
    "        X_val=X_val_H3,     y_val=y_val_H3,\n",
    "        X_test=X_test_H3\n",
    "    )\n",
    "    end_time_H3 = time.time()\n",
    "    time_H3 = end_time_H3 - end_time_H2\n",
    "\n",
    "    configs = [\n",
    "    {\"name\": \"H1_first24\", \"runtime_seconds\": round(time_24, 2), **best_config_24},\n",
    "    {\"name\": \"H2\",         \"runtime_seconds\": round(time_H2, 2), **best_config_H2},\n",
    "    {\"name\": \"H3\",         \"runtime_seconds\": round(time_H3, 2), **best_config_H3},\n",
    "  ]\n",
    "\n",
    "    df = pd.DataFrame(configs)\n",
    "    df.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/{MODEL_TYPE}_tuning_results.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b01a7b",
   "metadata": {},
   "source": [
    "LSTM, XGBoost & LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d46e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "MODEL_TYPE       = \"LGBM\"   # \"XGB\" or \"LGBM\" or \"LSTM\"\n",
    "for Forecast_Horizon in [48, 144]:     \n",
    "    H1 = 24                    \n",
    "    H2 = Forecast_Horizon - H1  \n",
    "    FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "    OUTPUT_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/{MODEL_TYPE}_forecasts_{Forecast_Horizon}.csv'\n",
    "    TRUTH_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Models_true/{MODEL_TYPE}_true_vals_{Forecast_Horizon}.csv'\n",
    "    #TRUTH_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/Truth_validation_{Forecast_Horizon}.csv'\n",
    "    Validation_Path = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/{MODEL_TYPE}_forecast_validation_{Forecast_Horizon}.csv'\n",
    "    load_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/load_error.csv')\n",
    "    Solar_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv')\n",
    "    WindShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv')\n",
    "    WindOffShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv')\n",
    "    all_preds = [] \n",
    "    for seed in [0, 1, 2]:\n",
    "        set_seed(seed)\n",
    "        def load_data(file_path):\n",
    "            df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "        \n",
    "            df['hour'] = df['start_datetime'].dt.hour\n",
    "            df['dayofweek'] = df['start_datetime'].dt.dayofweek\n",
    "            df['month'] = df['start_datetime'].dt.month\n",
    "            df['day']= df['start_datetime'].dt.date\n",
    "\n",
    "            if MODEL_TYPE == 'LSTM':\n",
    "                train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "                val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "                test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "                categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "                features_to_scale = [col for col in train_df.columns if col not in categorical_features]\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                train_scaled = pd.DataFrame(scaler.fit_transform(train_df[features_to_scale]), index=train_df.index, columns=features_to_scale)\n",
    "                val_scaled   = pd.DataFrame(scaler.transform(val_df[features_to_scale]), index=val_df.index, columns=features_to_scale)\n",
    "                test_scaled  = pd.DataFrame(scaler.transform(test_df[features_to_scale]), index=test_df.index, columns=features_to_scale)\n",
    "\n",
    "                price_scaler = StandardScaler().fit(train_df[['Price']])\n",
    "                for df_scaled, df_orig in zip([train_scaled, val_scaled, test_scaled], [train_df, val_df, test_df]):\n",
    "                    df_scaled['Price'] = price_scaler.transform(df_orig[['Price']])\n",
    "                    df_scaled[categorical_features] = df_orig[categorical_features]\n",
    "\n",
    "            else:\n",
    "                train_scaled = df[(df['start_datetime'].dt.year == 2023)]\n",
    "                val_scaled = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "                test_scaled  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "                categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "                \n",
    "                price_scaler = StandardScaler().fit(train_scaled[['Price']])\n",
    "            \n",
    "            return train_scaled, val_scaled, test_scaled, price_scaler\n",
    "\n",
    "        def make_lookup(df):\n",
    "            lookup = defaultdict(dict)\n",
    "            for _, row in df.iterrows():\n",
    "                key = (row['day'], row['hour'])\n",
    "                lookup[key] = row.to_dict()\n",
    "            return lookup\n",
    "\n",
    "        class ElectricityDataset(Dataset):\n",
    "            def __init__(self, lookup, base_dates, horizon, add_noise, first24hours):\n",
    "                self.lookup = lookup\n",
    "                if not first24hours:\n",
    "                    self.base_dates = [d + timedelta(days=1) for d in base_dates]\n",
    "                else:\n",
    "                    self.base_dates = base_dates\n",
    "                self.horizon = horizon\n",
    "                self.add_noise = add_noise\n",
    "\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.base_dates)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                base_day = self.base_dates[idx]\n",
    "                np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "                feats = []\n",
    "                y = []\n",
    "\n",
    "                for hour in range(self.horizon):\n",
    "                    days_offset = hour // 24\n",
    "                    h = hour % 24\n",
    "\n",
    "                    d = (base_day + timedelta(days=days_offset)).date()\n",
    "                    d1 = (base_day - timedelta(days=1)).date()\n",
    "                    d7 = (d - timedelta(days=7))\n",
    "\n",
    "                    p_d1 = self.lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "                    p_d7 = self.lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "            \n",
    "                    Z1 = self.lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "                    Z2 = self.lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "                    Z3 = self.lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "                    Z4 = self.lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "                    Z5 = self.lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "                    Z6 = self.lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "            \n",
    "                    hour = self.lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "                    dayofweek = self.lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "                    month = self.lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "                    x = [p_d1, p_d7, Z1, Z2, Z3, Z4,Z5, Z6, hour, dayofweek, month]\n",
    "                    feats.append(x)\n",
    "\n",
    "                    target = self.lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "                    y.append(target)\n",
    "\n",
    "                feats = np.array(feats, dtype=np.float32)\n",
    "                y = np.array(y, dtype=np.float32)\n",
    "\n",
    "                if self.add_noise:\n",
    "                    for start in range(0, self.horizon, 24):\n",
    "                        i = start // 24\n",
    "                        end = start + 24\n",
    "                        np.random.seed(42)\n",
    "                        feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                    \n",
    "                return torch.from_numpy(feats), torch.from_numpy(y)\n",
    "\n",
    "        def build_noisy_windows(lookup, days, horizon, add_noise, first24hours):\n",
    "            Xs = []\n",
    "            Ys = []\n",
    "            if not first24hours:\n",
    "                days = [d + timedelta(days=1) for d in days]\n",
    "\n",
    "            for base_day in days:\n",
    "                np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "                feats = []\n",
    "                y = []\n",
    "\n",
    "                for hour in range(horizon):\n",
    "                    days_offset = hour // 24\n",
    "                    h = hour % 24\n",
    "\n",
    "                    d = (base_day + timedelta(days=days_offset)).date()\n",
    "                    d1 = (base_day - timedelta(days=1)).date()\n",
    "                    d7 = (d - timedelta(days=7))\n",
    "\n",
    "                    p_d1 = lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "                    p_d7 = lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "\n",
    "                    Z1 = lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "                    Z2 = lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "                    Z3 = lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "                    Z4 = lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "                    Z5 = lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "                    Z6 = lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "\n",
    "                    hour_val = lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "                    dayofweek = lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "                    month = lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "                    x = [p_d1, p_d7, Z1, Z2, Z3, Z4, Z5, Z6, hour_val, dayofweek, month]\n",
    "                    feats.append(x)\n",
    "\n",
    "                    target = lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "                    y.append(target)\n",
    "\n",
    "                feats = np.array(feats, dtype=np.float32)\n",
    "                y = np.array(y, dtype=np.float32)\n",
    "\n",
    "                if add_noise:\n",
    "                    for start in range(0, horizon, 24):\n",
    "                        i = start // 24\n",
    "                        end = start + 24\n",
    "                        feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "\n",
    "                Xs.append(feats.flatten())  \n",
    "                Ys.append(y)\n",
    "\n",
    "            return np.vstack(Xs), np.vstack(Ys)\n",
    "        \n",
    "        class LSTMForecast(nn.Module):\n",
    "            def __init__(self, input_size, output_size, num_layers, dropout, hidden_size=64):\n",
    "                super().__init__()\n",
    "                self.lstm = nn.LSTM(\n",
    "                    input_size=input_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout  \n",
    "                )\n",
    "                self.dropout = nn.Dropout(dropout)  \n",
    "                self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "            def forward(self, x):\n",
    "                _, (h_n, _) = self.lstm(x)\n",
    "                h = h_n[-1] \n",
    "                h = self.dropout(h)\n",
    "                return self.fc(h)\n",
    "            \n",
    "        class EarlyStopping:\n",
    "            def __init__(self, patience=5, min_delta=0):\n",
    "                self.patience = patience\n",
    "                self.min_delta = min_delta\n",
    "                self.counter = 0\n",
    "                self.best_loss = None\n",
    "                self.early_stop = False\n",
    "                self.best_state = None  \n",
    "\n",
    "            def __call__(self, val_loss, model):\n",
    "                if self.best_loss is None:\n",
    "                    self.best_loss = val_loss\n",
    "                    self.best_state = model.state_dict()\n",
    "                    return False\n",
    "                \n",
    "                if val_loss < self.best_loss - self.min_delta:\n",
    "                    self.best_loss = val_loss\n",
    "                    self.counter = 0\n",
    "                    self.best_state = model.state_dict()\n",
    "                else:\n",
    "                    self.counter += 1\n",
    "                    print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "                    if self.counter >= self.patience:\n",
    "                        self.early_stop = True\n",
    "                return self.early_stop\n",
    "\n",
    "\n",
    "        def train_and_predict(model_type, feature_cols,  horizon, isfirst24, **kwargs):\n",
    "            if model_type == \"LSTM\":\n",
    "                if isfirst24:\n",
    "                    model = LSTMForecast(input_size=len(feature_cols), output_size=horizon, num_layers=2, dropout=0.1)\n",
    "                    loss_fn = nn.MSELoss()\n",
    "                    opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3, min_lr=1e-5)\n",
    "                    early_stopper = EarlyStopping(patience=5, min_delta=1e-4)\n",
    "                else:\n",
    "                    if Forecast_Horizon == 48:\n",
    "                        model = LSTMForecast(input_size=len(feature_cols), output_size=horizon, num_layers=2, dropout=0.1)\n",
    "                        loss_fn = nn.MSELoss()\n",
    "                        opt = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "                        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3, min_lr=1e-5)\n",
    "                        early_stopper = EarlyStopping(patience=5, min_delta=1e-4)\n",
    "                    else:\n",
    "                        model = LSTMForecast(input_size=len(feature_cols), output_size=horizon, num_layers=3, dropout=0.1)\n",
    "                        loss_fn = nn.MSELoss()\n",
    "                        opt = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "                        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3, min_lr=1e-5)\n",
    "                        early_stopper = EarlyStopping(patience=5, min_delta=1e-4)\n",
    "\n",
    "                train_loader = kwargs[f'train_loader']\n",
    "                val_loader   = kwargs[f'val_loader']\n",
    "                test_loader  = kwargs[f'test_loader']\n",
    "\n",
    "                for epoch in range(1, 30):\n",
    "                    model.train()\n",
    "                    train_loss = 0.0\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        opt.zero_grad()\n",
    "                        y_pred = model(X_batch)\n",
    "                        loss = loss_fn(y_pred, y_batch)\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "                        train_loss += loss.item() * X_batch.size(0)\n",
    "                    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "                    model.eval()\n",
    "                    val_loss = 0.0\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            y_pred = model(X_batch)\n",
    "                            val_loss += loss_fn(y_pred, y_batch).item() * X_batch.size(0)\n",
    "                    val_loss /= len(val_loader.dataset)\n",
    "                    scheduler.step(val_loss)\n",
    "                    print(f\"Epoch {epoch:02d} — Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "                    if early_stopper(val_loss, model):\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "                model.eval()\n",
    "                all_preds = []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, _ in test_loader:\n",
    "                        all_preds.append(model(X_batch))\n",
    "                preds = torch.cat(all_preds, dim=0).cpu().numpy()\n",
    "                return price_scaler.inverse_transform(preds)\n",
    "\n",
    "            else:\n",
    "                X_train = kwargs['X_train']\n",
    "                y_train = kwargs['y_train']\n",
    "                X_val   = kwargs['X_val']\n",
    "                y_val   = kwargs['y_val']\n",
    "                X_test  = kwargs['X_test']\n",
    "\n",
    "                preds = np.zeros((len(X_test), horizon))\n",
    "                for h in tqdm(range(horizon), desc=\"Training sub-horizon\"):\n",
    "                    if model_type == \"XGB\":\n",
    "                        if isfirst24:\n",
    "                            reg = XGBRegressor(\n",
    "                                n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                                subsample=0.8, colsample_bytree=0.8, random_state = seed,\n",
    "                                tree_method=\"hist\", eval_metric='rmse',\n",
    "                                early_stopping_rounds=15, n_jobs=-1,\n",
    "                            )\n",
    "                        else:\n",
    "                            if Forecast_Horizon == 48:\n",
    "                                reg = XGBRegressor(\n",
    "                                    n_estimators=200, learning_rate=0.05, max_depth=3,\n",
    "                                    subsample=0.8, colsample_bytree=0.8, random_state = seed,\n",
    "                                    tree_method=\"hist\", eval_metric='rmse',\n",
    "                                    early_stopping_rounds=15, n_jobs=-1\n",
    "                            )\n",
    "                            else:\n",
    "                                reg = XGBRegressor(\n",
    "                                    n_estimators=200, learning_rate=0.05, max_depth=3,\n",
    "                                    subsample=0.8, colsample_bytree=0.8, random_state = seed,\n",
    "                                    tree_method=\"hist\", eval_metric='rmse',\n",
    "                                    early_stopping_rounds=15, n_jobs=-1)\n",
    "                        reg.fit(X_train, y_train[:, h], eval_set=[(X_val, y_val[:, h])], verbose=False)\n",
    "                    elif model_type == \"LGBM\":\n",
    "                        if isfirst24:\n",
    "                            reg = LGBMRegressor(\n",
    "                                objective='regression', n_estimators=100, learning_rate=0.1,\n",
    "                                max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                random_state=seed, eval_metric='rmse',\n",
    "                                early_stopping_rounds=15, verbose=-1, n_jobs=-1\n",
    "                            )\n",
    "                        else:\n",
    "                            if Forecast_Horizon == 48:\n",
    "                                reg = LGBMRegressor(\n",
    "                                    objective='regression', n_estimators=100, learning_rate=0.1,\n",
    "                                    max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                    random_state=seed, eval_metric='rmse',\n",
    "                                    early_stopping_rounds=15, verbose=-1, n_jobs=-1\n",
    "                                )\n",
    "                            else: \n",
    "                                reg = LGBMRegressor(\n",
    "                                    objective='regression', n_estimators=200, learning_rate=0.1,\n",
    "                                    max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                    random_state=seed, eval_metric='rmse',\n",
    "                                    early_stopping_rounds=15, verbose=-1, n_jobs=-1\n",
    "                                )\n",
    "                        reg.fit(X_train, y_train[:, h].copy(), eval_set=[(X_val, y_val[:, h].copy())])\n",
    "                    preds[:, h] = reg.predict(X_test)\n",
    "                return preds\n",
    "\n",
    "        train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        train_dates = train_dates['date'].tolist()\n",
    "        val_dates = val_dates['date'].tolist()\n",
    "        test_dates = test_dates['date'].tolist()\n",
    "\n",
    "        feature_cols = ['price_d1','price_d7', 'Load', 'Solar', 'WindShore', 'WindOffShore', 'Net_Import', 'Coal_Price', 'hour', 'dayofweek', 'month']\n",
    "\n",
    "        train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "        lookup_train = make_lookup(train_scaled)\n",
    "        lookup_val = make_lookup(val_scaled)\n",
    "        lookup_test = make_lookup(test_scaled)\n",
    "\n",
    "        if MODEL_TYPE == \"LSTM\":\n",
    "            ds_train_H1 = ElectricityDataset(lookup_train, train_dates, horizon=H1, add_noise=True, first24hours=True)\n",
    "            ds_val_H1   = ElectricityDataset( lookup_val, val_dates, horizon=H1, add_noise=True, first24hours=True)\n",
    "            ds_test_H1  = ElectricityDataset( lookup_test, test_dates,  horizon=H1, add_noise=True, first24hours=True)\n",
    "            ds_train_H2 = ElectricityDataset( lookup_train, train_dates, horizon=H2, add_noise=True, first24hours=False)\n",
    "            ds_val_H2   = ElectricityDataset(lookup_val,  val_dates,   horizon=H2, add_noise=True, first24hours=False)\n",
    "            ds_test_H2  = ElectricityDataset( lookup_test, test_dates,  horizon=H2, add_noise=True, first24hours=False)\n",
    "\n",
    "            train_loader_24 = DataLoader(ds_train_H1, batch_size=16, shuffle=True)\n",
    "            val_loader_24   = DataLoader(ds_val_H1,   batch_size=16, shuffle=False)\n",
    "            test_loader_24  = DataLoader(ds_test_H1,  batch_size=16, shuffle=False)\n",
    "\n",
    "            print(\"Training for the first 24 hours\")\n",
    "            preds1 = train_and_predict(\"LSTM\", feature_cols= feature_cols, horizon =H1, isfirst24=True,\n",
    "                                    train_loader=train_loader_24,\n",
    "                                    val_loader=val_loader_24,\n",
    "                                    test_loader=val_loader_24)\n",
    "\n",
    "\n",
    "            train_loader_H2 = DataLoader(ds_train_H2, batch_size=16, shuffle=True)\n",
    "            val_loader_H2   = DataLoader(ds_val_H2,   batch_size=16, shuffle=False)\n",
    "            test_loader_H2  = DataLoader(ds_test_H2,  batch_size=16, shuffle=False)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(f\"Training the other {H2} horizon\")\n",
    "            preds2 = train_and_predict(\"LSTM\", feature_cols=feature_cols, horizon=H2, isfirst24=False,\n",
    "                train_loader= train_loader_H2,\n",
    "                val_loader  = val_loader_H2,\n",
    "                test_loader  = val_loader_H2)\n",
    "            y_true_H1 = np.stack([ y.numpy() for _, y in ds_val_H1 ], axis=0)  \n",
    "            y_true_H2 = np.stack([ y.numpy() for _, y in ds_val_H2 ], axis=0)  \n",
    "\n",
    "            y_true_1 = price_scaler.inverse_transform(y_true_H1)\n",
    "            y_true_2 = price_scaler.inverse_transform(y_true_H2)\n",
    "\n",
    "            y_true = np.hstack([y_true_1, y_true_2])  \n",
    "\n",
    "        else:\n",
    "            X_train_H1, y_train_H1 = build_noisy_windows(lookup_train, train_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "            X_val_H1, y_val_H1 = build_noisy_windows(lookup_val, val_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "            X_test_H1, y_test_H1 = build_noisy_windows(lookup_test, test_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "\n",
    "            X_train_H2, y_train_H2 = build_noisy_windows(lookup_train, train_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "            X_val_H2, y_val_H2 = build_noisy_windows(lookup_val, val_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "            X_test_H2, y_test_H2 = build_noisy_windows(lookup_test, test_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "\n",
    "            print(\"Training the first 24 hours\")\n",
    "            preds1 = train_and_predict(\n",
    "                MODEL_TYPE, feature_cols= feature_cols, horizon=H1, isfirst24=True,\n",
    "                X_train=X_train_H1, y_train=y_train_H1,\n",
    "                X_val=X_val_H1,     y_val=y_val_H1,\n",
    "                X_test=X_val_H1\n",
    "            )\n",
    "\n",
    "            print(f\"Training the other {H2} hours of the forecast horizon\")\n",
    "            preds2 = train_and_predict(\n",
    "                MODEL_TYPE, feature_cols=feature_cols, horizon= H2, isfirst24=False,\n",
    "                X_train=X_train_H2, y_train=y_train_H2,\n",
    "                X_val=X_val_H2,     y_val=y_val_H2,\n",
    "                X_test=X_val_H2\n",
    "            )\n",
    "            y_true = np.hstack([y_test_H1, y_test_H2])\n",
    "\n",
    "\n",
    "        y_pred = np.hstack([preds1, preds2])\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        print(\"\\n\")\n",
    "        print(f\"{MODEL_TYPE} Test RMSE ({Forecast_Horizon}-h avg): {rmse:.2f} €/MWh\")\n",
    "        all_preds.append(y_pred)\n",
    "\n",
    "    all_preds_array = np.stack(all_preds, axis=0)\n",
    "    mean_preds = np.mean(all_preds_array, axis=0)\n",
    "    avg_rmse = np.sqrt(mean_squared_error(y_true, mean_preds))\n",
    "    print(f\"\\n Avg_RMSE: {avg_rmse} \\n\")\n",
    "    final_forecasts_df = pd.DataFrame(mean_preds, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "    final_true_vals_df = pd.DataFrame(y_true, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "\n",
    "    final_forecasts_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    final_true_vals_df.to_csv(TRUTH_PATH, index = False)\n",
    "    #final_forecasts_df.to_csv(Validation_Path, index = False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58775ee9",
   "metadata": {},
   "source": [
    "LightGBM as before, but now generates Shap values and Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import shap\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "MODEL_TYPE       = \"LGBM\"  \n",
    "for Forecast_Horizon in [48, 144]:     \n",
    "    H1 = 24                    \n",
    "    H2 = Forecast_Horizon - H1  \n",
    "    FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "    OUTPUT_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/{MODEL_TYPE}_forecasts_{Forecast_Horizon}.csv'\n",
    "    TRUTH_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Models_true/{MODEL_TYPE}_true_vals_{Forecast_Horizon}.csv'\n",
    "    Validation_Path = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/{MODEL_TYPE}_forecast_validation_{Forecast_Horizon}.csv'\n",
    "    load_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/load_error.csv')\n",
    "    Solar_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv')\n",
    "    WindShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv')\n",
    "    WindOffShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv')\n",
    "    all_preds = [] \n",
    "    all_shap_values = []\n",
    "    all_importance = []\n",
    "    for seed in [0, 1, 3]:\n",
    "        set_seed(seed)\n",
    "        def load_data(file_path):\n",
    "            df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "        \n",
    "            df['hour'] = df['start_datetime'].dt.hour\n",
    "            df['dayofweek'] = df['start_datetime'].dt.dayofweek\n",
    "            df['month'] = df['start_datetime'].dt.month\n",
    "            df['day']= df['start_datetime'].dt.date\n",
    "\n",
    "            if MODEL_TYPE == 'LSTM':\n",
    "                \n",
    "                train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "                val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "                test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "                categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "                features_to_scale = [col for col in train_df.columns if col not in categorical_features]\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                train_scaled = pd.DataFrame(scaler.fit_transform(train_df[features_to_scale]), index=train_df.index, columns=features_to_scale)\n",
    "                val_scaled   = pd.DataFrame(scaler.transform(val_df[features_to_scale]), index=val_df.index, columns=features_to_scale)\n",
    "                test_scaled  = pd.DataFrame(scaler.transform(test_df[features_to_scale]), index=test_df.index, columns=features_to_scale)\n",
    "\n",
    "                price_scaler = StandardScaler().fit(train_df[['Price']])\n",
    "                for df_scaled, df_orig in zip([train_scaled, val_scaled, test_scaled], [train_df, val_df, test_df]):\n",
    "                    df_scaled['Price'] = price_scaler.transform(df_orig[['Price']])\n",
    "                    df_scaled[categorical_features] = df_orig[categorical_features]\n",
    "\n",
    "            else:\n",
    "                train_scaled = df[(df['start_datetime'].dt.year == 2023)]\n",
    "                val_scaled = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "                test_scaled  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "                categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "                \n",
    "                price_scaler = StandardScaler().fit(train_scaled[['Price']])\n",
    "            \n",
    "            return train_scaled, val_scaled, test_scaled, price_scaler\n",
    "\n",
    "        def make_lookup(df):\n",
    "            lookup = defaultdict(dict)\n",
    "            for _, row in df.iterrows():\n",
    "                key = (row['day'], row['hour'])\n",
    "                lookup[key] = row.to_dict()\n",
    "            return lookup\n",
    "\n",
    "\n",
    "        def build_noisy_windows(lookup, days, horizon, add_noise, first24hours):\n",
    "            Xs = []\n",
    "            Ys = []\n",
    "            if not first24hours:\n",
    "                days = [d + timedelta(days=1) for d in days]\n",
    "\n",
    "            for base_day in days:\n",
    "                np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "                feats = []\n",
    "                y = []\n",
    "\n",
    "                for hour in range(horizon):\n",
    "                    days_offset = hour // 24\n",
    "                    h = hour % 24\n",
    "\n",
    "                    d = (base_day + timedelta(days=days_offset)).date()\n",
    "                    d1 = (base_day - timedelta(days=1)).date()\n",
    "                    d7 = (d - timedelta(days=7))\n",
    "\n",
    "                    p_d1 = lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "                    p_d7 = lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "\n",
    "                    Z1 = lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "                    Z2 = lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "                    Z3 = lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "                    Z4 = lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "                    Z5 = lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "                    Z6 = lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "\n",
    "                    hour_val = lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "                    dayofweek = lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "                    month = lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "                    x = [p_d1, p_d7, Z1, Z2, Z3, Z4, Z5, Z6, hour_val, dayofweek, month]\n",
    "                    feats.append(x)\n",
    "\n",
    "                    target = lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "                    y.append(target)\n",
    "\n",
    "                feats = np.array(feats, dtype=np.float32)\n",
    "                y = np.array(y, dtype=np.float32)\n",
    "\n",
    "                if add_noise:\n",
    "                    for start in range(0, horizon, 24):\n",
    "                        i = start // 24\n",
    "                        end = start + 24\n",
    "                        feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "\n",
    "                Xs.append(feats.flatten()) \n",
    "                Ys.append(y)\n",
    "\n",
    "            return np.vstack(Xs), np.vstack(Ys)\n",
    "        \n",
    "        def train_and_predict(model_type, feature_cols, horizon, isfirst24, **kwargs):\n",
    "            X_train = kwargs['X_train']\n",
    "            y_train = kwargs['y_train']\n",
    "            X_val   = kwargs['X_val']\n",
    "            y_val   = kwargs['y_val']\n",
    "            X_test  = kwargs['X_test']\n",
    "\n",
    "            preds = np.zeros((len(X_test), horizon))\n",
    "            models = []\n",
    "            importances = []\n",
    "            shap_values_all = []\n",
    "\n",
    "            for h in tqdm(range(horizon), desc=\"Training sub-horizon\"):\n",
    "                if model_type == \"LGBM\":\n",
    "                    if isfirst24:\n",
    "                        reg = LGBMRegressor(\n",
    "                            objective='regression', n_estimators=100, learning_rate=0.1,\n",
    "                            max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                            random_state=seed, eval_metric='rmse',\n",
    "                            early_stopping_rounds=15, verbose=-1, n_jobs=-1\n",
    "                        )\n",
    "                    else:\n",
    "                        if Forecast_Horizon == 48:\n",
    "                            reg = LGBMRegressor(\n",
    "                                objective='regression', n_estimators=100, learning_rate=0.1,\n",
    "                                max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                random_state=seed, eval_metric='rmse',\n",
    "                                early_stopping_rounds=15, verbose=-1, n_jobs=-1\n",
    "                            )\n",
    "                        else:\n",
    "                            reg = LGBMRegressor(\n",
    "                                objective='regression', n_estimators=200, learning_rate=0.1,\n",
    "                                max_depth=3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                random_state=seed, eval_metric='rmse',\n",
    "                                early_stopping_rounds=15, verbose=-1, n_jobs=-1\n",
    "                            )\n",
    "                    reg.fit(X_train, y_train[:, h].copy(), eval_set=[(X_val, y_val[:, h].copy())])\n",
    "                    models.append(reg)\n",
    "                    importances.append(reg.feature_importances_)\n",
    "\n",
    "                    explainer = shap.TreeExplainer(reg)\n",
    "                    shap_vals = explainer.shap_values(X_test)\n",
    "                    shap_values_all.append(shap_vals)\n",
    "\n",
    "                preds[:, h] = reg.predict(X_test)\n",
    "\n",
    "            shap_values_avg = np.mean(np.array(shap_values_all), axis=0)\n",
    "            importances = np.mean(np.array(importances), axis=0)\n",
    "\n",
    "            return preds, models, importances, shap_values_avg\n",
    "\n",
    "\n",
    "        train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        train_dates = train_dates['date'].tolist()\n",
    "        val_dates = val_dates['date'].tolist()\n",
    "        test_dates = test_dates['date'].tolist()\n",
    "\n",
    "        feature_cols = ['price_d1','price_d7', 'Load', 'Solar', 'WindShore', 'WindOffShore', 'Net_Import', 'Coal_Price', 'hour', 'dayofweek', 'month']\n",
    "\n",
    "        train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "        lookup_train = make_lookup(train_scaled)\n",
    "        lookup_val = make_lookup(val_scaled)\n",
    "        lookup_test = make_lookup(test_scaled)\n",
    "\n",
    "        X_train_H1, y_train_H1 = build_noisy_windows(lookup_train, train_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "        X_val_H1, y_val_H1 = build_noisy_windows(lookup_val, val_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "        X_test_H1, y_test_H1 = build_noisy_windows(lookup_test, test_dates, horizon=H1, add_noise=True, first24hours = True)\n",
    "\n",
    "        X_train_H2, y_train_H2 = build_noisy_windows(lookup_train, train_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "        X_val_H2, y_val_H2 = build_noisy_windows(lookup_val, val_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "        X_test_H2, y_test_H2 = build_noisy_windows(lookup_test, test_dates, horizon=H2, add_noise=True, first24hours = False)\n",
    "\n",
    "        print(\"Training the first 24 hours\")\n",
    "        preds1, models1, importances1, shap_vals1 = train_and_predict(\n",
    "            MODEL_TYPE, feature_cols=feature_cols, horizon=H1, isfirst24=True,\n",
    "            X_train=X_train_H1, y_train=y_train_H1,\n",
    "            X_val=X_val_H1, y_val=y_val_H1,\n",
    "            X_test=X_test_H1\n",
    "        )\n",
    "\n",
    "        print(f\"Training the other {H2} hours of the forecast horizon\")\n",
    "        preds2, models2, importances2, shap_vals2 = train_and_predict(\n",
    "            MODEL_TYPE, feature_cols=feature_cols, horizon=H2, isfirst24=False,\n",
    "            X_train=X_train_H2, y_train=y_train_H2,\n",
    "            X_val=X_val_H2, y_val=y_val_H2,\n",
    "            X_test=X_test_H2\n",
    "        )\n",
    "\n",
    "        y_true = np.hstack([y_test_H1, y_test_H2])\n",
    "        y_pred = np.hstack([preds1, preds2])\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        print(f\"{MODEL_TYPE} Test RMSE ({Forecast_Horizon}-h avg): {rmse:.2f} €/MWh\")\n",
    "        all_preds.append(y_pred)\n",
    "\n",
    "        shap_vals_combined = np.hstack([shap_vals1, shap_vals2])  \n",
    "        all_shap_values.append(shap_vals_combined)\n",
    "        importance_combined = np.hstack([importances1, importances2]) \n",
    "        all_importance.append(importance_combined)\n",
    "\n",
    "    all_preds_array = np.stack(all_preds, axis=0)\n",
    "    mean_preds = np.mean(all_preds_array, axis=0)\n",
    "\n",
    "    all_shap_array = np.stack(all_shap_values, axis=0)\n",
    "    mean_shap_values = np.mean(all_shap_array, axis=0) \n",
    "    n_features = len(feature_cols)\n",
    "    horizon =Forecast_Horizon\n",
    "\n",
    "    all_importance_array = np.stack(all_importance, axis=0)\n",
    "    mean_importance = np.mean(all_importance_array, axis=0)\n",
    "    mean_importance_reshaped = mean_importance.reshape(horizon, n_features)  \n",
    "    mean_importance_per_feature = mean_importance_reshaped.mean(axis=0) \n",
    "    feature_cols = np.array(feature_cols)\n",
    "    mean_importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': mean_importance_per_feature\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    print(mean_importance_df)\n",
    "   \n",
    "    mean_abs_shap = np.abs(mean_shap_values).mean(axis=0)\n",
    "    mean_abs_shap_reshaped = mean_abs_shap.reshape(horizon, n_features) \n",
    "    mean_abs_shap_per_feature = mean_abs_shap_reshaped.mean(axis=0)   \n",
    "\n",
    "    shap_summary_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'mean_abs_shap': mean_abs_shap_per_feature\n",
    "    }).sort_values(by='mean_abs_shap', ascending=False)\n",
    "\n",
    "    print(\"\\nGlobal SHAP feature importance averaged over horizon and seeds:\")\n",
    "    print(shap_summary_df)\n",
    "\n",
    "    all_preds_array = np.stack(all_preds, axis=0)\n",
    "    mean_preds = np.mean(all_preds_array, axis=0)\n",
    "    avg_rmse = np.sqrt(mean_squared_error(y_true, mean_preds))\n",
    "    print(f\"\\n Avg_RMSE: {avg_rmse} \\n\")\n",
    "    final_forecasts_df = pd.DataFrame(mean_preds, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "    final_true_vals_df = pd.DataFrame(y_true, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "\n",
    "    #final_forecasts_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    #final_true_vals_df.to_csv(TRUTH_PATH, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b66d5",
   "metadata": {},
   "source": [
    "Combining LSTM, LightGBM and XGBOOSt into hybrid models (Including tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def tune_values(Forecast_Horizon):\n",
    "    xgb_results = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/XGB_forecast_validation_{Forecast_Horizon}.csv')\n",
    "    lgbm_results = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/LGBM_forecast_validation_{Forecast_Horizon}.csv')\n",
    "    lstm_results = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/LSTM_forecast_validation_{Forecast_Horizon}.csv')\n",
    "    true_vals = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/Truth_validation_{Forecast_Horizon}.csv')\n",
    "\n",
    "    xgb = xgb_results.values\n",
    "    lstm = lstm_results.values\n",
    "    lgbm = lgbm_results.values\n",
    "    true = true_vals.values\n",
    "\n",
    "    results = []\n",
    "    for comb in ['xgb', 'lgbm']:\n",
    "        print(f\"Start tuning combination of LSTM and {comb} for horizon {Forecast_Horizon}\")\n",
    "        start_time = time.time()\n",
    "        base_model = xgb if comb == 'xgb' else lgbm\n",
    "        lstm_flat = lstm.flatten()\n",
    "\n",
    "        t1_values = np.linspace(np.percentile(lstm_flat, 1), np.percentile(lstm_flat, 50), 200)\n",
    "        t2_values = np.linspace(np.percentile(lstm_flat, 50), np.percentile(lstm_flat, 99), 200)\n",
    "\n",
    "        best_rmse = float('inf')\n",
    "        best_t1, best_t2 = None, None\n",
    "\n",
    "    \n",
    "        for t1 in tqdm(t1_values, desc=\"Tuning T1 values\"):\n",
    "            for t2 in t2_values:\n",
    "                if t1 >= t2:\n",
    "                    continue\n",
    "                hybrid = np.where(lstm < t1, base_model, np.where(lstm < t2, lstm, base_model))\n",
    "                rmse = compute_rmse(true, hybrid)\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_t1, best_t2 = t1, t2\n",
    "        \n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        print(f\"Best RMSE: {best_rmse:.4f} with T1 = {best_t1:.2f}, T2 = {best_t2:.2f}\")\n",
    "        results.append((comb, best_t1, best_t2, best_rmse, run_time))\n",
    "    return pd.DataFrame(results, columns=[\"Model\", \"T1\", \"T2\", \"RMSE\", \"Run_time\"])  \n",
    "\n",
    "def tune_weighted_values(Forecast_Horizon):\n",
    "    xgb_results = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/XGB_forecast_validation_{Forecast_Horizon}.csv')\n",
    "    lgbm_results = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/LGBM_forecast_validation_{Forecast_Horizon}.csv')\n",
    "    lstm_results = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/LSTM_forecast_validation_{Forecast_Horizon}.csv')\n",
    "    true_vals = pd.read_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_validation/Truth_validation_{Forecast_Horizon}.csv')\n",
    "\n",
    "    xgb = xgb_results.values\n",
    "    lstm = lstm_results.values\n",
    "    lgbm = lgbm_results.values\n",
    "    true = true_vals.values\n",
    "\n",
    "    results = []\n",
    "    for comb in ['xgb', 'lgbm']:\n",
    "        print(f\"Start tuning combination of LSTM and {comb} for horizon {Forecast_Horizon}\")\n",
    "        start_time = time.time()\n",
    "        base_model = xgb if comb == 'xgb' else lgbm\n",
    "        W1_values = np.arange(0.0, 1.01, 0.01)\n",
    "       \n",
    "        best_rmse = float('inf')\n",
    "        best_w1, best_w2 = None, None\n",
    "\n",
    "    \n",
    "        for w1 in tqdm(W1_values, desc=\"Tuning W1 values\"):\n",
    "            hybrid = w1 * lstm + (1 - w1) * base_model\n",
    "            rmse = compute_rmse(true, hybrid)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_w1, best_w2 = w1, 1-w1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        print(f\"Best RMSE: {best_rmse:.4f} with T1 = {best_w1:.2f}, T2 = {best_w2:.2f}\")\n",
    "        results.append((comb, best_w1, best_w2, best_rmse, run_time))\n",
    "    \n",
    "    \n",
    "    print(f\"Start tuning triple combination for horizon {Forecast_Horizon}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    weight_steps = np.arange(0.0, 1.01, 0.01) \n",
    "\n",
    "    best_rmse = float('inf')\n",
    "    best_w1, best_w2, best_w3 = None, None, None\n",
    "\n",
    "    for w1 in tqdm(weight_steps, desc=\"Tuning w1 values\"):\n",
    "        for w2 in weight_steps:\n",
    "            if w1 + w2 > 1:\n",
    "                continue  \n",
    "            \n",
    "            w3 = 1 - w1 - w2\n",
    "            hybrid = w1 * lstm + w2 * xgb + w3 * lgbm\n",
    "            \n",
    "            rmse = compute_rmse(true, hybrid)\n",
    "            \n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_w1, best_w2, best_w3 = w1, w2, w3\n",
    "\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    print(f\"Best RMSE: {best_rmse:.4f} with weights w1={best_w1:.2f}, w2={best_w2:.2f}, w3={best_w3:.2f}, runtime={run_time:.2f}s\")\n",
    "    results.append(('Triple_combination', best_w1, best_w2, best_rmse, run_time))\n",
    "    return pd.DataFrame(results, columns=[\"Model\", \"W1\", \"W2\", \"RMSE\", \"Run_time\"])  \n",
    "\n",
    "\n",
    "def generate_hybrid_forecast_threshold(Forecast_Horizon):\n",
    "    lstm = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/LSTM_forecasts_{Forecast_Horizon}.csv\")\n",
    "    lgbm = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/LGBM_forecasts_{Forecast_Horizon}.csv\")\n",
    "    xgb = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/XGB_forecasts_{Forecast_Horizon}.csv\")\n",
    "    true_vals = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Models_true/LSTM_true_vals_{Forecast_Horizon}.csv\")\n",
    "    tuning_results = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_tuning_results_{Forecast_Horizon}.csv\", sep=\",\")\n",
    "\n",
    "    for comb in ['xgb', 'lgbm']:\n",
    "        base_model = xgb if comb == 'xgb' else lgbm\n",
    "        T1 = tuning_results[tuning_results['Model'] == comb]['T1'].values[0]\n",
    "        T2 = tuning_results[tuning_results['Model'] == comb]['T2'].values[0]\n",
    "        hybrid = np.where(lstm < T1, base_model, np.where(lstm < T2, lstm, base_model))\n",
    "        rmse = compute_rmse(true_vals, hybrid)\n",
    "        print(f'RMSE for Threshold combination of lstm & {comb} for forecast horizon: {Forecast_Horizon} is {rmse}')\n",
    "        final_forecasts_hybrid = pd.DataFrame(hybrid, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "        final_forecasts_hybrid.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/Hybrid_LSTM_{comb}_forecasts_{Forecast_Horizon}.csv', index=False)\n",
    "        \n",
    "\n",
    "        \n",
    "def generate_hybrid_forecast_weights(Forecast_Horizon):\n",
    "    lstm = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/LSTM_forecasts_{Forecast_Horizon}.csv\")\n",
    "    lgbm = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/LGBM_forecasts_{Forecast_Horizon}.csv\")\n",
    "    xgb = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/XGB_forecasts_{Forecast_Horizon}.csv\")\n",
    "    true_vals = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Models_true/LSTM_true_vals_{Forecast_Horizon}.csv\")\n",
    "    tuning_results = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_tuning_results_weighted_{Forecast_Horizon}.csv\", sep=\",\")\n",
    "\n",
    "    for comb in ['xgb', 'lgbm']:\n",
    "        base_model = xgb if comb == 'xgb' else lgbm\n",
    "        W1 = tuning_results[tuning_results['Model'] == comb]['W1'].values[0]\n",
    "        W2 = tuning_results[tuning_results['Model'] == comb]['W2'].values[0]\n",
    "        hybrid = W1 * lstm + W2 * base_model\n",
    "        rmse = compute_rmse(true_vals, hybrid)\n",
    "        print(f'RMSE for Weighted combination of lstm & {comb} for forecast horizon: {Forecast_Horizon} is {rmse}')\n",
    "        final_forecasts_hybrid = pd.DataFrame(hybrid, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "        final_forecasts_hybrid.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/Hybrid_weighted_LSTM_{comb}_forecasts_{Forecast_Horizon}.csv', index=False)\n",
    "\n",
    "    row = tuning_results[tuning_results['Model'] == 'Triple_combination']\n",
    "    W1three = row['W1'].values[0]\n",
    "    W2three = row['W2'].values[0]\n",
    "    W3three = 1 - W1three - W2three\n",
    "    hybrid_three = W1three * lstm + W2three * xgb + W3three * lgbm\n",
    "    rmse = compute_rmse(true_vals, hybrid_three)\n",
    "    print(f'RMSE for Weighted combination of all three for forecast horizon: {Forecast_Horizon} is {rmse}')\n",
    "    final_forecasts_hybrid_three = pd.DataFrame(hybrid_three, columns=[f'h+{i}' for i in range(Forecast_Horizon)])\n",
    "    final_forecasts_hybrid_three.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/Hybrid_weighted_AllThree_forecasts_{Forecast_Horizon}.csv', index=False)\n",
    "\n",
    "\n",
    "for Forecast_Horizon in [48, 144]:\n",
    "    # Uncomment the lines below to run the tuning\n",
    "    #result = tune_values(Forecast_Horizon)\n",
    "    #result.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_tuning_results_{Forecast_Horizon}.csv', index=False)\n",
    "    #result = tune_weighted_values(Forecast_Horizon)\n",
    "    #result.to_csv(f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/Combination_tuning_results_weighted_{Forecast_Horizon}.csv', index=False)\n",
    "    generate_hybrid_forecast_threshold(Forecast_Horizon)\n",
    "    generate_hybrid_forecast_weights(Forecast_Horizon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9d5a4",
   "metadata": {},
   "source": [
    "Tuning DDN, DDNN Johnson-SU and DDNN Mixture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b381583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  \n",
    "import tensorflow_probability as tfp  \n",
    "from tensorflow_probability import distributions as tfd\n",
    "from datetime import datetime, timedelta\n",
    "keras = tf.keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import brentq\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from keras import regularizers\n",
    "import time\n",
    "import random\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "distribution = True #False for DNN point forecasts, True for probabilistic forecasts\n",
    "FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "mixture = False # True for mixture density, False for regular Johnson-SU distribution\n",
    "load_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/load_error.csv')\n",
    "Solar_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv')\n",
    "WindShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv')\n",
    "WindOffShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv')\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def calc_rmse(y_true, y_pred):\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"RMSE: {rmse_val:.3f} €/MWh\")\n",
    "    return rmse_val\n",
    "\n",
    "def pinball_loss(y_true, y_pred, quantiles):\n",
    "   \n",
    "    N, H, Q = y_pred.shape\n",
    "    losses = np.zeros((N, H, Q))\n",
    "    for q_idx, q in enumerate(quantiles):\n",
    "        err = y_true - y_pred[:, :, q_idx]\n",
    "        losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "    return losses\n",
    "\n",
    "def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles):\n",
    "    pinball_losses = pinball_loss(np.array(y_true), Y_pred, np.array(quantiles))\n",
    "    crps = np.mean(np.trapz(pinball_losses, np.array(quantiles), axis=-1))\n",
    "    return pinball_losses, np.mean(pinball_losses), crps\n",
    "\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha):\n",
    "    width = y_upper - y_lower\n",
    "    under = (y_lower - y_true) * (y_true < y_lower)\n",
    "    over  = (y_true - y_upper) * (y_true > y_upper)\n",
    "    \n",
    "    score = width + (2.0 / alpha) * (under + over)\n",
    "    return score\n",
    "\n",
    "def calculate_winkler(Y_pred, y_true, quantiles, lower_q, upper_q):\n",
    "    alpha = 1.0 - (upper_q - lower_q)\n",
    "    try:\n",
    "        li = quantiles.index(lower_q)\n",
    "        ui = quantiles.index(upper_q)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"lower_q={lower_q} or upper_q={upper_q} not in quantiles list\")\n",
    "    \n",
    "    y_lower = Y_pred[:, :, li]\n",
    "    y_upper = Y_pred[:, :, ui]\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    scores = winkler_score(y_true, y_lower, y_upper, alpha)\n",
    "    mean_score = scores.mean()\n",
    "    print(f\"Mean Winkler score (α={alpha:.2f}, interval {lower_q}-{upper_q}): {mean_score:.4f}\")\n",
    "    return scores, mean_score\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "  \n",
    "    df['hour'] = df['start_datetime'].dt.hour\n",
    "    df['dayofweek'] = df['start_datetime'].dt.dayofweek\n",
    "    df['month'] = df['start_datetime'].dt.month\n",
    "    df['day']= df['start_datetime'].dt.date\n",
    "\n",
    "    train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "    val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "    test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "\n",
    "    categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "    features_to_scale = [col for col in train_df.columns if col not in categorical_features]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(train_df[features_to_scale]), index=train_df.index, columns=features_to_scale)\n",
    "    val_scaled   = pd.DataFrame(scaler.transform(val_df[features_to_scale]), index=val_df.index, columns=features_to_scale)\n",
    "    test_scaled  = pd.DataFrame(scaler.transform(test_df[features_to_scale]), index=test_df.index, columns=features_to_scale)\n",
    "\n",
    "    price_scaler = StandardScaler().fit(train_df[['Price']])\n",
    "    for df_scaled, df_orig in zip([train_scaled, val_scaled, test_scaled], [train_df, val_df, test_df]):\n",
    "        df_scaled['Price'] = price_scaler.transform(df_orig[['Price']])\n",
    "        df_scaled[categorical_features] = df_orig[categorical_features]\n",
    "\n",
    "    return train_scaled, val_scaled, test_scaled, price_scaler\n",
    "\n",
    "def make_lookup(df):\n",
    "    lookup = defaultdict(dict)\n",
    "    for _, row in df.iterrows():\n",
    "        key = (row['day'], row['hour'])\n",
    "        lookup[key] = row.to_dict()\n",
    "    return lookup\n",
    "\n",
    "def build_noisy_windows(lookup, days, horizon, add_noise):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "\n",
    "    for base_day in days:\n",
    "        np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "        feats = []\n",
    "        y = []\n",
    "\n",
    "        for hour in range(horizon):\n",
    "            days_offset = hour // 24\n",
    "            h = hour % 24\n",
    "\n",
    "            d = (base_day + timedelta(days=days_offset)).date()\n",
    "            d1 = (base_day - timedelta(days=1)).date()\n",
    "            d7 = (d - timedelta(days=7))\n",
    "\n",
    "            p_d1 = lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "            p_d7 = lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "\n",
    "            Z1 = lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "            Z2 = lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "            Z3 = lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "            Z4 = lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "            Z5 = lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "            Z6 = lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "\n",
    "            hour_val = lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "            dayofweek = lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "            month = lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "            x = [p_d1, p_d7, Z1, Z2, Z3, Z4, Z5, Z6, hour_val, dayofweek, month]\n",
    "            feats.append(x)\n",
    "\n",
    "            target = lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "            y.append(target)\n",
    "\n",
    "        feats = np.array(feats, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        if add_noise:\n",
    "            for start in range(0, horizon, 24):\n",
    "                i = start // 24\n",
    "                end = start + 24\n",
    "                feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "\n",
    "        Xs.append(feats.flatten())  \n",
    "        Ys.append(y)\n",
    "\n",
    "    return np.vstack(Xs), np.vstack(Ys)\n",
    "\n",
    "\n",
    "\n",
    "def build_model(input_dim, horizon, distribution, dropout_rates, activations, layer_sizes, learning_and_weight_rate, regularization, mixture):\n",
    "    inputs = keras.Input(shape=(input_dim,), name='input_layer')\n",
    "    \n",
    "    hidden = keras.layers.Dense(layer_sizes[0], \n",
    "                               activation=activations[0], \n",
    "                               kernel_regularizer=regularization[0])(inputs)\n",
    "    hidden = keras.layers.Dropout(dropout_rates[0])(hidden)\n",
    "    hidden = keras.layers.Dense(layer_sizes[1], \n",
    "                               activation=activations[1], \n",
    "                               kernel_regularizer=regularization[1])(hidden)\n",
    "    hidden = keras.layers.Dropout(dropout_rates[1])(hidden)\n",
    "\n",
    "    if not distribution:\n",
    "        outputs = keras.layers.Dense(horizon, activation='linear')(hidden)\n",
    "        model = keras.Model(inputs, outputs)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_and_weight_rate[0], weight_decay=learning_and_weight_rate[1]),\n",
    "                      loss='mse',\n",
    "                      metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    if distribution:\n",
    "        if not mixture:\n",
    "            param_layers = []\n",
    "            for p in range(4):\n",
    "                param_layers.append(keras.layers.Dense(\n",
    "                    horizon, activation='linear')(hidden))\n",
    "            linear = tf.keras.layers.concatenate(param_layers)\n",
    "            outputs = tfp.layers.DistributionLambda(\n",
    "                lambda t: tfd.JohnsonSU(\n",
    "                        loc=t[..., :horizon],\n",
    "                        scale=1e-3 + 3 * tf.math.softplus(t[..., horizon:2*horizon]),\n",
    "                        tailweight= 1 + 3 * tf.math.softplus(t[..., 2*horizon:3*horizon]),\n",
    "                        skewness=t[..., 3*horizon:]))(linear)\n",
    "        \n",
    "            model = keras.Model(inputs, outputs)\n",
    "\n",
    "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_and_weight_rate[0], weight_decay=learning_and_weight_rate[1]),\n",
    "                        loss=lambda y, rv_y: -rv_y.log_prob(y),\n",
    "                        metrics=['mae'])\n",
    "            return model\n",
    "        else:\n",
    "            param_output = keras.layers.Dense(9 * horizon)(hidden) \n",
    "\n",
    "            def make_mixture_distribution(params):\n",
    "                params = tf.reshape(params, [-1, horizon, 9])\n",
    "\n",
    "                loc1 = params[:, :, 0]\n",
    "                scale1 = 1e-3 + 1.5 * tf.nn.softplus(params[:, :, 1])\n",
    "                tail1 = 1 + 1.5 * tf.nn.softplus(params[:, :, 2])\n",
    "                skew1 = params[:, :, 3]\n",
    "\n",
    "                loc2 = params[:, :, 4]\n",
    "                scale2 = 1e-3 + 1.5 * tf.nn.softplus(params[:, :, 5])\n",
    "                tail2 = 1 + 1.5 * tf.nn.softplus(params[:, :, 6])\n",
    "                skew2 = params[:, :, 7]\n",
    "\n",
    "                raw_logits = params[:, :, 8]  \n",
    "                logits = tf.stack([raw_logits, -raw_logits], axis=-1)  \n",
    "\n",
    "                locs = tf.stack([loc1, loc2], axis=-1)\n",
    "                scales = tf.stack([scale1, scale2], axis=-1)\n",
    "                tails = tf.stack([tail1, tail2], axis=-1)\n",
    "                skews = tf.stack([skew1, skew2], axis=-1)\n",
    "\n",
    "                components = tfd.JohnsonSU(\n",
    "                    loc=locs,\n",
    "                    scale=scales,\n",
    "                    tailweight=tails,\n",
    "                    skewness=skews\n",
    "                )\n",
    "\n",
    "                mixture_dist = tfd.MixtureSameFamily(\n",
    "                    mixture_distribution=tfd.Categorical(logits=logits),\n",
    "                    components_distribution=components\n",
    "                )\n",
    "\n",
    "                return mixture_dist\n",
    "\n",
    "            outputs = tfp.layers.DistributionLambda(make_mixture_distribution)(param_output)\n",
    "\n",
    "            model = keras.Model(inputs, outputs)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_and_weight_rate[0], weight_decay=learning_and_weight_rate[1]),\n",
    "                        loss=lambda y, rv_y: -rv_y.log_prob(y),\n",
    "                        metrics=['mae'])\n",
    "            return model \n",
    "    \n",
    "def extract_distribution_parameters(model, test_ds):\n",
    "        locs = []\n",
    "        scales = []\n",
    "        tailweights = []\n",
    "        skewnesses = []\n",
    "\n",
    "        for x_batch, _ in test_ds:\n",
    "            dist = model(x_batch, training=False)\n",
    "            locs.append(dist.loc.numpy())\n",
    "            scales.append(dist.scale.numpy())\n",
    "            tailweights.append(dist.tailweight.numpy())\n",
    "            skewnesses.append(dist.skewness.numpy())\n",
    "\n",
    "        locs = np.concatenate(locs, axis=0)\n",
    "        scales = np.concatenate(scales, axis=0)\n",
    "        tailweights = np.concatenate(tailweights, axis=0)\n",
    "        skewnesses = np.concatenate(skewnesses, axis=0)\n",
    "\n",
    "        return locs, scales, tailweights, skewnesses\n",
    "\n",
    "def extract_distribution_parameters_mixture(model, test_ds):\n",
    "    locs = []\n",
    "    scales = []\n",
    "    tailweights = []\n",
    "    skewnesses = []\n",
    "    logits = []\n",
    "\n",
    "    for x_batch, _ in test_ds:\n",
    "        dist = model(x_batch, training=False)\n",
    "        locs.append(dist.components_distribution.loc.numpy())\n",
    "        scales.append(dist.components_distribution.scale.numpy())\n",
    "        tailweights.append(dist.components_distribution.tailweight.numpy())\n",
    "        skewnesses.append(dist.components_distribution.skewness.numpy())\n",
    "        logits.append(dist.mixture_distribution.logits.numpy())\n",
    "\n",
    "    locs = np.concatenate(locs, axis=0)\n",
    "    scales = np.concatenate(scales, axis=0)\n",
    "    tailweights = np.concatenate(tailweights, axis=0)\n",
    "    skewnesses = np.concatenate(skewnesses, axis=0)\n",
    "    logits = np.concatenate(logits, axis=0)\n",
    "    \n",
    "\n",
    "    return locs, scales, tailweights, skewnesses, logits\n",
    "\n",
    "\n",
    "    \n",
    "def hyperparameter_search(X_train, y_train, X_val, y_val, input_dim, horizon, distribution, price_scaler, mixture):\n",
    "    \n",
    "    dropout_options = [\n",
    "    [0.0, 0.0],\n",
    "    [0.1, 0.1],\n",
    "    [0.25, 0.25],\n",
    "    [0.1, 0.25],    \n",
    "    [0.25, 0.1]\n",
    "    ]\n",
    "\n",
    "    activation_options = [\n",
    "    ['relu', 'relu'],\n",
    "    ['elu', 'elu'],\n",
    "    ['tanh', 'tanh']]\n",
    "\n",
    " \n",
    "    layer_size_options = [\n",
    "        [256, 128], \n",
    "        [512, 256]  \n",
    "    ]\n",
    "\n",
    "    learning_rate_weight_decay_options = [\n",
    "        [1e-3, 0],\n",
    "        [1e-3, 1e-5],\n",
    "        [5e-4, 1e-5] \n",
    "    ]\n",
    "\n",
    "    regularization_options = [\n",
    "        [regularizers.l2(1e-5), regularizers.l2(1e-5)],\n",
    "        [regularizers.l2(1e-4), regularizers.l2(1e-4)]\n",
    "    ]\n",
    "    \n",
    "    if not distribution:\n",
    "        best_rmse = np.inf\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        numb_options = len(dropout_options) * len(activation_options) * len(layer_size_options) * len(learning_rate_weight_decay_options) * len(regularization_options)\n",
    "\n",
    "        i = 0\n",
    "        for dropout_rates in dropout_options:\n",
    "            for activations in activation_options:\n",
    "                for layer_sizes in layer_size_options:\n",
    "                    for learning_and_weight in learning_rate_weight_decay_options:\n",
    "                        for regularization in regularization_options:\n",
    "                            i += 1\n",
    "                            rmses = []\n",
    "                            reg_values = [reg.l2 if reg is not None else None for reg in regularization]\n",
    "                            print(f\"\\nTraining model {i} of {numb_options} with dropout={dropout_rates}, activations={activations}, layer_sizes={layer_sizes}, learning&weight ={learning_and_weight} and regularization = {reg_values}\")\n",
    "\n",
    "                            for seed in tqdm([0,1,2], desc=\"Seeds\", disable = True):\n",
    "                                set_seed(seed)\n",
    "\n",
    "                                model = build_model(input_dim, horizon, distribution,\n",
    "                                                    dropout_rates=dropout_rates,\n",
    "                                                    activations=activations,\n",
    "                                                    layer_sizes=layer_sizes,\n",
    "                                                    learning_and_weight_rate=learning_and_weight,\n",
    "                                                    regularization=regularization, mixture=mixture)\n",
    "                                \n",
    "                                early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',  \n",
    "                                    patience=4,                          \n",
    "                                    restore_best_weights=True            \n",
    "                                )\n",
    "\n",
    "                                model.fit(X_train, y_train,\n",
    "                                                    validation_data=(X_val, y_val),\n",
    "                                                    epochs=25,\n",
    "                                                    batch_size=16,\n",
    "                                                    callbacks=[early_stopping],\n",
    "                                                    verbose=0)\n",
    "                                \n",
    "                                pred = model.predict(X_val)\n",
    "                                y_pred = price_scaler.inverse_transform(pred)\n",
    "                                y_true = price_scaler.inverse_transform(y_val).round(2)\n",
    "\n",
    "                                val_rmse = np.sqrt(mean_squared_error(y_pred, y_true))\n",
    "                                rmses.append(val_rmse)\n",
    "                            avg_rmse = np.mean(rmses)    \n",
    "                            print(f\"Validation RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "                            if avg_rmse < best_rmse:\n",
    "                                best_rmse = avg_rmse\n",
    "                                best_params = (dropout_rates, activations, layer_sizes, learning_and_weight, reg_values)\n",
    "                                best_model = model\n",
    "                                print(f\"New best model with validation RMSE: {best_rmse}\")\n",
    "\n",
    "        print(f\"\\nFinished Tuning! Best validation RMSE: {best_rmse:.4f}\")\n",
    "        print(f\"Best params: dropout={best_params[0]}, activations={best_params[1]}, layer_sizes={best_params[2]}, learning&weight={best_params[3]}, regularization={best_params[4]}\")\n",
    "\n",
    "        return best_model, best_params, best_rmse\n",
    "    \n",
    "    if distribution:\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(16)\n",
    "        best_crps = np.inf\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "        numb_options = len(dropout_options) * len(activation_options) * len(layer_size_options) * len(learning_rate_weight_decay_options) * len(regularization_options)\n",
    "\n",
    "        i = 0\n",
    "        for dropout_rates in dropout_options:\n",
    "            for activations in activation_options:\n",
    "                for layer_sizes in layer_size_options:\n",
    "                    for learning_and_weight in learning_rate_weight_decay_options:\n",
    "                        for regularization in regularization_options:\n",
    "                            i += 1\n",
    "                            crpses = []\n",
    "                            reg_values = [reg.l2 if reg is not None else None for reg in regularization]\n",
    "                            print(f\"\\nTraining model {i} of {numb_options} with dropout={dropout_rates}, activations={activations}, layer_sizes={layer_sizes}, learning&weight ={learning_and_weight} and regularization = {reg_values}\")\n",
    "\n",
    "                            for seed in tqdm([0,1,3], desc=\"Seeds\", disable=True):\n",
    "                                set_seed(seed)\n",
    "\n",
    "                                model = build_model(input_dim, horizon, distribution,\n",
    "                                                    dropout_rates=dropout_rates,\n",
    "                                                    activations=activations,\n",
    "                                                    layer_sizes=layer_sizes,\n",
    "                                                    learning_and_weight_rate=learning_and_weight,\n",
    "                                                    regularization=regularization, mixture =mixture)\n",
    "                                \n",
    "                                early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='val_loss',  \n",
    "                                    patience=4,                          \n",
    "                                    restore_best_weights=True            \n",
    "                                )\n",
    "\n",
    "                                model.fit(X_train, y_train,\n",
    "                                                    validation_data=(X_val, y_val),\n",
    "                                                    epochs=20,\n",
    "                                                    batch_size=16,\n",
    "                                                    callbacks=[early_stopping],\n",
    "                                                    verbose=0)\n",
    "                                \n",
    "                                if not mixture:\n",
    "                                    locs, scales, tailweights, skewnesses = extract_distribution_parameters(model, val_ds)\n",
    "                                    distributions = tfd.JohnsonSU(\n",
    "                                    loc=np.array(locs),\n",
    "                                    scale=np.array(scales),\n",
    "                                    tailweight=np.array(tailweights),\n",
    "                                    skewness=np.array(skewnesses)\n",
    "                                )\n",
    "                                    \n",
    "                                    n_samples = 2000\n",
    "                                    samples = distributions.sample(n_samples) \n",
    "\n",
    "                                    quantiles = np.arange(0.01, 1.00, 0.01).round(2)\n",
    "                                    samples_np = samples.numpy()  \n",
    "\n",
    "                                    Y_pred = np.quantile(samples_np, q=quantiles, axis=0)  \n",
    "\n",
    "                                    Y_pred = np.transpose(Y_pred, (1, 2, 0)) \n",
    "                                    (N, H, Q)  = Y_pred.shape\n",
    "\n",
    "                                \n",
    "                                    Y_pred = price_scaler.inverse_transform(Y_pred.reshape(-1,1)).reshape(N, H, Q)\n",
    "                                    y_true = price_scaler.inverse_transform(y_val).round(2)\n",
    "\n",
    "                                else:\n",
    "                                    locs, scales, tailweights, skewnesses, logits = extract_distribution_parameters_mixture(model, val_ds)\n",
    "                                    distributions = tfd.JohnsonSU(\n",
    "                                    loc=np.array(locs),\n",
    "                                    scale=np.array(scales),\n",
    "                                    tailweight=np.array(tailweights),\n",
    "                                    skewness=np.array(skewnesses),\n",
    "                                )\n",
    "                                    mixture_distribution = tfd.MixtureSameFamily(\n",
    "                                        mixture_distribution=tfd.Categorical(logits=logits),\n",
    "                                        components_distribution=distributions,\n",
    "                                    )\n",
    "                                    n_samples = 1000\n",
    "                                    samples = mixture_distribution.sample(n_samples)  \n",
    "\n",
    "                                    quantiles = np.arange(0.01, 1.00, 0.01).round(2)\n",
    "                                    samples_np = samples.numpy()  \n",
    "\n",
    "                                    Y_pred = np.quantile(samples_np, q=quantiles, axis=0)  \n",
    "\n",
    "                                    Y_pred = np.transpose(Y_pred, (1, 2, 0))  \n",
    "                                    (N, H, q)  = Y_pred.shape\n",
    "\n",
    "                                    Y_pred = price_scaler.inverse_transform(Y_pred.reshape(-1,1)).reshape(N, H, len(quantiles))\n",
    "                                    y_true = price_scaler.inverse_transform(y_val).round(2)\n",
    "                                _, _, val_crps = calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles)\n",
    "                                crpses.append(val_crps)\n",
    "                                print(f\"CRPS of seed {seed} is {val_crps}\")\n",
    "                                del model\n",
    "                                del distributions, locs, scales, tailweights, skewnesses, Y_pred\n",
    "                                K.clear_session()\n",
    "                                gc.collect()\n",
    "                            avg_crps = np.mean(crpses)    \n",
    "                            print(f\"Validation CRPS: {avg_crps:.4f}\")\n",
    "\n",
    "                            if avg_crps < best_crps:\n",
    "                                best_crps= avg_crps\n",
    "                                best_params = (dropout_rates, activations, layer_sizes, learning_and_weight, reg_values)\n",
    "                                print(f\"New best model with validation CRPS: {best_crps}\")\n",
    "\n",
    "        print(f\"\\nFinished Tuning! Best validation CRPS: {best_crps:.4f}\")\n",
    "        print(f\"Best params: dropout={best_params[0]}, activations={best_params[1]}, layer_sizes={best_params[2]}, learning&weight={best_params[3]}, regularization={best_params[4]}\")\n",
    "\n",
    "        return best_params, best_crps\n",
    "\n",
    "def prepare_and_train(distribution, Forecast_Horizon):\n",
    "    train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "    val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "    test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "    train_dates = train_dates['date'].tolist()\n",
    "    val_dates = val_dates['date'].tolist()\n",
    "    test_dates = test_dates['date'].tolist()\n",
    "\n",
    "    train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "    lookup_train = make_lookup(train_scaled)\n",
    "    lookup_val = make_lookup(val_scaled)\n",
    "    lookup_test = make_lookup(test_scaled)\n",
    "\n",
    "    x_train, y_train = build_noisy_windows(lookup_train, train_dates, Forecast_Horizon, add_noise=True)\n",
    "    x_val, y_val = build_noisy_windows(lookup_val, val_dates, Forecast_Horizon, add_noise=True)\n",
    "    x_test, y_test= build_noisy_windows(lookup_test, test_dates, Forecast_Horizon, add_noise = True)\n",
    "\n",
    "    model = build_model(input_dim=x_train.shape[1], horizon=Forecast_Horizon, distribution=distribution)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(16)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(16)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=[early_stop])\n",
    "\n",
    "    if not distribution:\n",
    "        preds = model.predict(test_ds)\n",
    "        return preds, y_test, price_scaler, test_dates\n",
    "    else:\n",
    "        if not mixture:\n",
    "            def extract_distribution_parameters(model, test_ds, horizon):\n",
    "                locs = []\n",
    "                scales = []\n",
    "                tailweights = []\n",
    "                skewnesses = []\n",
    "\n",
    "                for x_batch, _ in test_ds:\n",
    "                    dist = model(x_batch, training=False)\n",
    "                    locs.append(dist.loc.numpy())\n",
    "                    scales.append(dist.scale.numpy())\n",
    "                    tailweights.append(dist.tailweight.numpy())\n",
    "                    skewnesses.append(dist.skewness.numpy())\n",
    "\n",
    "                locs = np.concatenate(locs, axis=0)\n",
    "                scales = np.concatenate(scales, axis=0)\n",
    "                tailweights = np.concatenate(tailweights, axis=0)\n",
    "                skewnesses = np.concatenate(skewnesses, axis=0)\n",
    "\n",
    "                return locs, scales, tailweights, skewnesses\n",
    "            locs, scales, tailweights, skewnesses = extract_distribution_parameters(model, test_ds, horizon=Forecast_Horizon)\n",
    "            return locs, scales, tailweights, skewnesses, y_test, price_scaler, test_dates\n",
    "\n",
    "        else:\n",
    "            def extract_distribution_parameters(model, test_ds, horizon):\n",
    "                locs = []\n",
    "                scales = []\n",
    "                tailweights = []\n",
    "                skewnesses = []\n",
    "                logits = []\n",
    "\n",
    "                for x_batch, _ in test_ds:\n",
    "                    dist = model(x_batch, training=False)\n",
    "                    locs.append(dist.components_distribution.loc.numpy())\n",
    "                    scales.append(dist.components_distribution.scale.numpy())\n",
    "                    tailweights.append(dist.components_distribution.tailweight.numpy())\n",
    "                    skewnesses.append(dist.components_distribution.skewness.numpy())\n",
    "                    logits.append(dist.mixture_distribution.logits.numpy())\n",
    "\n",
    "                locs = np.concatenate(locs, axis=0)\n",
    "                scales = np.concatenate(scales, axis=0)\n",
    "                tailweights = np.concatenate(tailweights, axis=0)\n",
    "                skewnesses = np.concatenate(skewnesses, axis=0)\n",
    "                logits = np.concatenate(logits, axis=0)\n",
    "                \n",
    "\n",
    "                return locs, scales, tailweights, skewnesses, logits\n",
    "       \n",
    "            locs, scales, tailweights, skewnesses, logits = extract_distribution_parameters(model, test_ds, horizon=Forecast_Horizon)\n",
    "            return locs, scales, tailweights, skewnesses, logits, y_test, price_scaler, test_dates\n",
    "\n",
    "\n",
    "if not distribution:\n",
    "    results = []\n",
    "    for Forecast_horizon in [48, 144]:\n",
    "        print(f\"\\nStart tuning for Forecast horizon: {Forecast_horizon}\")\n",
    "        start_time = time.time()\n",
    "        train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "        lookup_train = make_lookup(train_scaled)\n",
    "        lookup_val = make_lookup(val_scaled)\n",
    "\n",
    "        train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        train_dates = train_dates['date'].tolist()\n",
    "        val_dates = val_dates['date'].tolist()\n",
    "\n",
    "\n",
    "        X_train, y_train = build_noisy_windows(lookup_train, train_dates, Forecast_horizon, add_noise = True)\n",
    "        X_val, y_val = build_noisy_windows(lookup_val, val_dates, Forecast_horizon, add_noise = True)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        best_model, best_params, best_rmse = hyperparameter_search(X_train, y_train, X_val, y_val, input_dim, Forecast_horizon, distribution, price_scaler, mixture)\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        results.append((Forecast_horizon, best_params, best_rmse, run_time))\n",
    "    results_df = pd.DataFrame(results, columns=['Forecast Horizon', 'Best Params', 'Best RMSE', 'Run time'])\n",
    "    results_df.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/DDNN_point_tuning_results.csv', index=False)\n",
    "if distribution:\n",
    "    results = []\n",
    "    for Forecast_horizon in [48, 144]:\n",
    "        print(f\"\\nStart tuning probability model for Forecast horizon: {Forecast_horizon}\")\n",
    "        start_time = time.time()\n",
    "        train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "        lookup_train = make_lookup(train_scaled)\n",
    "        lookup_val = make_lookup(val_scaled)\n",
    "\n",
    "        train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "        train_dates = train_dates['date'].tolist()\n",
    "        val_dates = val_dates['date'].tolist()\n",
    "\n",
    "\n",
    "        X_train, y_train = build_noisy_windows(lookup_train, train_dates, Forecast_horizon, add_noise = True)\n",
    "        X_val, y_val = build_noisy_windows(lookup_val, val_dates, Forecast_horizon, add_noise = True)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        best_params, best_crps = hyperparameter_search(X_train, y_train, X_val, y_val, input_dim, Forecast_horizon, distribution, price_scaler, mixture)\n",
    "        end_time = time.time()\n",
    "        run_time = end_time - start_time\n",
    "        results.append((Forecast_horizon, best_params, best_crps, run_time))\n",
    "    results_df = pd.DataFrame(results, columns=['Forecast Horizon', 'Best Params', 'Best CRPS', 'Run time'])\n",
    "    results_df.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Tuning/DDNN_prob_tuning_results_144.csv', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36adb892",
   "metadata": {},
   "source": [
    "DDN point, DDNN Johnson-SU and DDNN Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v2.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp  \n",
    "from tensorflow_probability import distributions as tfd\n",
    "from datetime import datetime, timedelta\n",
    "keras = tf.keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import brentq\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from keras import regularizers\n",
    "\n",
    "for Forecast_Horizon in [48,144]: \n",
    "    distribution = True  # False for point forecasts, True for distributional forecasts\n",
    "    FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "    OUTPUT_PATH = f'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/DDNN_samples_{Forecast_Horizon}.csv'\n",
    "    mixture = False # False for single Johnson-SU, True for mixture of distributions\n",
    "    load_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/load_error.csv')\n",
    "    Solar_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv')\n",
    "    WindShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv')\n",
    "    WindOffShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv')\n",
    "\n",
    "    all_preds = [] \n",
    "    for seed in [0, 1, 2, 4]:\n",
    "\n",
    "        def calc_rmse(y_true, y_pred):\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            print(f\"RMSE: {rmse_val:.3f} €/MWh\")\n",
    "            return rmse_val\n",
    "\n",
    "        def pinball_loss(y_true, y_pred, quantiles):\n",
    "        \n",
    "            N, H, Q = y_pred.shape\n",
    "            losses = np.zeros((N, H, Q))\n",
    "            for q_idx, q in enumerate(quantiles):\n",
    "                err = y_true - y_pred[:, :, q_idx]\n",
    "                losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "            return losses\n",
    "\n",
    "        def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles):\n",
    "            pinball_losses = pinball_loss(np.array(y_true), Y_pred, np.array(quantiles))\n",
    "            print(f\"Mean Pinball Loss: {np.mean(pinball_losses):.4f}\")\n",
    "            crps = np.mean(np.trapz(pinball_losses, np.array(quantiles), axis=-1))\n",
    "            print(f'Approx CRPS: {crps:.4f}')\n",
    "            return pinball_losses, np.mean(pinball_losses), crps\n",
    "\n",
    "        def winkler_score(y_true, y_lower, y_upper, alpha):\n",
    "            width = y_upper - y_lower\n",
    "            \n",
    "            under = (y_lower - y_true) * (y_true < y_lower)\n",
    "            over  = (y_true - y_upper) * (y_true > y_upper)\n",
    "            \n",
    "            score = width + (2.0 / alpha) * (under + over)\n",
    "            return score\n",
    "\n",
    "        def calculate_winkler(Y_pred, y_true, quantiles, lower_q, upper_q):\n",
    "            alpha = 1.0 - (upper_q - lower_q)\n",
    "            try:\n",
    "                li = quantiles.index(lower_q)\n",
    "                ui = quantiles.index(upper_q)\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"lower_q={lower_q} or upper_q={upper_q} not in quantiles list\")\n",
    "            \n",
    "            y_lower = Y_pred[:, :, li]\n",
    "            y_upper = Y_pred[:, :, ui]\n",
    "            y_true = np.array(y_true)\n",
    "            \n",
    "            scores = winkler_score(y_true, y_lower, y_upper, alpha)\n",
    "            mean_score = scores.mean()\n",
    "            print(f\"Mean Winkler score (α={alpha:.2f}, interval {lower_q}-{upper_q}): {mean_score:.4f}\")\n",
    "            return scores, mean_score\n",
    "\n",
    "        def load_data(file_path):\n",
    "            df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "        \n",
    "            df['hour'] = df['start_datetime'].dt.hour\n",
    "            df['dayofweek'] = df['start_datetime'].dt.dayofweek\n",
    "            df['month'] = df['start_datetime'].dt.month\n",
    "            df['day']= df['start_datetime'].dt.date\n",
    "\n",
    "            train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "            val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "            test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "\n",
    "            categorical_features = ['hour', 'dayofweek', 'month', 'day', 'start_datetime', 'end_datetime', 'Timestamp']\n",
    "            features_to_scale = [col for col in train_df.columns if col not in categorical_features]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            train_scaled = pd.DataFrame(scaler.fit_transform(train_df[features_to_scale]), index=train_df.index, columns=features_to_scale)\n",
    "            val_scaled   = pd.DataFrame(scaler.transform(val_df[features_to_scale]), index=val_df.index, columns=features_to_scale)\n",
    "            test_scaled  = pd.DataFrame(scaler.transform(test_df[features_to_scale]), index=test_df.index, columns=features_to_scale)\n",
    "\n",
    "            price_scaler = StandardScaler().fit(train_df[['Price']])\n",
    "            for df_scaled, df_orig in zip([train_scaled, val_scaled, test_scaled], [train_df, val_df, test_df]):\n",
    "                df_scaled['Price'] = price_scaler.transform(df_orig[['Price']])\n",
    "                df_scaled[categorical_features] = df_orig[categorical_features]\n",
    "\n",
    "            return train_scaled, val_scaled, test_scaled, price_scaler\n",
    "\n",
    "        def make_lookup(df):\n",
    "            lookup = defaultdict(dict)\n",
    "            for _, row in df.iterrows():\n",
    "                key = (row['day'], row['hour'])\n",
    "                lookup[key] = row.to_dict()\n",
    "            return lookup\n",
    "\n",
    "        def build_noisy_windows(lookup, days, horizon, add_noise):\n",
    "            Xs = []\n",
    "            Ys = []\n",
    "\n",
    "            for base_day in days:\n",
    "                np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "                feats = []\n",
    "                y = []\n",
    "\n",
    "                for hour in range(horizon):\n",
    "                    days_offset = hour // 24\n",
    "                    h = hour % 24\n",
    "\n",
    "                    d = (base_day + timedelta(days=days_offset)).date()\n",
    "                    d1 = (base_day - timedelta(days=1)).date()\n",
    "                    d7 = (d - timedelta(days=7))\n",
    "\n",
    "                    p_d1 = lookup.get((d1, h), {}).get(\"Price\", np.nan)\n",
    "                    p_d7 = lookup.get((d7, h), {}).get(\"Price\", np.nan)\n",
    "\n",
    "                    Z1 = lookup.get((d, h), {}).get(\"Load\", np.nan)\n",
    "                    Z2 = lookup.get((d, h), {}).get(\"Solar\", np.nan)\n",
    "                    Z3 = lookup.get((d, h), {}).get(\"WindShore\", np.nan)\n",
    "                    Z4 = lookup.get((d, h), {}).get(\"WindOffShore\", np.nan)\n",
    "                    Z5 = lookup.get((d, h), {}).get(\"Net_Import\", np.nan)\n",
    "                    Z6 = lookup.get((d, h), {}).get(\"Coal_Price\", np.nan)\n",
    "\n",
    "\n",
    "                    hour_val = lookup.get((d, h), {}).get(\"hour\", 0)\n",
    "                    dayofweek = lookup.get((d, h), {}).get(\"dayofweek\", 0)\n",
    "                    month = lookup.get((d, h), {}).get(\"month\", 0)\n",
    "\n",
    "                    x = [p_d1, p_d7, Z1, Z2, Z3, Z4, Z5, Z6, hour_val, dayofweek, month]\n",
    "                    feats.append(x)\n",
    "\n",
    "                    target = lookup.get((d, h), {}).get(\"Price\", np.nan)\n",
    "                    y.append(target)\n",
    "\n",
    "                feats = np.array(feats, dtype=np.float32)\n",
    "                y = np.array(y, dtype=np.float32)\n",
    "\n",
    "                if add_noise:\n",
    "                    for start in range(0, horizon, 24):\n",
    "                        i = start // 24\n",
    "                        end = start + 24\n",
    "                        feats[start:end, 2] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 3] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 4] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 5] *= np.random.normal(WindOffShore_error.iloc[i, 0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                        feats[start:end, 6] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "                        feats[start:end, 7] *= np.random.normal(load_error.iloc[0, 0] + 1, load_error.iloc[1, 0], size=24)\n",
    "\n",
    "                Xs.append(feats.flatten()) \n",
    "                Ys.append(y)\n",
    "\n",
    "            return np.vstack(Xs), np.vstack(Ys)\n",
    "\n",
    "\n",
    "        def build_model(input_dim, horizon, distribution):\n",
    "            if distribution:\n",
    "                if not mixture:\n",
    "                    if Forecast_Horizon == 48:\n",
    "                        (dropout, activation, layer_size, learning_and_weight, regularization) = ([0.1, 0.1], ['tanh', 'tanh'], [512, 256], [0.0001, 1e-04], [0.00001, 0.00001])\n",
    "                    else:\n",
    "                        (dropout, activation, layer_size, learning_and_weight, regularization) = ([0.1, 0.1], ['tanh', 'tanh'], [256, 128], [0.0001, 1e-04], [0.0001, 0.0001])\n",
    "                else:\n",
    "                    if Forecast_Horizon == 48:\n",
    "                        (dropout, activation, layer_size, learning_and_weight, regularization) = ([0.25, 0.25], ['tanh', 'tanh'], [512, 256], [0.0001, 1e-04], [0.00001, 0.00001])\n",
    "                    else:\n",
    "                        (dropout, activation, layer_size, learning_and_weight, regularization) = ([0.1, 0.1], ['tanh', 'tanh'], [256, 128], [0.0001, 1e-04], [0.0001, 0.0001])   \n",
    "\n",
    "            else:\n",
    "                    if Forecast_Horizon == 48:\n",
    "                        (dropout, activation, layer_size, learning_and_weight, regularization) = ([0.25, 0.1], ['tanh', 'tanh'], [256, 128], [0.0001, 1e-04], [0.0001, 0.0001])\n",
    "                    else:\n",
    "                        (dropout, activation, layer_size, learning_and_weight, regularization) = ([0.25, 0.1], ['tanh', 'tanh'], [256, 128], [0.0001, 1e-04], [0.0001, 0.0001])   \n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            inputs = keras.Input(shape=(input_dim,), name='input_layer')\n",
    "            hidden = keras.layers.Dense(layer_size[0], \n",
    "                                    activation=activation[0], \n",
    "                                    kernel_regularizer=regularizers.l2(regularization[0]))(inputs)\n",
    "            hidden = keras.layers.Dropout(dropout[0])(hidden)\n",
    "            hidden = keras.layers.Dense(layer_size[1], \n",
    "                                    activation=activation[1], \n",
    "                                    kernel_regularizer=regularizers.l2(regularization[1]))(hidden)\n",
    "            hidden = keras.layers.Dropout(dropout[1])(hidden)\n",
    "\n",
    "\n",
    "            if not distribution:\n",
    "                outputs = keras.layers.Dense(horizon, activation='linear')(hidden)\n",
    "                model = keras.Model(inputs, outputs)\n",
    "                model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_and_weight[0], weight_decay=learning_and_weight[1]),\n",
    "                            loss='mae',\n",
    "                            metrics=['mae'])\n",
    "                return model\n",
    "\n",
    "            else:\n",
    "                if mixture:\n",
    "                    param_output = keras.layers.Dense(9 * horizon)(hidden) \n",
    "\n",
    "                    def make_mixture_distribution(params):\n",
    "                        params = tf.reshape(params, [-1, horizon, 9])\n",
    "\n",
    "                        loc1 = params[:, :, 0]\n",
    "                        scale1 = 1e-3 + 1.5 * tf.nn.softplus(params[:, :, 1])\n",
    "                        tail1 = 1 + 1.5 * tf.nn.softplus(params[:, :, 2])\n",
    "                        skew1 = params[:, :, 3]\n",
    "\n",
    "                        loc2 = params[:, :, 4]\n",
    "                        scale2 = 1e-3 + 1.5 * tf.nn.softplus(params[:, :, 5])\n",
    "                        tail2 = 1 + 1.5 * tf.nn.softplus(params[:, :, 6])\n",
    "                        skew2 = params[:, :, 7]\n",
    "\n",
    "                        raw_logits = params[:, :, 8] \n",
    "                        logits = tf.stack([raw_logits, -raw_logits], axis=-1)  \n",
    "\n",
    "                        locs = tf.stack([loc1, loc2], axis=-1)\n",
    "                        scales = tf.stack([scale1, scale2], axis=-1)\n",
    "                        tails = tf.stack([tail1, tail2], axis=-1)\n",
    "                        skews = tf.stack([skew1, skew2], axis=-1)\n",
    "\n",
    "                        components = tfd.JohnsonSU(\n",
    "                            loc=locs,\n",
    "                            scale=scales,\n",
    "                            tailweight=tails,\n",
    "                            skewness=skews\n",
    "                        )\n",
    "\n",
    "                        mixture_dist = tfd.MixtureSameFamily(\n",
    "                            mixture_distribution=tfd.Categorical(logits=logits),\n",
    "                            components_distribution=components\n",
    "                        )\n",
    "\n",
    "                        return mixture_dist\n",
    "\n",
    "                    outputs = tfp.layers.DistributionLambda(make_mixture_distribution)(param_output)\n",
    "\n",
    "                    model = keras.Model(inputs, outputs)\n",
    "                    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_and_weight[0], weight_decay=learning_and_weight[1]),\n",
    "                                loss=lambda y, rv_y: -rv_y.log_prob(y),\n",
    "                                metrics=['mae'])\n",
    "                    return model\n",
    "                \n",
    "                else:\n",
    "                    param_layers = []\n",
    "                    for p in range(4):\n",
    "                        param_layers.append(keras.layers.Dense(\n",
    "                            horizon, activation='linear')(hidden))\n",
    "                    linear = tf.keras.layers.concatenate(param_layers)\n",
    "                    outputs = tfp.layers.DistributionLambda(\n",
    "                        lambda t: tfd.JohnsonSU(\n",
    "                                loc=t[..., :horizon],\n",
    "                                scale=1e-3 + 3 * tf.math.softplus(t[..., horizon:2*horizon]),\n",
    "                                tailweight= 1 + 3 * tf.math.softplus(t[..., 2*horizon:3*horizon]),\n",
    "                                skewness=t[..., 3*horizon:]))(linear)\n",
    "                \n",
    "                    model = keras.Model(inputs, outputs)\n",
    "\n",
    "                    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_and_weight[0], weight_decay=learning_and_weight[1]), loss=lambda y, rv_y: -rv_y.log_prob(y),\n",
    "                                metrics=['mae'])\n",
    "                    return model\n",
    "\n",
    "\n",
    "        def prepare_and_train(distribution):\n",
    "            train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "            val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "            test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "            train_dates = train_dates['date'].tolist()\n",
    "            val_dates = val_dates['date'].tolist()\n",
    "            test_dates = test_dates['date'].tolist()\n",
    "\n",
    "            train_scaled, val_scaled, test_scaled, price_scaler = load_data(FILE_PATH)\n",
    "            lookup_train = make_lookup(train_scaled)\n",
    "            lookup_val = make_lookup(val_scaled)\n",
    "            lookup_test = make_lookup(test_scaled)\n",
    "\n",
    "            x_train, y_train = build_noisy_windows(lookup_train, train_dates, Forecast_Horizon, add_noise=True)\n",
    "            x_val, y_val = build_noisy_windows(lookup_val, val_dates, Forecast_Horizon, add_noise=True)\n",
    "            x_test, y_test= build_noisy_windows(lookup_test, test_dates, Forecast_Horizon, add_noise = True)\n",
    "\n",
    "            model = build_model(input_dim=x_train.shape[1], horizon=Forecast_Horizon, distribution=distribution)\n",
    "\n",
    "            train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(16)\n",
    "            val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(16)\n",
    "            test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(16)\n",
    "\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "            model.fit(train_ds, epochs=60, validation_data=val_ds, callbacks=[early_stop])\n",
    "\n",
    "            if not distribution:\n",
    "                preds = model.predict(test_ds)\n",
    "                return preds, y_test, price_scaler, test_dates\n",
    "            else:\n",
    "                if not mixture:\n",
    "                    def extract_distribution_parameters(model, test_ds, horizon):\n",
    "                        locs = []\n",
    "                        scales = []\n",
    "                        tailweights = []\n",
    "                        skewnesses = []\n",
    "\n",
    "                        for x_batch, _ in test_ds:\n",
    "                            dist = model(x_batch, training=False)\n",
    "                            locs.append(dist.loc.numpy())\n",
    "                            scales.append(dist.scale.numpy())\n",
    "                            tailweights.append(dist.tailweight.numpy())\n",
    "                            skewnesses.append(dist.skewness.numpy())\n",
    "\n",
    "                        locs = np.concatenate(locs, axis=0)\n",
    "                        scales = np.concatenate(scales, axis=0)\n",
    "                        tailweights = np.concatenate(tailweights, axis=0)\n",
    "                        skewnesses = np.concatenate(skewnesses, axis=0)\n",
    "\n",
    "                        return locs, scales, tailweights, skewnesses\n",
    "                    locs, scales, tailweights, skewnesses = extract_distribution_parameters(model, test_ds, horizon=Forecast_Horizon)\n",
    "                    return locs, scales, tailweights, skewnesses, y_test, price_scaler, test_dates\n",
    "\n",
    "                else:\n",
    "                    def extract_distribution_parameters(model, test_ds, horizon):\n",
    "                        locs = []\n",
    "                        scales = []\n",
    "                        tailweights = []\n",
    "                        skewnesses = []\n",
    "                        logits = []\n",
    "\n",
    "                        for x_batch, _ in test_ds:\n",
    "                            dist = model(x_batch, training=False)\n",
    "                            locs.append(dist.components_distribution.loc.numpy())\n",
    "                            scales.append(dist.components_distribution.scale.numpy())\n",
    "                            tailweights.append(dist.components_distribution.tailweight.numpy())\n",
    "                            skewnesses.append(dist.components_distribution.skewness.numpy())\n",
    "                            logits.append(dist.mixture_distribution.logits.numpy())\n",
    "\n",
    "                        locs = np.concatenate(locs, axis=0)\n",
    "                        scales = np.concatenate(scales, axis=0)\n",
    "                        tailweights = np.concatenate(tailweights, axis=0)\n",
    "                        skewnesses = np.concatenate(skewnesses, axis=0)\n",
    "                        logits = np.concatenate(logits, axis=0)\n",
    "                        \n",
    "\n",
    "                        return locs, scales, tailweights, skewnesses, logits\n",
    "            \n",
    "                    locs, scales, tailweights, skewnesses, logits = extract_distribution_parameters(model, test_ds, horizon=Forecast_Horizon)\n",
    "                    return locs, scales, tailweights, skewnesses, logits, y_test, price_scaler, test_dates\n",
    "\n",
    "\n",
    "        if not distribution:\n",
    "            outputs, y_test, price_scaler, start_times = prepare_and_train(distribution)\n",
    "            y_pred = price_scaler.inverse_transform(outputs)\n",
    "            y_true = price_scaler.inverse_transform(y_test).round(2)\n",
    "            rmse_val = calc_rmse(y_true, y_pred)\n",
    "            all_preds.append(y_pred)\n",
    "\n",
    "\n",
    "        else:\n",
    "            if not mixture:\n",
    "                locs, scales, tailweights, skewnesses, y_test, price_scaler, start_times = prepare_and_train(distribution)\n",
    "                \n",
    "                distributions = tfd.JohnsonSU(\n",
    "                loc=np.array(locs),\n",
    "                scale=np.array(scales),\n",
    "                tailweight=np.array(tailweights),\n",
    "                skewness=np.array(skewnesses)\n",
    "            )\n",
    "                \n",
    "                n_samples = 2000\n",
    "                samples = distributions.sample(n_samples) \n",
    "                samples_np = samples.numpy()\n",
    "\n",
    "                quantiles = np.arange(0.01, 1.00, 0.01).round(2)\n",
    "                samples_np = samples.numpy() \n",
    "                Y_pred = np.quantile(samples_np, q=quantiles, axis=0)  \n",
    "\n",
    "                Y_pred = np.transpose(Y_pred, (1, 2, 0)) \n",
    "                (N, H, Q)  = Y_pred.shape\n",
    "\n",
    "            \n",
    "                Y_pred = price_scaler.inverse_transform(Y_pred.reshape(-1,1)).reshape(N, H, Q)\n",
    "                y_true = price_scaler.inverse_transform(y_test).round(2)\n",
    "                all_preds.append(Y_pred)\n",
    "                \n",
    "                quantiles = quantiles.tolist()\n",
    "                midquantile = Y_pred[:, :, quantiles.index(0.5)]\n",
    "                rmse_val = calc_rmse(y_true, midquantile)\n",
    "                pinball_losses, pinball_loss_mean, crps = calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles)\n",
    "                nterval_scores, mean_interval_score = calculate_winkler(Y_pred, y_true, quantiles, lower_q = 0.2, upper_q=0.8)\n",
    "   \n",
    "            else:\n",
    "                locs, scales, tailweights, skewnesses, logits, y_test, price_scaler, start_times = prepare_and_train(distribution)\n",
    "                \n",
    "                distributions = tfd.JohnsonSU(\n",
    "                loc=np.array(locs),\n",
    "                scale=np.array(scales),\n",
    "                tailweight=np.array(tailweights),\n",
    "                skewness=np.array(skewnesses),\n",
    "            )\n",
    "                mixture_distribution = tfd.MixtureSameFamily(\n",
    "                    mixture_distribution=tfd.Categorical(logits=logits),\n",
    "                    components_distribution=distributions,\n",
    "                )\n",
    "                \n",
    "                \n",
    "                n_samples = 2000\n",
    "                samples = mixture_distribution.sample(n_samples) \n",
    "                quantiles = np.arange(0.01, 1.00, 0.01).round(2)\n",
    "                samples_np = samples.numpy()  \n",
    "\n",
    "                Y_pred = np.quantile(samples_np, q=quantiles, axis=0) \n",
    "                Y_pred = np.transpose(Y_pred, (1, 2, 0))  \n",
    "                (N, H, q)  = Y_pred.shape\n",
    "\n",
    "                Y_pred = price_scaler.inverse_transform(Y_pred.reshape(-1,1)).reshape(N, H, len(quantiles))\n",
    "                all_preds.append(Y_pred)\n",
    "                quantiles = quantiles.tolist()\n",
    "                midquantile = Y_pred[:, :, quantiles.index(0.5)]\n",
    "                y_true = price_scaler.inverse_transform(y_test).round(2)\n",
    "\n",
    "                rmse_val = calc_rmse(y_true, midquantile)\n",
    "                pinball_losses, pinball_loss_mean, crps = calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles)\n",
    "                interval_scores, mean_interval_score = calculate_winkler(Y_pred, y_true, quantiles, lower_q = 0.3, upper_q=0.7)\n",
    "\n",
    "    all_preds_array = np.stack(all_preds, axis=0)\n",
    "    mean_preds = np.mean(all_preds_array, axis=0)\n",
    "\n",
    "    if distribution:\n",
    "        pinball_losses, pinball_loss_mean, crps = calculate_pinball_loss_CRPS(y_true, mean_preds, quantiles)\n",
    "        interval_scores, mean_interval_score = calculate_winkler(mean_preds, y_true, quantiles, lower_q = 0.3, upper_q=0.7)\n",
    "        midquantile = mean_preds[:, :, quantiles.index(0.5)]\n",
    "        avg_rmse = np.sqrt(mean_squared_error(y_true, midquantile))\n",
    "        mean_preds = mean_preds.reshape(mean_preds.shape[0], -1)\n",
    "    else:\n",
    "        avg_rmse = np.sqrt(mean_squared_error(y_true, mean_preds))\n",
    "\n",
    "    print(f\"\\n Avg_RMSE: {avg_rmse} \\n\")\n",
    "    final_forecasts_df = pd.DataFrame(mean_preds)\n",
    "    final_forecasts_df.to_csv(OUTPUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4110bb",
   "metadata": {},
   "source": [
    "LQRA and LQRA Quick (Both tuning and forecasting), also used for ablation study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cplex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from numpy import log, sqrt, sinh\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import statsmodels.api as sm\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cvxpy as cp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ecos\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "load_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/load_error.csv')\n",
    "Solar_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv')\n",
    "WindShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv')\n",
    "WindOffShore_error = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv')\n",
    "\n",
    "def calc_rmse(y_true, y_pred):\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"RMSE: {rmse_val:.3f} €/MWh\")\n",
    "    return rmse_val\n",
    "\n",
    "def pinball_loss(y_true, y_pred, quantiles):\n",
    "   \n",
    "    N, H, Q = y_pred.shape\n",
    "    losses = np.zeros((N, H, Q))\n",
    "    for q_idx, q in enumerate(quantiles):\n",
    "        err = y_true - y_pred[:, :, q_idx]\n",
    "        losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "    return losses\n",
    "\n",
    "def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles):\n",
    "    pinball_losses = pinball_loss(np.array(y_true), Y_pred, np.array(quantiles))\n",
    "    print(f\"Mean Pinball Loss: {np.mean(pinball_losses):.4f}\")\n",
    "    crps = np.mean(np.trapz(pinball_losses, np.array(quantiles), axis=-1))\n",
    "    print(f'Approx CRPS: {crps:.4f}')  \n",
    "    return pinball_losses, np.mean(pinball_losses), crps\n",
    "\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha):\n",
    "    width = y_upper - y_lower\n",
    "    under = (y_lower - y_true) * (y_true < y_lower)\n",
    "    over  = (y_true - y_upper) * (y_true > y_upper)\n",
    "    \n",
    "    score = width + (2.0 / alpha) * (under + over)\n",
    "    return score\n",
    "\n",
    "def calculate_winkler(Y_pred, y_true, quantiles, lower_q, upper_q):\n",
    "    alpha = 1.0 - (upper_q - lower_q)\n",
    "    try:\n",
    "        li = quantiles.index(lower_q)\n",
    "        ui = quantiles.index(upper_q)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"lower_q={lower_q} or upper_q={upper_q} not in quantiles list\")\n",
    "    \n",
    "    y_lower = Y_pred[:, :, li]\n",
    "    y_upper = Y_pred[:, :, ui]\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    scores = winkler_score(y_true, y_lower, y_upper, alpha)\n",
    "    mean_score = scores.mean()\n",
    "    print(f\"Mean Winkler score (α={alpha:.2f}, interval {lower_q}-{upper_q}): {mean_score:.4f}\")\n",
    "    return scores, mean_score\n",
    "\n",
    "def robust_standardize(series):\n",
    "    median = np.median(series)\n",
    "    mad = np.median(np.abs(series - median))\n",
    "    z_075 = 0.6745 \n",
    "    scale = mad / z_075 if mad != 0 else 1e-6\n",
    "    return (series - median) / scale, median, scale\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "\n",
    "    df['hour'] = df['start_datetime'].dt.hour\n",
    "    df['day'] = df['start_datetime'].dt.date\n",
    "    df['dayofweek'] = df['start_datetime'].dt.dayofweek\n",
    "    df['month'] = df['start_datetime'].dt.month\n",
    "    df['is_sat'] = (df['dayofweek'] == 5).astype(int)\n",
    "    df['is_sun'] = (df['dayofweek'] == 6).astype(int)\n",
    "    df['is_mon'] = (df['dayofweek'] == 0).astype(int)\n",
    "\n",
    "    categorical_features = ['hour', 'dayofweek', 'month', 'start_datetime', 'end_datetime', 'Timestamp', 'day', 'is_sat', 'is_sun', 'is_mon']\n",
    "    features_to_scale = [col for col in df.columns if col not in categorical_features]\n",
    "\n",
    "    train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "    val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "    test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "    \n",
    "    test_df_original = test_df.copy()\n",
    "\n",
    "    for feature in features_to_scale:\n",
    "        if feature == 'Price':\n",
    "            x_std, x_median_price, x_scale_price = robust_standardize(train_df[feature])\n",
    "            train_df.loc[:,feature] = np.arcsinh((train_df[feature] - x_median_price) / x_scale_price).copy()\n",
    "            val_df.loc[:,feature] = np.arcsinh((val_df[feature] - x_median_price) / x_scale_price).copy()\n",
    "            test_df.loc[:,feature] = np.arcsinh((test_df[feature] - x_median_price) / x_scale_price).copy()\n",
    "            df.loc[:,feature] = np.arcsinh((df[feature] - x_median_price) / x_scale_price).copy()\n",
    "        else:\n",
    "            x_std, x_median, x_scale = robust_standardize(train_df[feature])\n",
    "            train_df.loc[:,feature] = np.arcsinh((train_df[feature] - x_median) / x_scale).copy()\n",
    "            val_df.loc[:,feature] = np.arcsinh((val_df[feature] - x_median) / x_scale).copy()\n",
    "            test_df.loc[:,feature] = np.arcsinh((test_df[feature] - x_median) / x_scale).copy()\n",
    "            df.loc[:,feature] = np.arcsinh((df[feature] - x_median) / x_scale).copy()\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['month'], drop_first=True)  \n",
    "\n",
    "    return df, train_df, val_df, test_df, test_df_original, features_to_scale, x_median_price, x_scale_price\n",
    "\n",
    "\n",
    "def make_lookup(df):\n",
    "    from collections import defaultdict\n",
    "    lookup = defaultdict(dict)\n",
    "    for _, row in df.iterrows():\n",
    "        lookup[(row['day'], row['hour'])] = row.to_dict()\n",
    "    return lookup\n",
    "\n",
    "def construct_features_day(lookup, df, base_day, horizon, add_noise=True):\n",
    "    np.random.seed(int(base_day.strftime(\"%Y%m%d\")))\n",
    "    feats =[]\n",
    "    for hour in range(horizon):\n",
    "        days_offset = hour // 24\n",
    "        h = hour % 24\n",
    "\n",
    "        d = (base_day + timedelta(days=days_offset))\n",
    "        d1 = (base_day - timedelta(days = 1))\n",
    "        d2 = (base_day - timedelta(days = 2))\n",
    "        d7 = (d - timedelta(days = 7))\n",
    "        \n",
    "      \n",
    "        Z1, Z2, Z3, Z4, Z5, Z6 = [\n",
    "            lookup.get((d, h), {}).get(name, np.nan)\n",
    "            for name in ['Load', 'Solar', 'WindShore', 'WindOffShore', \"Net_Import\", \"Coal_Price\"]\n",
    "            ]\n",
    "        f = [\n",
    "                lookup.get((d1, h), {}).get(\"Price\", np.nan),\n",
    "                lookup.get((d2, h), {}).get(\"Price\", np.nan),\n",
    "                lookup.get((d7, h), {}).get(\"Price\", np.nan),\n",
    "                lookup.get((d1, 23), {}).get(\"Price\", np.nan),\n",
    "                df[df['day']==(d1) ]['Price'].max(),\n",
    "                df[df['day']==(d1) ]['Price'].min(),\n",
    "                Z1, Z2, Z3, Z4, Z5, Z6,\n",
    "                lookup.get((d, h),{}).get(\"is_sat\", np.nan),\n",
    "                lookup.get((d, h),{}).get(\"is_sun\", np.nan),\n",
    "                lookup.get((d, h),{}).get(\"is_mon\", np.nan),\n",
    "                *[lookup.get((d,h), {}).get(f'month_{m}', 0) for m in range(2, 13)]\n",
    "         ]\n",
    "       \n",
    "        feats.append(f)\n",
    "    feats = np.array(feats)\n",
    "    if add_noise:\n",
    "            start = 0\n",
    "            while start < Forecast_Horizon:\n",
    "                i = (start) // 24  \n",
    "                end = start + 24\n",
    "    \n",
    "                feats[start:end, 6] *= np.random.normal(load_error.iloc[0,0]+ 1, load_error.iloc[1,0], size=24)\n",
    "                feats[start:end, 8] *= np.random.normal(WindShore_error.iloc[i, 0] + 1, WindShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 9] *= np.random.normal(WindOffShore_error.iloc[i,0] + 1, WindOffShore_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 7] *= np.random.normal(Solar_error.iloc[i, 0] + 1, Solar_error.iloc[i, 1], size=24)\n",
    "                feats[start:end, 10] *= np.random.normal(load_error.iloc[0,0]+ 1, load_error.iloc[1,0], size=24)\n",
    "                feats[start:end, 11] *= np.random.normal(load_error.iloc[0,0]+ 1, load_error.iloc[1,0], size=24)\n",
    "\n",
    "                start += 24\n",
    "    return feats  \n",
    "\n",
    "def cache_features_targets(df, days, lookup, horizon):\n",
    "    features_day = {}\n",
    "    targets_day = {}\n",
    "    for base_day in tqdm(days, desc=\"Caching features and targets\"):\n",
    "        feats = construct_features_day(lookup, df, base_day, horizon)\n",
    "        features_day[base_day] = feats  \n",
    "        targets = np.array([\n",
    "            lookup.get(((base_day + timedelta(days=i//24)), i%24),{}).get(\"Price\", np.nan)\n",
    "            for i in range(horizon)\n",
    "        ])\n",
    "        targets_day[base_day] = targets\n",
    "\n",
    "    N = len(days)\n",
    "    features_array = np.stack([features_day[d] for d in days])  \n",
    "    targets_array = np.stack([targets_day[d] for d in days])   \n",
    "    return features_array, targets_array\n",
    "\n",
    "\n",
    "def forecast_day_calibration(\n",
    "    features_array_all,\n",
    "    targets_array_all,\n",
    "    all_days,\n",
    "    base_day,\n",
    "    horizon,\n",
    "    calibration_lengths,\n",
    "    median, scale\n",
    "):\n",
    "    results = defaultdict(list)  \n",
    "\n",
    "    forecast_day_idx = all_days.index(base_day)\n",
    "    for h in range(horizon):\n",
    "        preds_for_hour = []\n",
    "        for cal_len in calibration_lengths:\n",
    "            train_end_idx = forecast_day_idx\n",
    "            train_start_idx = max(0, forecast_day_idx - cal_len)\n",
    "\n",
    "            if train_start_idx == train_end_idx:\n",
    "                preds_for_hour.append(None)\n",
    "                continue\n",
    "\n",
    "            X_train = features_array_all[train_start_idx:train_end_idx, h, :]\n",
    "            y_train = targets_array_all[train_start_idx:train_end_idx, h]\n",
    "            x_test = features_array_all[forecast_day_idx, h, :]\n",
    "\n",
    "            X_train = sm.add_constant(X_train, has_constant='add')\n",
    "            x_test = sm.add_constant(x_test.reshape(1, -1), has_constant='add')\n",
    "\n",
    "            model = sm.OLS(y_train, X_train).fit()\n",
    "            y_hat_transformed = model.predict(x_test)[0]\n",
    "\n",
    "            residuals = np.asarray(y_train - model.fittedvalues) \n",
    "\n",
    "            pred_with_residuals = np.sinh(y_hat_transformed + residuals)  \n",
    "            mean_pred = np.mean(pred_with_residuals)\n",
    "            y_hat = median + scale * mean_pred\n",
    "\n",
    "            preds_for_hour.append(y_hat)\n",
    "\n",
    "        results[h] = preds_for_hour\n",
    "\n",
    "    return base_day, results\n",
    "\n",
    "\n",
    "def run_point_forecasting_parallel(\n",
    "    features_array_all,\n",
    "    targets_array_all,\n",
    "    all_days,\n",
    "    forecast_days,\n",
    "    calibration_length,\n",
    "    forecast_horizon,\n",
    "    median,\n",
    "    scale\n",
    "):\n",
    "    final_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                forecast_day_calibration,\n",
    "                features_array_all,\n",
    "                targets_array_all,\n",
    "                all_days,\n",
    "                fd,\n",
    "                forecast_horizon,\n",
    "                calibration_length,\n",
    "                median,\n",
    "                scale\n",
    "            ): fd for fd in forecast_days\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parallel forecasting\"):\n",
    "            try:\n",
    "                fd, day_results = future.result()\n",
    "                for rel_hour, preds in day_results.items():\n",
    "                    final_results[fd][rel_hour] = preds\n",
    "            except Exception as e:\n",
    "                print(f\"Error for forecast day {futures[future]}: {e}\")\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def convert_results_to_array(results, forecast_horizon, calibration_lengths):\n",
    "    test_days_sorted = sorted(results.keys())\n",
    "    N_test_days = len(test_days_sorted)\n",
    "    N_windows = len(calibration_lengths)\n",
    "\n",
    "    forecasts_array = np.empty((N_test_days, forecast_horizon, N_windows))\n",
    "    forecasts_array[:] = np.nan  \n",
    "\n",
    "    for i, day in enumerate(test_days_sorted):\n",
    "        for h in range(forecast_horizon):\n",
    "            preds = results[day].get(h, [np.nan] * N_windows)\n",
    "            if len(preds) != N_windows:\n",
    "                padded_preds = np.full(N_windows, np.nan)\n",
    "                padded_preds[:len(preds)] = preds\n",
    "                preds = padded_preds\n",
    "            forecasts_array[i, h, :] = preds\n",
    "\n",
    "    return forecasts_array, test_days_sorted\n",
    "\n",
    "def get_true_price_array(day_list, targets_array, unique_days, forecast_horizon):\n",
    "    start_idx = unique_days.index(day_list[0].date())\n",
    "    end_idx = start_idx + len(day_list)\n",
    "    true_prices = targets_array[start_idx:end_idx]\n",
    "    return np.array(true_prices) \n",
    "\n",
    "\n",
    "def compute_bic(y_true, y_pred, n_params, n_obs):\n",
    "    residuals = y_true - y_pred\n",
    "    loss = np.mean(np.abs(residuals)) \n",
    "    bic = n_obs * np.log(loss + 1e-8) + n_params * np.log(n_obs)\n",
    "    return bic\n",
    "\n",
    "def fit_lqra_quantile_bic(X, y, alpha, lambda_grid):\n",
    "    best_bic = np.inf\n",
    "    best_beta = None\n",
    "    best_lambda = None\n",
    "    n_obs, n_models = X.shape\n",
    "\n",
    "    for lam in lambda_grid:\n",
    "        beta = cp.Variable(n_models)\n",
    "        residuals = y - X @ beta\n",
    "        quantile_loss = cp.sum(cp.maximum(alpha * residuals, (alpha - 1) * residuals))\n",
    "        l1_penalty = lam * cp.norm1(beta)\n",
    "        problem = cp.Problem(cp.Minimize(quantile_loss + l1_penalty))\n",
    "        problem.solve(solver=cp.CPLEX)\n",
    "\n",
    "        if beta.value is not None:\n",
    "            y_hat = X @ beta.value\n",
    "            bic = compute_bic(y, y_hat, np.count_nonzero(beta.value), n_obs)\n",
    "            if bic < best_bic:\n",
    "                best_bic = bic\n",
    "                best_beta = beta.value\n",
    "                best_lambda = lam\n",
    "\n",
    "    return best_beta, best_lambda\n",
    "\n",
    "\n",
    "def rolling_lqra_bic_forecasting_hourly_lambda(\n",
    "    forecast_array, true_array, quantile_levels, all_days, test_days, calibration_window\n",
    "):\n",
    "\n",
    "    n_total_days, horizon, n_models = forecast_array.shape\n",
    "    n_quantiles = len(quantile_levels)\n",
    "    lambda_grid = np.logspace(-1, 3, 19) #10 values for quick version\n",
    " \n",
    "    Q_preds = np.full((len(test_days), horizon, n_quantiles), np.nan)\n",
    "    all_param = np.empty((len(test_days), horizon, n_quantiles), dtype=object)\n",
    "    lambda_per_h = np.full((horizon, n_quantiles), np.nan)  \n",
    "\n",
    "    print(\"Tuning λ once for each hour (shared across quantiles)...\")\n",
    "    first_test_idx = all_days.index(test_days[0].date())\n",
    "    train_start = first_test_idx - calibration_window\n",
    "    train_end = first_test_idx\n",
    "\n",
    "    for h in range(24):\n",
    "        X_train = forecast_array[train_start:train_end, h, :]\n",
    "        y_train = true_array[train_start:train_end, h]\n",
    "\n",
    "        for q_idx, alpha in enumerate(tqdm(quantile_levels, desc=f\"Tuning quantiles for hour {h}\")):\n",
    "            _, best_lam = fit_lqra_quantile_bic(X_train, y_train, alpha, lambda_grid)\n",
    "            lambda_per_h[h, q_idx] = best_lam\n",
    "\n",
    "    test_indices = [all_days.index(day.date()) for day in test_days]\n",
    "\n",
    "    def process_day(i, test_idx):\n",
    "        Q_pred_day = np.full((horizon, n_quantiles), np.nan)\n",
    "        param_day = np.empty((horizon, n_quantiles), dtype=object)\n",
    "\n",
    "        train_start = test_idx - calibration_window\n",
    "        train_end = test_idx\n",
    "\n",
    "        for hour in range(horizon):\n",
    "            h = hour % 24\n",
    "            X_train = forecast_array[train_start:train_end, hour, :]\n",
    "            y_train = true_array[train_start:train_end, hour]\n",
    "            X_test = forecast_array[test_idx, hour, :]\n",
    "\n",
    "            results = [\n",
    "                fit_lqra_quantile_bic(X_train, y_train, alpha, [lambda_per_h[h, q_idx]])\n",
    "                for q_idx, alpha in enumerate(quantile_levels)\n",
    "            ]\n",
    "\n",
    "            for q_idx, (beta, _) in enumerate(results):\n",
    "                if beta is not None:\n",
    "                    Q_pred_day[hour, q_idx] = np.dot(X_test, beta)\n",
    "                    param_day[hour, q_idx] = (beta, lambda_per_h[h, q_idx])\n",
    "\n",
    "        return i, Q_pred_day, param_day\n",
    "\n",
    "    print(\"Rolling LQRA forecasts (parallel)...\")\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_day)(i, test_idx)\n",
    "        for i, test_idx in enumerate(tqdm(test_indices))\n",
    "    )\n",
    "\n",
    "    for i, Q_pred_day, param_day in results:\n",
    "        Q_preds[i] = Q_pred_day\n",
    "        all_param[i] = param_day\n",
    "\n",
    "    return Q_preds, all_param \n",
    "\n",
    "\n",
    "\n",
    "Forecast_Horizon = 48 # 48 or 144\n",
    "FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "train_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "val_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "train_dates = train_dates['date'].tolist()\n",
    "val_dates = val_dates['date'].tolist()\n",
    "test_dates = test_dates['date'].tolist()\n",
    "\n",
    "\n",
    "df, train_df, val_df, test_df, test_df_original, features_to_scale, median, scale = load_data(FILE_PATH)\n",
    "print(features_to_scale)\n",
    "unique_days = sorted(df['start_datetime'].dt.date.unique())[7:]\n",
    "calibration_lengths = range(50, 350, 15) #(50, 225, 15) for quick version\n",
    "quantiles = np.arange(0.01, 1.00, 0.01).round(2).tolist()\n",
    "\n",
    "lookup = make_lookup(df)\n",
    "features_array, targets_array_scaled = cache_features_targets(df, unique_days, lookup, Forecast_Horizon)\n",
    "y_true= get_true_price_array(test_dates, targets_array_scaled, unique_days, Forecast_Horizon)\n",
    "y_true_std= np.sinh(y_true) \n",
    "y_true = y_true_std * scale + median\n",
    "\n",
    "results = run_point_forecasting_parallel(features_array,targets_array_scaled, unique_days, unique_days, calibration_lengths,Forecast_Horizon, median, scale)\n",
    "\n",
    "forecast_array, val_days_sorted = convert_results_to_array(results, Forecast_Horizon, list(calibration_lengths))\n",
    "\n",
    "targets_array_std = np.sinh(targets_array_scaled) \n",
    "targets_array = targets_array_std* scale + median\n",
    "Q_preds, all_params = rolling_lqra_bic_forecasting_hourly_lambda(\n",
    "    forecast_array, targets_array, quantiles, unique_days, test_dates, calibration_window =364 #175 for quick version)\n",
    ")\n",
    "print(Q_preds.shape)\n",
    "toSave = pd.DataFrame(Q_preds.reshape(Q_preds.shape[0], -1))\n",
    "Forecast_Horizon_name = Forecast_Horizon + 24\n",
    "toSave.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/LQRA_prob_forecast_{Forecast_Horizon_name}.csv\", index=False)\n",
    "\n",
    "midquantile = Q_preds[:, :, quantiles.index(0.5)].round(2)\n",
    "\n",
    "midquantile = pd.DataFrame(midquantile)\n",
    "#midquantile.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Attempt_LQRA_point_test_{Forecast_Horizon_name}.csv\", index=False)\n",
    "\n",
    "rmse_val = calc_rmse(y_true, midquantile)\n",
    "pinball_losses, pinball_loss_mean, crps = calculate_pinball_loss_CRPS(y_true, Q_preds, quantiles)\n",
    "interval_scores, mean_interval_score = calculate_winkler(Q_preds, y_true, quantiles, lower_q = 0.4, upper_q=0.6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e3c62",
   "metadata": {},
   "source": [
    "Generating true electricity prices in the right format for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "FILE_PATH = 'C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv'\n",
    "df = pd.read_csv(FILE_PATH, parse_dates=['start_datetime', 'end_datetime'])\n",
    "\n",
    "df['hour'] = df['start_datetime'].dt.hour\n",
    "df['day'] = df['start_datetime'].dt.date\n",
    "\n",
    "test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "\n",
    "test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "test_dates = test_dates['date'].tolist()\n",
    "test_dates = [d - timedelta(days=1) for d in test_dates] # Adjust to match the day in test_dates for h = 24, 72 or 168\n",
    "\n",
    "def get_true_price_array(day_list, targets_array, forecast_horizon):\n",
    "    true_prices = []\n",
    "    for d in day_list:\n",
    "        mask = targets_array['start_datetime'] == d\n",
    "        start_idx = np.where(mask)[0][0]\n",
    "        end_idx = start_idx + forecast_horizon\n",
    "        daily_prices = targets_array[start_idx:end_idx]['Price'].values\n",
    "        true_prices.append(daily_prices.reshape(-1))  \n",
    "    return np.array(true_prices) \n",
    "\n",
    "for forecast_horizon in [24,72, 168]: #or 48, 144\n",
    "    y_true = get_true_price_array(test_dates, test_df, forecast_horizon= forecast_horizon)\n",
    "    y_true = pd.DataFrame(y_true, columns=[f'h+{i+1}' for i in range(forecast_horizon)])\n",
    "    y_true.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/Values_for_evaluation_{forecast_horizon}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258d0d3",
   "metadata": {},
   "source": [
    "Deterministic Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c23a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cplex \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "E_max = 75           \n",
    "range_km = 400       \n",
    "c_max = 11          \n",
    "eta = 0.95          \n",
    "SoC_min = 0.2 * E_max        \n",
    "SoC_max = 0.8 * E_max       \n",
    "energy_per_km = E_max / range_km  \n",
    "\n",
    "def get_trip_schedule(day_type, day_index=None):\n",
    "    if day_type < 5:\n",
    "        return [\n",
    "            {\"hour\": 8, \"duration\": 9, \"distance\": 50, \"prob\": 1.0},\n",
    "            {\"hour\": 18, \"duration\": 3, \"distance\": 30, \"prob\": 0.2}\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            {\"hour\": 14, \"duration\": 4, \"distance\": 60, \"prob\": 0.5}\n",
    "        ]\n",
    "  \n",
    "\n",
    "def create_availability_and_energy_matrix(day_of_week, horizon=24):\n",
    "    availability = np.ones(horizon, dtype=bool) \n",
    "    energy = np.zeros(horizon) \n",
    "    trips = get_trip_schedule(day_of_week)\n",
    "\n",
    "    for trip in trips:\n",
    "        if np.random.rand() < trip.get(\"prob\", 1.0):\n",
    "            dep = trip[\"hour\"]\n",
    "            dur = trip[\"duration\"]\n",
    "            distance = trip[\"distance\"]\n",
    "            energy_trip = distance * energy_per_km  \n",
    "            for h in range(dep, dep + dur):\n",
    "                availability[h] = False\n",
    "            energy[dep] = energy_trip\n",
    "    return availability, energy\n",
    "\n",
    "def get_trip_energy(day_type):\n",
    "    trips = get_trip_schedule(day_type)\n",
    "    energy = 0\n",
    "    for trip in trips:\n",
    "        if np.random.rand() < trip.get(\"prob\", 1.0):\n",
    "            energy += trip[\"distance\"] * energy_per_km\n",
    "    return energy\n",
    "\n",
    "def get_night_charging_hours(forecasted, hours_to_select=3):\n",
    "    night_charging_mask = np.zeros_like(forecasted, dtype=bool)\n",
    "    num_days = len(forecasted) // 24\n",
    "\n",
    "    for day in range(num_days):\n",
    "        start = day * 24\n",
    "\n",
    "        night_indices = []\n",
    "        if day > 0:\n",
    "            night_indices += [((day - 1) * 24 + h) for h in [22, 23]]\n",
    "        night_indices += [(start + h) for h in range(7)]\n",
    "        night_indices = [i for i in night_indices if i < len(forecasted)]\n",
    "\n",
    "        night_prices = [(i, forecasted[i]) for i in night_indices]\n",
    "        lowest_hours = sorted(night_prices, key=lambda x: x[1])[:hours_to_select]\n",
    "\n",
    "        for idx, _ in lowest_hours:\n",
    "            night_charging_mask[idx] = True\n",
    "\n",
    "    return night_charging_mask\n",
    "\n",
    "def simulate_strategy(strategy, availability, trip_energy, State_of_Charge, Charging, Y_true):\n",
    "    charging_strategy = np.zeros_like(Y_true)\n",
    "    SoC = np.zeros_like(Y_true)\n",
    "    State = State_of_Charge \n",
    "    charging = Charging\n",
    "    night_charging_mask = get_night_charging_hours(Y_true)\n",
    "   \n",
    "    for t in range(len(Y_true)):\n",
    "        if not availability[t]:  \n",
    "            charging = 0  \n",
    "            State = State - trip_energy[t]  \n",
    "            if State < SoC_min:\n",
    "                print(f\"Warning: SoC below minimum at time {t}, current SoC: {State} on day {t} for strategy = {strategy}.\")\n",
    "\n",
    "        can_charge = availability[t]\n",
    "\n",
    "        if strategy == \"default\" and can_charge:\n",
    "            if State < SoC_max:\n",
    "                charging = min(c_max, (SoC_max - State)/eta)  \n",
    "                State = State + charging * eta \n",
    "            else:\n",
    "                charging = 0  \n",
    "\n",
    "        elif strategy == \"rule\" and can_charge: \n",
    "            if State < 0.5 * SoC_max or charging != 0:  \n",
    "                charging = min(c_max, (SoC_max - State)/eta)  \n",
    "                State = State + charging * eta  \n",
    "            else:\n",
    "                charging = 0\n",
    "        \n",
    "        elif strategy == \"night\" and can_charge:\n",
    "            if night_charging_mask[t] and State < SoC_max:\n",
    "                charging = min(c_max, (SoC_max - State) / eta)\n",
    "                State += charging * eta\n",
    "            else:\n",
    "                charging = 0\n",
    "\n",
    "        charging_strategy[t] = charging\n",
    "        SoC[t]= State\n",
    "\n",
    "    return charging_strategy, SoC\n",
    "\n",
    "def compute_cost(Charging_Strategy, prices):\n",
    "    prices_in_KWh_excl_tax = prices / 1000  \n",
    "    prices_in_KWh_incl_tax = prices/1000 +  0.10154\n",
    "    return np.sum(Charging_Strategy * prices_in_KWh_excl_tax), np.sum(Charging_Strategy*prices_in_KWh_incl_tax)\n",
    "\n",
    "\n",
    "def load_and_prepare_data(Forecast_Horizon, model, true_values_24):\n",
    "    horizon_file = Forecast_Horizon -24\n",
    "    y_pred = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/{model}_forecasts_{horizon_file}.csv\")\n",
    "    Y_pred = pd.concat([true_values_24, y_pred], axis=1)\n",
    "    return Y_pred\n",
    "\n",
    "def load_and_prepare_quantile_data(quantile, probability_forecasts, true_values_24, all_quantiles):\n",
    "    quantile_pred = pd.DataFrame(probability_forecasts[:, :, all_quantiles.index(quantile)])\n",
    "    Y_pred = pd.concat([true_values_24, quantile_pred], axis=1)\n",
    "    return Y_pred\n",
    "\n",
    "def create_availability_and_energy_matrices(test_dates, Forecast_Horizon,  create_matrices):\n",
    "    if create_matrices:\n",
    "    \n",
    "        Y_available = []\n",
    "        Y_energy_trip = []\n",
    "        extra = test_dates.iloc[-7:].apply(lambda d: d + timedelta(days=7))\n",
    "\n",
    "        extra_days = pd.concat([test_dates, extra], ignore_index=True)\n",
    "\n",
    "        for start_day in extra_days:\n",
    "            availability, energy = create_availability_and_energy_matrix(day_of_week=start_day.dayofweek, horizon=24)\n",
    "            Y_available.append(pd.DataFrame([[start_day] + availability.tolist()]))\n",
    "            Y_energy_trip.append(pd.DataFrame([[start_day] + energy.tolist()]))\n",
    "\n",
    "        Y_available_df = pd.concat(Y_available, ignore_index=True)\n",
    "        Y_available_df.columns = ['start_datetime'] + [f\"t+{i}\" for i in range(24)]\n",
    "        Y_energy_trip_df = pd.concat(Y_energy_trip, ignore_index=True)\n",
    "        Y_energy_trip_df.columns = ['start_datetime'] + [f\"t+{i}\" for i in range(24)]\n",
    "\n",
    "        for horizon in [24, 72, 168]:\n",
    "            print(f\"Processing horizon: {horizon}\")\n",
    "            Y_available = []\n",
    "            Y_energy_trip = []\n",
    "\n",
    "            for start_day in test_dates:\n",
    "                avail =[]\n",
    "                ener =[]\n",
    "                for i in range(0, 1 if horizon == 24 else (3 if horizon == 72 else 7)): \n",
    "                    next_day = start_day + timedelta(days=i)\n",
    "                    availability = Y_available_df[Y_available_df['start_datetime'] == next_day].iloc[:, 1:].values[0]\n",
    "                    avail.extend(availability)\n",
    "                    energy = Y_energy_trip_df[Y_energy_trip_df['start_datetime'] == next_day].iloc[:, 1:].values[0]\n",
    "                    ener.extend(energy)\n",
    "\n",
    "                Y_available.append(pd.DataFrame([[start_day] + avail]))\n",
    "                Y_energy_trip.append(pd.DataFrame([[start_day] + ener]))\n",
    "    \n",
    "            Y_available = pd.concat(Y_available, ignore_index=True)\n",
    "            Y_available.columns = ['start_datetime'] + [f\"t+{i}\" for i in range(horizon)]\n",
    "            Y_energy_trip = pd.concat(Y_energy_trip, ignore_index=True)\n",
    "            Y_energy_trip.columns = ['start_datetime'] + [f\"t+{i}\" for i in range(horizon)]\n",
    "            Y_available.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_available_{horizon}.csv\", index=False)\n",
    "            Y_energy_trip.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_energy_trip_{horizon}.csv\", index=False) \n",
    "    else:\n",
    "        Y_available = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_available_{Forecast_Horizon}.csv\", parse_dates=['start_datetime'])\n",
    "        Y_energy_trip = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_energy_trip_{Forecast_Horizon}.csv\", parse_dates=['start_datetime'])\n",
    "\n",
    "    return Y_available, Y_energy_trip\n",
    "\n",
    "def load_all_benchmark_strategies(strategies, Forecast_Horizon, Y_true, Y_available, Y_energy_trip):\n",
    "    results = []\n",
    "    State = SoC_max\n",
    "    charging = 0\n",
    "\n",
    "    for strategy in strategies:\n",
    "        full_charging_strategy = []\n",
    "        full_soc = []\n",
    "        for i in tqdm(range(Y_available.shape[0])):  \n",
    "            true_prices = Y_true.iloc[i,].values[:Forecast_Horizon]\n",
    "            availability = Y_available.iloc[i,1:].values[:Forecast_Horizon].astype(bool)\n",
    "            energy_trip = Y_energy_trip.iloc[i,1:].values[:Forecast_Horizon]\n",
    "\n",
    "            charging_strategy, SoC = simulate_strategy(strategy=strategy, availability=availability, trip_energy=energy_trip, State_of_Charge=State, Charging=charging, Y_true = true_prices)\n",
    "            State = SoC[23]\n",
    "            charging = charging_strategy[23]\n",
    "\n",
    "            full_charging_strategy.append(charging_strategy)\n",
    "            full_soc.append(SoC)\n",
    "                \n",
    "            cost_excl_tax, cost_incl_tax = compute_cost(charging_strategy[:24], true_prices[:24])\n",
    "\n",
    "            results.append({\n",
    "                \"sample\": i,\n",
    "                \"strategy\": strategy,\n",
    "                \"cost_excl_tax\": cost_excl_tax,\n",
    "                \"cost_incl_tax\": cost_incl_tax\n",
    "                })\n",
    "\n",
    "        pd.DataFrame(full_charging_strategy).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Charging/{strategy}_strategy_charge_schedule.csv\")\n",
    "        pd.DataFrame(full_soc).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/SoC/{strategy}_strategy_SoC.csv\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    summary = results_df.groupby(\"strategy\").agg(\n",
    "    total_cost_excl_tax=(\"cost_excl_tax\", \"sum\"),\n",
    "    daily_mean_excl_tax=(\"cost_excl_tax\", \"mean\"),\n",
    "    daily_std_excl_tax=(\"cost_excl_tax\", \"std\"),\n",
    "    total_cost_incl_tax=(\"cost_incl_tax\", \"sum\"),\n",
    "    daily_mean_incl_tax=(\"cost_incl_tax\", \"mean\"),\n",
    "    daily_std_incl_tax=(\"cost_incl_tax\", \"std\"),\n",
    "    )\n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "def optimize_ev_charging(P, availability, trip_energy, SoC_init):\n",
    "    H = len(P)  \n",
    "    P = P/1000 \n",
    "\n",
    "    charging = cp.Variable(H)\n",
    "    soc = cp.Variable(H + 1)  \n",
    "\n",
    "    constraints = [soc[0] == SoC_init]\n",
    "\n",
    "    for t in range(H):\n",
    "        if not availability[t]:\n",
    "            constraints += [charging[t] == 0]\n",
    "        else:\n",
    "            constraints += [\n",
    "                charging[t] >= 0,\n",
    "                charging[t] <= c_max,  \n",
    "            ]\n",
    "\n",
    "        constraints += [\n",
    "            soc[t] >= SoC_min,\n",
    "            soc[t] <= SoC_max,\n",
    "        ]\n",
    "\n",
    "        if trip_energy[t] > 0:\n",
    "            constraints += [\n",
    "                soc[t] >= trip_energy[t] + SoC_min,\n",
    "                soc[t+1] == soc[t] - trip_energy[t]\n",
    "            ]\n",
    "        else:\n",
    "            constraints += [\n",
    "                soc[t+1] == soc[t] + eta * charging[t]]\n",
    "\n",
    "    objective = cp.Minimize(cp.sum(cp.multiply(P, charging)))\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver = cp.CPLEX)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Optimization failed with status: {problem.status}\")\n",
    "        return None, None\n",
    "\n",
    "    soc.value = np.round(soc.value, decimals=3)\n",
    "    charging.value[np.abs(charging.value) < 1e-3] = 0\n",
    "\n",
    "    return charging.value , soc.value[1:]\n",
    "\n",
    "def optimize_ev_charging_twostage(P_prob, availability, trip_energy, SoC_init):\n",
    "    import cvxpy as cp\n",
    "    import numpy as np\n",
    "\n",
    "    P_prob = P_prob / 1000  \n",
    "\n",
    "    H = len(P_prob)\n",
    "    charging_1 = cp.Variable(24)\n",
    "    soc_1 = cp.Variable(25)\n",
    "\n",
    "    hours_stage2 = H - 24\n",
    "    num_days_stage2 = hours_stage2 // 24\n",
    "    charging_shared = [cp.Variable(24) for _ in range(num_days_stage2)]\n",
    "    soc_shared = [cp.Variable(25) for _ in range(num_days_stage2)]\n",
    "\n",
    "    constraints = []\n",
    "    constraints.append(soc_1[0] == SoC_init)\n",
    "\n",
    "    for t in range(24):\n",
    "        if not availability[t]:\n",
    "            constraints.append(charging_1[t] == 0)\n",
    "        else:\n",
    "            constraints += [0 <= charging_1[t], charging_1[t] <= c_max]\n",
    "        constraints += [SoC_min <= soc_1[t], soc_1[t] <= SoC_max]\n",
    "        if trip_energy[t] > 0:\n",
    "            constraints += [\n",
    "                soc_1[t] >= trip_energy[t] + SoC_min,\n",
    "                soc_1[t + 1] == soc_1[t] - trip_energy[t]\n",
    "            ]\n",
    "        else:\n",
    "            constraints.append(soc_1[t + 1] == soc_1[t] + eta * charging_1[t])\n",
    "\n",
    "    constraints.append(soc_shared[0][0] == soc_1[24])\n",
    "\n",
    "    for d in range(num_days_stage2):\n",
    "        c = charging_shared[d]\n",
    "        s = soc_shared[d]\n",
    "        t_global = 24 + d * 24  \n",
    "\n",
    "        if d > 0:\n",
    "            constraints.append(s[0] == soc_shared[d - 1][24])  \n",
    "\n",
    "        for h in range(24):\n",
    "            t_abs = t_global + h\n",
    "            if not availability[t_abs]:\n",
    "                constraints.append(c[h] == 0)\n",
    "            else:\n",
    "                constraints += [0 <= c[h], c[h] <= c_max]\n",
    "\n",
    "            constraints += [SoC_min <= s[h], s[h] <= SoC_max]\n",
    "\n",
    "            if trip_energy[t_abs] > 0:\n",
    "                constraints += [\n",
    "                    s[h] >= trip_energy[t_abs] + SoC_min,\n",
    "                    s[h + 1] == s[h] - trip_energy[t_abs]\n",
    "                ]\n",
    "            else:\n",
    "                constraints.append(s[h + 1] == s[h] + eta * c[h])\n",
    "\n",
    "    cost_day1 = cp.sum(cp.multiply(P_prob[:24], charging_1))\n",
    "\n",
    "    cost_stage2 = 0\n",
    "    for d in range(num_days_stage2):\n",
    "        P_day_d = P_prob[(d+1) * 24:(d + 2) * 24] \n",
    "        cost_stage2 += cp.sum(cp.multiply(P_day_d, charging_shared[d]))\n",
    "\n",
    "    total_cost = cost_day1 + cost_stage2\n",
    "\n",
    "    problem = cp.Problem(cp.Minimize(total_cost), constraints)\n",
    "    problem.solve(solver=cp.CPLEX, verbose=False)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Optimisation failed: {problem.status}\")\n",
    "        return None, None\n",
    "\n",
    "    charging_total = [charging_1.value] + [c.value for c in charging_shared]\n",
    "    soc_total = [soc_1.value[1:]] + [s.value[1:] for s in soc_shared]\n",
    "\n",
    "    charging_total = np.concatenate(charging_total)\n",
    "    soc_total = np.concatenate(soc_total)\n",
    "\n",
    "    charging_total[np.abs(charging_total) < 1e-3] = 0\n",
    "    soc_total = np.round(soc_total, 3)\n",
    "\n",
    "    return charging_total, soc_total\n",
    "\n",
    "\n",
    "def load_optimization_point_forecasts(Forecast_Horizon, Y_true, Y_pred, Y_available, Y_energy_trip, model):\n",
    "    State = SoC_max\n",
    "    charging = 0\n",
    "    Strategy_optimization = []\n",
    "    SoC_optimization = []\n",
    "    results = []\n",
    "    for i in tqdm(range(Y_pred.shape[0])): \n",
    "        forecast_prices = Y_pred.iloc[i,].values[:Forecast_Horizon]\n",
    "        true_prices = Y_true.iloc[i, ].values[:Forecast_Horizon]\n",
    "        availability = Y_available.iloc[i, 1:].values[:Forecast_Horizon].astype(bool)\n",
    "        energy_trip = Y_energy_trip.iloc[i, 1:].values[:Forecast_Horizon]\n",
    "\n",
    "        if Forecast_Horizon > 24:    \n",
    "            charging_strategy, SoC = optimize_ev_charging_twostage(forecast_prices, availability, energy_trip, State)\n",
    "        else:\n",
    "            charging_strategy, SoC = optimize_ev_charging(forecast_prices, availability, energy_trip, State)\n",
    "        State = SoC[23]\n",
    "        Strategy_optimization.append(charging_strategy)\n",
    "        SoC_optimization.append(SoC)\n",
    "\n",
    "        cost_excl_tax, cost_incl_tax = compute_cost(charging_strategy[:24], true_prices[:24])\n",
    "\n",
    "        results.append({\n",
    "                    \"sample\": i,\n",
    "                    \"strategy\": model,\n",
    "                    \"horizon\": Forecast_Horizon,\n",
    "                    \"cost_excl_tax\": cost_excl_tax,\n",
    "                    \"cost_incl_tax\": cost_incl_tax\n",
    "                    })\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    summary = results_df.groupby(\"strategy\").agg(\n",
    "    total_cost_excl_tax=(\"cost_excl_tax\", \"sum\"),\n",
    "    daily_mean_excl_tax=(\"cost_excl_tax\", \"mean\"),\n",
    "    daily_std_excl_tax=(\"cost_excl_tax\", \"std\"),\n",
    "    total_cost_incl_tax=(\"cost_incl_tax\", \"sum\"),\n",
    "    daily_mean_incl_tax=(\"cost_incl_tax\", \"mean\"),\n",
    "    daily_std_incl_tax=(\"cost_incl_tax\", \"std\"),\n",
    "    )\n",
    "    pd.DataFrame(Strategy_optimization).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Charging/{model}_charge_schedule_{Forecast_Horizon}.csv\")\n",
    "    pd.DataFrame(SoC_optimization).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/SoC/{model}_SoC_{Forecast_Horizon}.csv\")\n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "true_values_24 = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/True_values_24.csv\")\n",
    "test_dates = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', parse_dates=['date']).sort_values(by='date')\n",
    "test_dates = test_dates['date']\n",
    "test_dates = [d - timedelta(days=1) for d in test_dates] \n",
    "test_dates = pd.Series(test_dates, name='start_datetime')\n",
    "\n",
    "create_availability_and_energy_matrices(test_dates, Forecast_Horizon = None, create_matrices = True)\n",
    "Y_available24 = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_available_24.csv\", parse_dates=['start_datetime'])\n",
    "Y_energy_trip24 = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_energy_trip_24.csv\", parse_dates=['start_datetime'])\n",
    "strategies = [\"default\", \"rule\", \"night\"] \n",
    "results_strategy, summary_strategy = load_all_benchmark_strategies(strategies, Forecast_Horizon = 24, Y_true=true_values_24, Y_available = Y_available24, Y_energy_trip = Y_energy_trip24)\n",
    "results24, summary24 = load_optimization_point_forecasts(Forecast_Horizon=24, Y_true = true_values_24, Y_pred=true_values_24, Y_available= Y_available24, Y_energy_trip = Y_energy_trip24, model =\"24hours\")\n",
    "pd.concat([results_strategy, results24], ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Benchmarks_charging_cost_eachday.csv\")\n",
    "pd.concat([summary_strategy, summary24], ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Benchmarks_charging_cost_average.csv\")\n",
    "print(pd.concat([summary_strategy, summary24], ignore_index=False))\n",
    "\n",
    "for Forecast_Horizon in [72, 168]: \n",
    "    all_result = []\n",
    "    all_summary = []\n",
    "    true_values = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/True_values_{Forecast_Horizon}.csv\")\n",
    "    Y_available, Y_energy_trip = create_availability_and_energy_matrices(test_dates, Forecast_Horizon=Forecast_Horizon, create_matrices = False)\n",
    "    list_names = ['Arima', 'GJR_Garch', 'Garch', 'Hybrid_LSTM_lgbm', 'Hybrid_LSTM_xgb', \"Hybrid_weighted_AllThree\", 'Hybrid_weighted_LSTM_lgbm', 'Hybrid_weighted_LSTM_xgb', \"LGBM\", 'LSTM', \"XGB\", \"DNN\", \"Full_info\"]\n",
    "    \n",
    "    for model in list_names:\n",
    "        if model == 'Full_info':\n",
    "           Y_pred = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/True_values_{Forecast_Horizon}.csv\")\n",
    "        else:\n",
    "            Y_pred = load_and_prepare_data(Forecast_Horizon, model, true_values_24)\n",
    "        result, summary = load_optimization_point_forecasts(Forecast_Horizon, true_values, Y_pred, Y_available, Y_energy_trip, model)\n",
    "        all_result.append(result)\n",
    "        all_summary.append(summary)\n",
    "    pd.concat(all_result, ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Optimization_charging_cost_eachday_{Forecast_Horizon}.csv\")\n",
    "    pd.concat(all_summary, ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Optimization_charging_cost_average_{Forecast_Horizon}.csv\")\n",
    "    print(pd.concat(all_summary, ignore_index=False))\n",
    "    \n",
    "    model_names = ['DDNN', 'DDNN_mixture', 'Regular_LQRA', 'Very_quick_LQRA']\n",
    "    for name in  model_names:\n",
    "        horizon = Forecast_Horizon - 24\n",
    "        prob = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Probability/{name}_prob_forecasts_{horizon}.csv\").values.reshape(290, horizon, 99)\n",
    "        all_results = []\n",
    "        all_summary = []\n",
    "        all_quantiles = np.arange(0.01, 1.00, 0.01).round(2).tolist()\n",
    "        for quantile in [0.5]:\n",
    "            y_pred_quantile = load_and_prepare_quantile_data(quantile, prob, true_values_24, all_quantiles)\n",
    "            result, summary = load_optimization_point_forecasts(Forecast_Horizon, true_values, y_pred_quantile, Y_available, Y_energy_trip, f\"{name}_{quantile}\")\n",
    "            all_results.append(result)\n",
    "            all_summary.append(summary)\n",
    "        pd.concat(all_results, ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/{name}_quantiles_charging_cost_eachday_{Forecast_Horizon}.csv\")\n",
    "        pd.concat(all_summary, ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/{name}_quantiles_charging_cost_average_{Forecast_Horizon}.csv\")\n",
    "        print(pd.concat(all_summary, ignore_index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b18a8",
   "metadata": {},
   "source": [
    "Stochastic optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cplex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "E_max = 75          \n",
    "range_km = 400     \n",
    "c_max = 11          \n",
    "eta = 0.95         \n",
    "SoC_min = 0.2 * E_max       \n",
    "SoC_max = 0.8 * E_max       \n",
    "energy_per_km = E_max / range_km  \n",
    "\n",
    "\n",
    "def compute_cost(Charging_Strategy, prices):\n",
    "    prices_in_KWh_excl_tax = prices / 1000  \n",
    "    prices_in_KWh_incl_tax = prices/1000 + 0.10154\n",
    "    return np.sum(Charging_Strategy * prices_in_KWh_excl_tax), np.sum(Charging_Strategy*prices_in_KWh_incl_tax)\n",
    "\n",
    "\n",
    "def load_availability_and_energy_matrices(Forecast_Horizon):\n",
    "    Y_available = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_available_{Forecast_Horizon}.csv\", parse_dates=['start_datetime'])\n",
    "    Y_energy_trip = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Y_energy_trip_{Forecast_Horizon}.csv\", parse_dates=['start_datetime'])\n",
    "    return Y_available, Y_energy_trip\n",
    "\n",
    "\n",
    "def optimize_ev_charging_stochastic(Forecast_24h, Forecast_prob, availability, trip_energy, SoC_init, weights, Forecast_Horizon):\n",
    "    Forecast_24h = Forecast_24h / 1000  \n",
    "    Forecast_prob = Forecast_prob / 1000\n",
    "\n",
    "    Q = len(weights)  \n",
    "    T = Forecast_Horizon \n",
    "    W = weights\n",
    "\n",
    "    charging = cp.Variable(T)       \n",
    "    soc = cp.Variable(T + 1)         \n",
    "\n",
    "    constraints = [soc[0] == SoC_init]\n",
    "\n",
    "    for t in range(T):\n",
    "        if not availability[t]:\n",
    "            constraints.append(charging[t] == 0)\n",
    "        else:\n",
    "            constraints.append(charging[t] >= 0)\n",
    "            constraints.append(charging[t] <= c_max)\n",
    "\n",
    "        constraints.append(soc[t] >= SoC_min)\n",
    "        constraints.append(soc[t] <= SoC_max)\n",
    "\n",
    "        if trip_energy[t] > 0:\n",
    "            constraints.append(soc[t] >= trip_energy[t] + SoC_min)\n",
    "            constraints.append(soc[t + 1] == soc[t] - trip_energy[t])\n",
    "        else:\n",
    "            constraints.append(soc[t + 1] == soc[t] + eta * charging[t])\n",
    "\n",
    "    cost_day1 = cp.sum(cp.multiply(Forecast_24h, charging[:24]))\n",
    "\n",
    "    cost_after24h = 0\n",
    "    for day in range((Forecast_Horizon-24)//24):\n",
    "        for q in range(Q):\n",
    "            scenario_prices = Forecast_prob[day*24:(day+1)*24, q]\n",
    "            cost_q = cp.sum(cp.multiply(scenario_prices, charging[(day+1)*24:(day+2)*24])) \n",
    "            cost_after24h += W[q] * cost_q\n",
    "    \n",
    "    total_cost = cost_day1 + cost_after24h\n",
    "    problem = cp.Problem(cp.Minimize(total_cost), constraints)\n",
    "    problem.solve(solver=cp.CPLEX)\n",
    "\n",
    "    if problem.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "        print(f\"Optimization failed with status: {problem.status}\")\n",
    "        return None, None\n",
    "\n",
    "    charging_opt = charging.value\n",
    "    soc_opt = soc.value[1:] \n",
    "\n",
    "    charging_opt[np.abs(charging_opt) < 1e-3] = 0\n",
    "    soc_opt = np.round(soc_opt, decimals=3)\n",
    "\n",
    "    return charging_opt, soc_opt\n",
    "\n",
    "\n",
    "def load_optimization_prob_forecasts(Y_pred_prob, true_values_24, true_values, weights, Forecast_Horizon, model):\n",
    "    State = SoC_max\n",
    "    charging = 0\n",
    "    Strategy_optimization = []\n",
    "    SoC_optimization = []\n",
    "    results = []\n",
    "    for i in tqdm(range(true_values.shape[0])): \n",
    "        forecast_prices_24h = true_values_24.iloc[i, ].values[:24]\n",
    "        forecast_prices_prob = Y_pred_prob[i, :Forecast_Horizon-24, :]  \n",
    "        true_prices = true_values.iloc[i, ].values[:Forecast_Horizon]\n",
    "        availability = Y_available.iloc[i, 1:].values[:Forecast_Horizon].astype(bool)\n",
    "        energy_trip = Y_energy_trip.iloc[i, 1:].values[:Forecast_Horizon]\n",
    "        \n",
    "       \n",
    "        charging_strategy, SoC = optimize_ev_charging_stochastic(forecast_prices_24h, forecast_prices_prob, availability, energy_trip, State, weights, Forecast_Horizon)\n",
    "\n",
    "        State = SoC[23]\n",
    "        Strategy_optimization.append(charging_strategy)\n",
    "        SoC_optimization.append(SoC)\n",
    "\n",
    "        cost_excl_tax, cost_incl_tax = compute_cost(charging_strategy[:24], true_prices[:24])\n",
    "    \n",
    "\n",
    "        results.append({\"sample\": i,\n",
    "                    \"strategy\": f'{model}',\n",
    "                    \"cost_excl_tax\": cost_excl_tax,\n",
    "                    \"cost_incl_tax\": cost_incl_tax\n",
    "                    })\n",
    "        \n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    summary = results_df.groupby(\"strategy\").agg(\n",
    "    total_cost_excl_tax=(\"cost_excl_tax\", \"sum\"),\n",
    "    daily_mean_excl_tax=(\"cost_excl_tax\", \"mean\"),\n",
    "    daily_std_excl_tax=(\"cost_excl_tax\", \"std\"),\n",
    "    total_cost_incl_tax=(\"cost_incl_tax\", \"sum\"),\n",
    "    daily_mean_incl_tax=(\"cost_incl_tax\", \"mean\"),\n",
    "    daily_std_incl_tax=(\"cost_incl_tax\", \"std\"),\n",
    "    )\n",
    "    pd.DataFrame(Strategy_optimization).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/Charging/{model}_prob_charge_schedule_{Forecast_Horizon}.csv\")\n",
    "    pd.DataFrame(SoC_optimization).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Schedules/SoC/{model}_prob_SoC_{Forecast_Horizon}.csv\")\n",
    "    return results_df, summary\n",
    "\n",
    "\n",
    "for Forecast_Horizon in [72, 168]:\n",
    "    true_values_24 = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/True_values_24.csv\")\n",
    "    true_values = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/True_values_{Forecast_Horizon}.csv\")\n",
    "    all_results = []\n",
    "    all_summary = []\n",
    "    model_names = [\"DDNN\", \"DDNN_mixture\", \"Regular_LQRA\", \"Quick_LQRA\"]\n",
    "    for model in model_names:\n",
    "        horizon = Forecast_Horizon - 24\n",
    "        Y_pred_prob = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Probability/{model}_prob_forecasts_{horizon}.csv\").values.reshape(290, Forecast_Horizon-24, 99)\n",
    "        Y_available, Y_energy_trip = load_availability_and_energy_matrices(Forecast_Horizon)\n",
    "        Q = 99\n",
    "        weights = np.ones(Q)/Q\n",
    "        results_optimization, summary_optimization = load_optimization_prob_forecasts(Y_pred_prob, true_values_24, true_values, weights, Forecast_Horizon, model)\n",
    "        all_results.append(results_optimization)\n",
    "        all_summary.append(summary_optimization)\n",
    "    pd.concat(all_results, ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Probability_charging_cost_eachday{Forecast_Horizon}.csv\")\n",
    "    pd.concat(all_summary, ignore_index=False).to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Proabability_charging_cost_average_{Forecast_Horizon}.csv\")\n",
    "    print(pd.concat(all_summary, ignore_index=False))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f237a6",
   "metadata": {},
   "source": [
    "Generate RMSE, CRPS and Winkler Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38110daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calc_rmse(y_true, y_pred):\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse_val\n",
    "\n",
    "def pinball_loss(y_true, y_pred, quantiles):\n",
    "   \n",
    "    N, H, Q = y_pred.shape\n",
    "    losses = np.zeros((N, H, Q))\n",
    "    for q_idx, q in enumerate(quantiles):\n",
    "        err = y_true - y_pred[:, :, q_idx]\n",
    "        losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "    return losses\n",
    "\n",
    "def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles):\n",
    "    pinball_losses = pinball_loss(np.array(y_true), Y_pred, np.array(quantiles))\n",
    "    crps = np.mean(np.trapz(pinball_losses, np.array(quantiles), axis=-1))\n",
    "    return pinball_losses, np.mean(pinball_losses), crps\n",
    "\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha):\n",
    "    width = y_upper - y_lower\n",
    "    \n",
    "    under = (y_lower - y_true) * (y_true < y_lower)\n",
    "    over  = (y_true - y_upper) * (y_true > y_upper)\n",
    "    \n",
    "    score = width + (2.0 / alpha) * (under + over)\n",
    "    return score\n",
    "\n",
    "def calculate_winkler(Y_pred, y_true, quantiles, lower_q, upper_q):\n",
    "    alpha = 1.0 - (upper_q - lower_q)\n",
    "    try:\n",
    "        li = quantiles.index(lower_q)\n",
    "        ui = quantiles.index(upper_q)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"lower_q={lower_q} or upper_q={upper_q} not in quantiles list\")\n",
    "    \n",
    "    y_lower = Y_pred[:, :, li]\n",
    "    y_upper = Y_pred[:, :, ui]\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    scores = winkler_score(y_true, y_lower, y_upper, alpha)\n",
    "    mean_score = scores.mean()\n",
    "    return scores, mean_score\n",
    "\n",
    "def load_and_prepare_quantile_data(quantile, probability_forecasts, all_quantiles):\n",
    "    quantile_pred = pd.DataFrame(probability_forecasts[:, :, all_quantiles.index(quantile)])\n",
    "    return quantile_pred\n",
    "\n",
    "for Forecast_Horizon in [48, 144]: \n",
    "    results = []\n",
    "    true_values = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/Values_for_evaluation_{Forecast_Horizon}.csv\")\n",
    "    list_names = ['Arima', 'garch_asymmetric', 'garch_regular', 'Hybrid_LSTM_lgbm', 'Hybrid_LSTM_xgb', \"Hybrid_weighted_AllThree\", 'Hybrid_weighted_LSTM_lgbm', 'Hybrid_weighted_LSTM_xgb', \"LGBM\", 'LSTM', \"XGB\", 'DNN']\n",
    "    for model in list_names:\n",
    "        y_pred = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/{model}_forecasts_{Forecast_Horizon}.csv\")\n",
    "        rmse = calc_rmse(true_values, y_pred)\n",
    "        results.append((model, rmse))\n",
    "    \n",
    "    prob_results = []\n",
    "    all_quantiles = np.arange(0.01, 1.00, 0.01).round(2).tolist()\n",
    "    model_names = [\"Regular_LQRA\", \"Quick_LQRA\", \"DDNN\", \"DDNN_mixture\"]\n",
    "    for prob_model in model_names:\n",
    "        prob_pred = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Probability/{prob_model}_prob_forecasts_{Forecast_Horizon}.csv\").values.reshape(290, Forecast_Horizon, 99)\n",
    "        pinball_losses, avg_pinball_loss, crps = calculate_pinball_loss_CRPS(true_values, prob_pred, all_quantiles)   \n",
    "        scores, mean_score_1 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.4, 0.6)\n",
    "        scores, mean_score_2 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.3, 0.7)\n",
    "        scores, mean_score_3 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.2, 0.8)\n",
    "        scores, mean_score_4 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.1, 0.9)\n",
    "        scores, mean_score_5 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.05, 0.95)\n",
    "        prob_results.append((prob_model, avg_pinball_loss, crps, mean_score_1, mean_score_2, mean_score_3, mean_score_4, mean_score_5))\n",
    "        for quantile in np.arange(0.1, 1.0, 0.1).round(2):\n",
    "            y_pred_quantile = load_and_prepare_quantile_data(quantile, prob_pred, all_quantiles)\n",
    "            rmse = calc_rmse(true_values, y_pred_quantile)\n",
    "            results.append((f\"{prob_model}_Q_{quantile}\", rmse))\n",
    "            \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.columns = [\"Model\", \"RMSE\"]\n",
    "    prob_results_df = pd.DataFrame(prob_results)\n",
    "    prob_results_df.columns = [\"Model\", \"Avg_pinball_loss\", \"CRPS\", \"Winkler_Score(40%-60%)\", \"Winkler_Score(30%-70%)\", \"Winkler_Score(20%-80%)\", \"Winkler_Score(10%-90%)\", \"Winkler_Score(5%-95%)\"]\n",
    "    results_df.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Point_pred_RMSE_{Forecast_Horizon}_extra.csv\", index=False )\n",
    "    prob_results_df.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Prob_pred_CRPS_Winkler_{Forecast_Horizon}.csv\", index=False)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00828e1e",
   "metadata": {},
   "source": [
    "Generate evaluation metrics for ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calc_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def pinball_loss(y_true, y_pred, quantiles):\n",
    "    N, H, Q = y_pred.shape\n",
    "    losses = np.zeros((N, H, Q))\n",
    "    for q_idx, q in enumerate(quantiles):\n",
    "        err = y_true - y_pred[:, :, q_idx]\n",
    "        losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "    return losses\n",
    "\n",
    "def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles):\n",
    "    pinball_losses = pinball_loss(np.array(y_true), Y_pred, np.array(quantiles))\n",
    "    crps = np.mean(np.trapz(pinball_losses, np.array(quantiles), axis=-1))\n",
    "    return crps\n",
    "\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha):\n",
    "    width = y_upper - y_lower\n",
    "    under = (y_lower - y_true) * (y_true < y_lower)\n",
    "    over = (y_true - y_upper) * (y_true > y_upper)\n",
    "    score = width + (2.0 / alpha) * (under + over)\n",
    "    return score\n",
    "\n",
    "def calculate_winkler(Y_pred, y_true, quantiles, lower_q, upper_q):\n",
    "    alpha = 1.0 - (upper_q - lower_q)\n",
    "    li = quantiles.index(lower_q)\n",
    "    ui = quantiles.index(upper_q)\n",
    "    y_lower = Y_pred[:, :, li]\n",
    "    y_upper = Y_pred[:, :, ui]\n",
    "    return np.mean(winkler_score(np.array(y_true), y_lower, y_upper, alpha))\n",
    "\n",
    "def calculate_interval_width(Y_pred, quantiles, lower_q, upper_q):\n",
    "    li = quantiles.index(lower_q)\n",
    "    ui = quantiles.index(upper_q)\n",
    "    y_lower = Y_pred[:, :, li]\n",
    "    y_upper = Y_pred[:, :, ui]\n",
    "    return np.mean(y_upper - y_lower)\n",
    "\n",
    "def coverage_probability(y_true, y_pred, lower_idx, upper_idx):\n",
    "    y_lower = y_pred[:, :, lower_idx]\n",
    "    y_upper = y_pred[:, :, upper_idx]\n",
    "    inside = (y_true >= y_lower) & (y_true <= y_upper)\n",
    "    coverage = np.mean(inside)\n",
    "    return coverage\n",
    "\n",
    "ablation_vars = [\"Quick\", \"No_Solar\", \"No_Wind\", \"No_Load\", \"No_Last_Day\", \"No_d7\"]\n",
    "quantiles = np.round(np.arange(0.01, 1.00, 0.01), 2).tolist()\n",
    "\n",
    "results = []\n",
    "for h in [72, 168]:\n",
    "    y_true = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/Values_for_evaluation_{h - 24}.csv\").values\n",
    "    for var in ablation_vars:\n",
    "        file_path = f\"C:/Users/daniq/iCloudDrive\\Erasmus University Rotterdam/Master/Thesis/Output/{var}_LQRA_prob_test_{h}.csv\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Missing: {file_path}\")\n",
    "            continue\n",
    "        y_pred = pd.read_csv(file_path).values.reshape(290, h - 24, 99)\n",
    "\n",
    "        median_idx = quantiles.index(0.5)\n",
    "        y_median = y_pred[:, :, median_idx]\n",
    "        rmse = calc_rmse(y_true, y_median)\n",
    "\n",
    "        crps = calculate_pinball_loss_CRPS(y_true, y_pred, quantiles)\n",
    "        winkler_80 = calculate_winkler(y_pred, y_true, quantiles, 0.1, 0.9)\n",
    "        interval_width = np.mean(y_pred[:, :, quantiles.index(0.9)] - y_pred[:, :, quantiles.index(0.1)])\n",
    "\n",
    "        width_by_segment = []\n",
    "        for i in range(0, h - 24, 24):  \n",
    "            width_segment = y_pred[:, i:i+24, quantiles.index(0.9)] - y_pred[:, i:i+24, quantiles.index(0.1)]\n",
    "            width_by_segment.append(np.mean(width_segment))\n",
    "\n",
    "        lower_idx = quantiles.index(0.1)\n",
    "        upper_idx = quantiles.index(0.9)\n",
    "        coverage_80 = coverage_probability(y_true, y_pred, lower_idx, upper_idx)\n",
    "\n",
    "        coverage_by_segment = []\n",
    "        for i in range(0, h - 24, 24): \n",
    "            coverage_segment = coverage_probability(y_true[:, i:i+24], y_pred[:,i:i+24, :],  quantiles.index(0.1), quantiles.index(0.9))\n",
    "            coverage_by_segment.append(np.mean(coverage_segment))\n",
    "\n",
    "        results_dict = {\n",
    "            \"Variable\": var,\n",
    "            \"Horizon\": h,\n",
    "            \"RMSE\": rmse,\n",
    "            \"CRPS\": crps,\n",
    "            \"Winkler(10-90%)\": winkler_80,\n",
    "            \"Coverage(10-90%)\": coverage_80,\n",
    "            \"IntervalWidth(10-90%)\": interval_width,\n",
    "        }\n",
    "\n",
    "        for j, width_val in enumerate(width_by_segment):\n",
    "            results_dict[f\"Width_d+{2 + j}\"] = width_val\n",
    "        \n",
    "        for j, cov_val in enumerate(coverage_by_segment):\n",
    "            results_dict[f\"Coverage_d+{2 + j}\"] = cov_val\n",
    "\n",
    "        results.append(results_dict)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"C:/Users/daniq/iCloudDrive\\Erasmus University Rotterdam/Master/Thesis/Output/Ablation_Effect_QuickLQRA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9cb1a",
   "metadata": {},
   "source": [
    "Generate evaluation metrics without outliers and only outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calc_rmse(y_true, y_pred, mask):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    if mask is not None:\n",
    "        y_true = y_true[~mask]\n",
    "        y_pred = y_pred[~mask]\n",
    "    \n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def pinball_loss(y_true, y_pred, quantiles):\n",
    "   \n",
    "    N, H, Q = y_pred.shape\n",
    "    losses = np.zeros((N, H, Q))\n",
    "    for q_idx, q in enumerate(quantiles):\n",
    "        err = y_true - y_pred[:, :, q_idx]\n",
    "        losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "    return losses\n",
    "\n",
    "def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles, mask=None):\n",
    "    y_true = np.array(y_true)\n",
    "    Y_pred = np.array(Y_pred)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask_3d = np.expand_dims(mask, axis=2) \n",
    "        mask_3d = np.broadcast_to(mask_3d, Y_pred.shape)  \n",
    "        \n",
    "        y_true = y_true.astype(float)\n",
    "        y_true[mask] = np.nan\n",
    "        \n",
    "        Y_pred = Y_pred.astype(float)\n",
    "        Y_pred[mask_3d] = np.nan\n",
    "\n",
    "    pinball_losses = pinball_loss(y_true, Y_pred, quantiles) \n",
    "\n",
    "    mean_pinball_loss = np.nanmean(pinball_losses)\n",
    "    crps_per_point = np.nanmean(np.trapz(pinball_losses, quantiles, axis=-1), axis=-1)\n",
    "    crps = np.nanmean(crps_per_point)\n",
    "\n",
    "    return pinball_losses, mean_pinball_loss, crps\n",
    "\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha):\n",
    "    width = y_upper - y_lower\n",
    "    \n",
    "    under = (y_lower - y_true) * (y_true < y_lower)\n",
    "    over  = (y_true - y_upper) * (y_true > y_upper)\n",
    "    \n",
    "    score = width + (2.0 / alpha) * (under + over)\n",
    "    return score\n",
    "\n",
    "def calculate_winkler(Y_pred, y_true, quantiles, lower_q, upper_q, mask):\n",
    "    y_true = np.array(y_true)\n",
    "    if mask is not None:\n",
    "        mask_3d = np.expand_dims(mask, axis=2) \n",
    "        mask_3d = np.broadcast_to(mask_3d, Y_pred.shape) \n",
    "\n",
    "        y_true = y_true.astype(float)\n",
    "        y_true[mask] = np.nan\n",
    "        \n",
    "        Y_pred = Y_pred.astype(float)\n",
    "        Y_pred[mask_3d] = np.nan\n",
    "\n",
    "    alpha = 1.0 - (upper_q - lower_q)\n",
    "    try:\n",
    "        li = quantiles.index(lower_q)\n",
    "        ui = quantiles.index(upper_q)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"lower_q={lower_q} or upper_q={upper_q} not in quantiles list\")\n",
    "    \n",
    "    y_lower = Y_pred[:, :, li]\n",
    "    y_upper = Y_pred[:, :, ui]\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    scores = winkler_score(y_true, y_lower, y_upper, alpha)\n",
    "    mean_score = np.nanmean(scores)\n",
    "    return scores, mean_score\n",
    "\n",
    "def load_and_prepare_quantile_data(quantile, probability_forecasts, all_quantiles):\n",
    "    quantile_pred = pd.DataFrame(probability_forecasts[:, :, all_quantiles.index(quantile)])\n",
    "    return quantile_pred\n",
    "\n",
    "for Forecast_Horizon in [48, 144]: \n",
    "    results = []\n",
    "    outlier_matrix = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/outlier_matrix_{Forecast_Horizon}.csv\")\n",
    "    outlier_mask = outlier_matrix.astype(bool).values\n",
    "    #outlier_mask = ~outlier_mask # Invert mask to only focus on outliers\n",
    "\n",
    "    true_values = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/Values_for_evaluation_{Forecast_Horizon}.csv\")\n",
    "    list_names = ['Arima', 'GJR_Garch', 'Garch', 'Hybrid_LSTM_lgbm', 'Hybrid_LSTM_xgb', \"Hybrid_weighted_AllThree\", 'Hybrid_weighted_LSTM_lgbm', 'Hybrid_weighted_LSTM_xgb', \"LGBM\", 'LSTM', \"XGB\", 'DNN']\n",
    "    for model in list_names:\n",
    "        y_pred = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/{model}_forecasts_{Forecast_Horizon}.csv\")\n",
    "        rmse = calc_rmse(true_values, y_pred, outlier_mask)\n",
    "        results.append((model, rmse))\n",
    "    \n",
    "    prob_results = []\n",
    "    all_quantiles = np.arange(0.01, 1.00, 0.01).round(2).tolist()\n",
    "    model_names = [\"Regular_LQRA\", \"Quick_LQRA\", \"DDNN\", \"DDNN_mixture\"]\n",
    "    for prob_model in model_names:\n",
    "        prob_pred = pd.read_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Probability/{prob_model}_prob_forecasts_{Forecast_Horizon}.csv\").values.reshape(290, Forecast_Horizon, 99)\n",
    "        pinball_losses, avg_pinball_loss, crps = calculate_pinball_loss_CRPS(true_values, prob_pred, all_quantiles, outlier_mask)    \n",
    "        scores, mean_score_1 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.4, 0.6, outlier_mask)\n",
    "        scores, mean_score_2 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.3, 0.7, outlier_mask)\n",
    "        scores, mean_score_3 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.2, 0.8, outlier_mask)\n",
    "        scores, mean_score_4 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.1, 0.9, outlier_mask)\n",
    "        scores, mean_score_5 = calculate_winkler(prob_pred, true_values, all_quantiles, 0.05, 0.95, outlier_mask)\n",
    "        prob_results.append((prob_model, avg_pinball_loss, crps, mean_score_1, mean_score_2, mean_score_3, mean_score_4, mean_score_5))\n",
    "        for quantile in np.arange(0.1, 1.0, 0.1).round(2):\n",
    "            y_pred_quantile = load_and_prepare_quantile_data(quantile, prob_pred, all_quantiles)\n",
    "            rmse = calc_rmse(true_values, y_pred_quantile, outlier_mask)\n",
    "            results.append((f\"{prob_model}_Q_{quantile}\", rmse))\n",
    "            \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.columns = [\"Model\", \"RMSE\"]\n",
    "    prob_results_df = pd.DataFrame(prob_results)\n",
    "    prob_results_df.columns = [\"Model\", \"Avg_pinball_loss\", \"CRPS\", \"Winkler_Score(40%-60%)\", \"Winkler_Score(30%-70%)\", \"Winkler_Score(20%-80%)\", \"Winkler_Score(10%-90%)\", \"Winkler_Score(5%-95%)\"]\n",
    "    results_df.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Point_pred_RMSE_{Forecast_Horizon}_OnlyOutlier.csv\", index=False )\n",
    "    prob_results_df.to_csv(f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/Prob_pred_CRPS_Winkler_{Forecast_Horizon}_OnlyOutlier.csv\", index=False)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260174e8",
   "metadata": {},
   "source": [
    "Diebold-Mariano test for the point forecasts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a68f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import t\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "def newey_west_variance(d, lag=None):\n",
    "    T = len(d)\n",
    "    d_mean = np.mean(d)\n",
    "    d = d - d_mean\n",
    "    if lag is None:\n",
    "        lag = int(np.floor(T ** (1 / 4)))\n",
    "\n",
    "    gamma_0 = np.sum(d * d) / T\n",
    "    var = gamma_0\n",
    "\n",
    "    for l in range(1, lag + 1):\n",
    "        gamma_l = np.sum(d[l:] * d[:-l]) / T\n",
    "        weight = 1 - l / (lag + 1)\n",
    "        var += 2 * weight * gamma_l\n",
    "\n",
    "    return var\n",
    "\n",
    "def diebold_mariano_test(y_true, y_pred1, y_pred2, h=1, power=2):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred1 = y_pred1.flatten()\n",
    "    y_pred2 = y_pred2.flatten()\n",
    "\n",
    "    e1 = y_true - y_pred1\n",
    "    e2 = y_true - y_pred2\n",
    "    d = np.abs(e1)**power - np.abs(e2)**power\n",
    "\n",
    "    d_mean = np.mean(d)\n",
    "    N = len(d)\n",
    "    var_d = newey_west_variance(d, lag=h - 1 if h > 1 else 0)\n",
    "\n",
    "    dm_stat = d_mean / np.sqrt(var_d / N)\n",
    "    p_value = t.cdf(dm_stat, df=N - 1) \n",
    "\n",
    "    return dm_stat, p_value\n",
    "\n",
    "\n",
    "for Forecast_Horizon in [48, 144]:\n",
    "    print(f\"\\n--- Forecast Horizon: {Forecast_Horizon} ---\")\n",
    "    model_preds = {}\n",
    "    true_values = pd.read_csv(\n",
    "        f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/Values_for_evaluation_{Forecast_Horizon}.csv\"\n",
    "    ).values\n",
    "\n",
    "    list_names_point = ['Arima', 'GJR_Garch', 'Garch','LSTM', 'LGBM', 'XGB',\n",
    "                        'Hybrid_LSTM_lgbm', 'Hybrid_LSTM_xgb', 'Hybrid_weighted_AllThree',\n",
    "                        'Hybrid_weighted_LSTM_lgbm', 'Hybrid_weighted_LSTM_xgb', 'DNN']\n",
    "\n",
    "    for model in list_names_point:\n",
    "        y_pred = pd.read_csv(\n",
    "            f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Point/{model}_forecasts_{Forecast_Horizon}.csv\"\n",
    "        ).values\n",
    "        model_preds[model] = y_pred\n",
    "\n",
    "    list_names_prob = [\"DDNN\", \"DDNN_mixture\", \"Regular_LQRA\", \"Quick_LQRA\"]\n",
    "\n",
    "    for model in list_names_prob:\n",
    "        y_pred_quantiles = pd.read_csv(\n",
    "            f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Probability/{model}_prob_forecasts_{Forecast_Horizon}.csv\"\n",
    "        ).values.reshape(290, Forecast_Horizon, 99)\n",
    "        \n",
    "        median_forecast = y_pred_quantiles[:, :, 49]\n",
    "        model_preds[f\"{model}_median\"] = median_forecast\n",
    "    combined_models = list_names_point + [f\"{m}_median\" for m in list_names_prob]\n",
    "\n",
    "    dm_stat_matrix = pd.DataFrame(index=combined_models, columns=combined_models)\n",
    "    p_val_matrix = pd.DataFrame(index=combined_models, columns=combined_models)\n",
    "\n",
    "    for i, model_i in enumerate(combined_models):\n",
    "        for j, model_j in enumerate(combined_models):\n",
    "            if i == j:\n",
    "                dm_stat_matrix.loc[model_i, model_j] = 0.0\n",
    "                p_val_matrix.loc[model_i, model_j] = 1.0\n",
    "            else:\n",
    "                dm_stat, p_val = diebold_mariano_test(\n",
    "                    true_values, model_preds[model_i], model_preds[model_j], h=Forecast_Horizon\n",
    "                )\n",
    "                dm_stat_matrix.loc[model_i, model_j] = dm_stat\n",
    "                p_val_matrix.loc[model_i, model_j] = p_val\n",
    "\n",
    "    dm_stat_float = dm_stat_matrix.astype(float)\n",
    "    p_val_float = p_val_matrix.astype(float)\n",
    "\n",
    "    mask_not_sig = (p_val_float >= 0.1) | (dm_stat_float >= 0) \n",
    "\n",
    "    filtered_matrix = p_val_float.mask(mask_not_sig)\n",
    "\n",
    "    annot_matrix = filtered_matrix.copy().astype(str)\n",
    "    for i in range(len(annot_matrix)):\n",
    "        for j in range(len(annot_matrix)):\n",
    "            if i == j:\n",
    "                annot_matrix.iloc[i, j] = 'X'\n",
    "            elif pd.isna(filtered_matrix.iloc[i, j]):\n",
    "                annot_matrix.iloc[i, j] = ''\n",
    "            else:\n",
    "                annot_matrix.iloc[i, j] = f\"{filtered_matrix.iloc[i, j]:.3f}\"\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    gray_color = '#444444'\n",
    "    mask = filtered_matrix.isnull()\n",
    "    mask.values[np.diag_indices_from(mask)] = False\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for (i, j), val in np.ndenumerate(mask.values):\n",
    "        if val or i == j:\n",
    "            ax.add_patch(patches.Rectangle((j, i), 1, 1, fill=True, color=gray_color, lw=0))\n",
    "    sns.heatmap(\n",
    "        filtered_matrix,\n",
    "        annot=False,\n",
    "        cmap='coolwarm_r',\n",
    "        vmin=0, vmax=0.1,\n",
    "        cbar_kws={'label': 'p-value'},\n",
    "        linewidths=0,\n",
    "        linecolor='none',\n",
    "        mask=mask,\n",
    "        ax=ax\n",
    "    )\n",
    "    n = filtered_matrix.shape[0]\n",
    "    for i in range(n + 1):\n",
    "        ax.axhline(i, color='white', linewidth=0.3)\n",
    "        ax.axvline(i, color='white', linewidth=0.3)\n",
    "    for i in range(n):\n",
    "        ax.plot(i + 0.5, i + 0.5, marker='x', markersize=14, color='white', markeredgewidth=2)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/DM_pval_heatmap_{Forecast_Horizon}.png\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeaf95d",
   "metadata": {},
   "source": [
    "Diebold-Mariano test for the probabilistic forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import t\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "\n",
    "\n",
    "def pinball_loss(y_true, y_pred, quantiles):\n",
    "    N, H, Q = y_pred.shape\n",
    "    losses = np.zeros((N, H, Q))\n",
    "    for q_idx, q in enumerate(quantiles):\n",
    "        err = y_true - y_pred[:, :, q_idx]\n",
    "        losses[:, :, q_idx] = np.maximum(q * err, (q - 1) * err)\n",
    "    return losses\n",
    "\n",
    "def calculate_pinball_loss_CRPS(y_true, Y_pred, quantiles):\n",
    "    pinball_losses = pinball_loss(np.array(y_true), Y_pred, np.array(quantiles))\n",
    "    crps = np.mean(np.trapz(pinball_losses, np.array(quantiles), axis=-1))\n",
    "    return pinball_losses, np.mean(pinball_losses), crps\n",
    "\n",
    "def newey_west_variance(d, lag=None):\n",
    "    T = len(d)\n",
    "    d_mean = np.mean(d)\n",
    "    d = d - d_mean\n",
    "    if lag is None:\n",
    "        lag = int(np.floor(T ** (1 / 4)))\n",
    "\n",
    "    gamma_0 = np.sum(d * d) / T\n",
    "    var = gamma_0\n",
    "\n",
    "    for l in range(1, lag + 1):\n",
    "        gamma_l = np.sum(d[l:] * d[:-l]) / T\n",
    "        weight = 1 - l / (lag + 1)\n",
    "        var += 2 * weight * gamma_l\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "def diebold_mariano_test(pl1, pl2, h=1):\n",
    "    d = pl1 - pl2\n",
    "    d_mean = np.mean(d)\n",
    "    N = len(d)\n",
    "    var_d = newey_west_variance(d, lag=h - 1 if h > 1 else 0)\n",
    "    dm_stat = d_mean / np.sqrt(var_d / N)\n",
    "    p_value = t.cdf(dm_stat, df=N - 1)  \n",
    "    return dm_stat, p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for Forecast_Horizon in [48, 144]:\n",
    "    true_values = pd.read_csv(\n",
    "        f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Full_true/Values_for_evaluation_{Forecast_Horizon}.csv\"\n",
    "    ).values\n",
    "\n",
    "    list_names = [\"DDNN\", \"DDNN_mixture\", \"Regular_LQRA\", \"Very_quick_LQRA\"]\n",
    "    model_losses = {}\n",
    "\n",
    "    for model in list_names:\n",
    "        y_pred = pd.read_csv(\n",
    "                f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Forecasts/Probability/{model}_prob_forecasts_{Forecast_Horizon}.csv\"\n",
    "        ).values.reshape(290, Forecast_Horizon, 99)\n",
    "        pinball, _, _ = calculate_pinball_loss_CRPS(true_values, y_pred, np.arange(0.01, 1.00, 0.01))\n",
    "        daily_mean_loss = pinball.mean(axis=(1, 2))  \n",
    "        model_losses[model] = daily_mean_loss\n",
    "\n",
    "    dm_stat_matrix = pd.DataFrame(index=list_names, columns=list_names)\n",
    "    p_val_matrix = pd.DataFrame(index=list_names, columns=list_names)\n",
    "\n",
    "    for i, model_i in enumerate(list_names):\n",
    "        for j, model_j in enumerate(list_names):\n",
    "            if i == j:\n",
    "                dm_stat_matrix.loc[model_i, model_j] = 0.0\n",
    "                p_val_matrix.loc[model_i, model_j] = 1.0\n",
    "            else:\n",
    "                dm_stat, p_val = diebold_mariano_test(\n",
    "                    model_losses[model_i], model_losses[model_j], h=Forecast_Horizon\n",
    "                )\n",
    "                dm_stat_matrix.loc[model_i, model_j] = dm_stat\n",
    "                p_val_matrix.loc[model_i, model_j] = p_val\n",
    "\n",
    "\n",
    "    dm_stat_float = dm_stat_matrix.astype(float)\n",
    "    p_val_float = p_val_matrix.astype(float)\n",
    "\n",
    "    mask_not_sig = (p_val_float >= 0.1) | (dm_stat_float >= 0)\n",
    "    filtered_matrix = p_val_float.mask(mask_not_sig)\n",
    "\n",
    "    annot_matrix = filtered_matrix.copy().astype(str)\n",
    "    for i in range(len(annot_matrix)):\n",
    "        for j in range(len(annot_matrix)):\n",
    "            if i == j:\n",
    "                annot_matrix.iloc[i, j] = 'X'\n",
    "            elif pd.isna(filtered_matrix.iloc[i, j]):\n",
    "                annot_matrix.iloc[i, j] = ''\n",
    "            else:\n",
    "                annot_matrix.iloc[i, j] = f\"{filtered_matrix.iloc[i, j]:.3f}\"\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    gray_color = '#444444'\n",
    "    mask = filtered_matrix.isnull()\n",
    "    mask.values[np.diag_indices_from(mask)] = False\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    for (i, j), val in np.ndenumerate(mask.values):\n",
    "        if val or i == j:\n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (j, i), 1, 1,\n",
    "                    fill=True,\n",
    "                    color=gray_color,\n",
    "                    lw=0\n",
    "                )\n",
    "            )\n",
    "\n",
    "    sns.heatmap(\n",
    "        filtered_matrix,\n",
    "        annot=False,\n",
    "        cmap='coolwarm_r',\n",
    "        vmin=0, vmax=0.1,\n",
    "        cbar_kws={'label': 'p-value'},\n",
    "        linewidths=0,\n",
    "        linecolor='none',\n",
    "        mask=mask,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    n = filtered_matrix.shape[0]\n",
    "    for i in range(n + 1):\n",
    "        ax.axhline(i, color='white', linewidth=0.3)\n",
    "        ax.axvline(i, color='white', linewidth=0.3)\n",
    "\n",
    "    for i in range(n):\n",
    "        ax.plot(i + 0.5, i + 0.5, marker='x', markersize=34, color='white', markeredgewidth=2)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "    f\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Evaluation_Metrics/DM_prob_pval_heatmap_{Forecast_Horizon}.png\"\n",
    "    )\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
