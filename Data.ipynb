{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b641d8c",
   "metadata": {},
   "source": [
    "Create a combined Dataframe of price and load for the years 2023-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def price_data():\n",
    "    folder_path = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data\"\n",
    "    file_names = [\n",
    "        \"Energy_Prices_2023.xlsx\",\n",
    "        \"Energy_Prices_2024.xlsx\",\n",
    "        \"Energy_Prices_2025.xlsx\"\n",
    "    ]\n",
    "\n",
    "    def load_and_process_excel(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        year_in_file = ''.join(filter(str.isdigit, filename))\n",
    "        \n",
    "        df = pd.read_excel(file_path, skiprows=6, names=[\"Timestamp\", \"Price\"])    \n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].str.replace(r\"\\s*\\(.*?\\)\", \"\", regex=True).str.strip()\n",
    "        \n",
    "        df.loc[df['Timestamp'] == '26/03/2023 01:00:00 - 26/03/2023 03:00:00', 'Timestamp'] = '26/03/2023 01:00:00 - 26/03/2023 02:00:00'\n",
    "        df.loc[df['Timestamp'] == '31/03/2024 01:00:00 - 31/03/2024 03:00:00', 'Timestamp'] = '31/03/2024 01:00:00 - 31/03/2024 02:00:00'\n",
    "        df.loc[df['Timestamp'] == '30/03/2025 01:00:00 - 30/03/2025 03:00:00', 'Timestamp'] = '30/03/2025 01:00:00 - 30/03/2025 02:00:00'\n",
    "\n",
    "        df.loc[df['Timestamp'] == '27/10/2024 02:00:00 - 27/10/2024 02:00:00', 'Timestamp'] = '27/10/2024 02:00:00 - 27/10/2024 03:00:00'\n",
    "        df.loc[df['Timestamp'] == '29/10/2023 02:00:00 - 29/10/2023 02:00:00', 'Timestamp'] = '29/10/2023 02:00:00 - 29/10/2023 03:00:00'\n",
    "\n",
    "        \n",
    "        new_timestamps = [\n",
    "        ('26/03/2023 01:00:00 - 26/03/2023 02:00:00', '26/03/2023 02:00:00 - 26/03/2023 03:00:00'),\n",
    "         ('31/03/2024 01:00:00 - 31/03/2024 02:00:00', '31/03/2024 02:00:00 - 31/03/2024 03:00:00'),\n",
    "        ('30/03/2025 01:00:00 - 30/03/2025 02:00:00', '30/03/2025 02:00:00 - 30/03/2025 03:00:00'),\n",
    "        ]\n",
    "\n",
    "        new_rows = []\n",
    "        for old_ts, new_ts in new_timestamps:\n",
    "            match = df[df['Timestamp'] == old_ts]\n",
    "            if not match.empty:\n",
    "                new_row = match.copy()\n",
    "                new_row['Timestamp'] = new_ts\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "        if new_rows:\n",
    "            df = pd.concat([df] + new_rows, ignore_index=True)\n",
    "\n",
    "        df = df.groupby(\"Timestamp\", as_index=False).agg({\n",
    "            \"Price\": \"mean\"\n",
    "            })\n",
    "       \n",
    "        df[[\"start\", \"end\"]] = df[\"Timestamp\"].str.split(\" - \", expand=True)\n",
    "\n",
    "        df[\"start_datetime\"] = pd.to_datetime(df[\"start\"], format=\"%d/%m/%Y  %H:%M:%S\")\n",
    "        df[\"end_datetime\"] = pd.to_datetime(df[\"end\"], format=\"%d/%m/%Y  %H:%M:%S\")\n",
    "\n",
    "        if year_in_file == \"2025\":\n",
    "            end_of_april = pd.Timestamp(\"2025-04-30 23:00:00\")\n",
    "            df = df[df[\"start_datetime\"] <= end_of_april]\n",
    "    \n",
    "        df = df.sort_values(by='start_datetime')\n",
    "\n",
    "        cols = [\n",
    "            \"start_datetime\", \"end_datetime\",\n",
    "            \"Timestamp\", \"Price\"\n",
    "        ]\n",
    "        return df[cols]\n",
    "\n",
    "    dfs = [load_and_process_excel(os.path.join(folder_path, f)) for f in file_names]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    folder_path = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data\"\n",
    "    loadfile_names = [\n",
    "        \"Load_2023.xlsx\",\n",
    "        \"Load_2024.xlsx\",\n",
    "        \"Load_2025.xlsx\",\n",
    "    ]\n",
    "    def load_and_process_excel(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        year_in_file = ''.join(filter(str.isdigit, filename))\n",
    "\n",
    "        df = pd.read_excel(file_path, skiprows=6, names=[\"Timestamp\", \"Actual_Load\", \"Forecasted_Load\"])\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].str.replace(r\"\\s*\\(.*?\\)\", \"\", regex=True).str.strip()\n",
    "        df[[\"start\", \"end\"]] = df[\"Timestamp\"].str.split(\" - \", expand=True)\n",
    "        df[\"start_datetime\"] = pd.to_datetime(df[\"start\"], format=\"%d/%m/%Y  %H:%M\")\n",
    "        df[\"end_datetime\"] = pd.to_datetime(df[\"end\"], format=\"%d/%m/%Y  %H:%M\")\n",
    "        \n",
    "\n",
    "        if year_in_file == \"2025\":\n",
    "            end_of_april = pd.Timestamp(\"2025-04-30 23:45\")\n",
    "            df = df[df[\"start_datetime\"] <= end_of_april]\n",
    "\n",
    "        df[[\"Actual_Load\",\"Forecasted_Load\"]] = df[[\"Actual_Load\",\"Forecasted_Load\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        df[\"start_datetime\"] = (\n",
    "            df[\"start_datetime\"]\n",
    "              .dt.tz_localize(\"Europe/Amsterdam\", ambiguous=\"infer\")\n",
    "              .dt.tz_convert(\"UTC\")\n",
    "        )\n",
    "        df[\"end_datetime\"] = (\n",
    "            df[\"end_datetime\"]\n",
    "              .dt.tz_localize(\"Europe/Amsterdam\", ambiguous=\"infer\")\n",
    "              .dt.tz_convert(\"UTC\")\n",
    "        )\n",
    "\n",
    "        df['hour_start'] = df['start_datetime'].dt.floor(\"h\")\n",
    "        df['hour_end']   = df['end_datetime'].dt.ceil(\"h\")\n",
    "\n",
    "        agg_df = (df.groupby([\"hour_start\", \"hour_end\"])\n",
    "          .agg({\"Actual_Load\":\"sum\",\"Forecasted_Load\":\"sum\"})\n",
    "          .round(2)\n",
    "          .reset_index()\n",
    "    )\n",
    "        agg_df[\"start_datetime\"] = (\n",
    "            agg_df[\"hour_start\"]\n",
    "            .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "        )\n",
    "        agg_df[\"end_datetime\"] = (\n",
    "            agg_df[\"hour_end\"]  \n",
    "            .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "        )\n",
    "\n",
    "        agg_df[\"Timestamp\"] = agg_df[\"start_datetime\"].dt.strftime(\"%d/%m/%Y %H:%M:%S\") + \" - \" + agg_df[\"end_datetime\"].dt.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        agg_df.loc[agg_df['Timestamp'] == '26/03/2023 01:00:00 - 26/03/2023 03:00:00', 'Timestamp'] = '26/03/2023 01:00:00 - 26/03/2023 02:00:00'\n",
    "        agg_df.loc[agg_df['Timestamp'] == '31/03/2024 01:00:00 - 31/03/2024 03:00:00', 'Timestamp'] = '31/03/2024 01:00:00 - 31/03/2024 02:00:00'\n",
    "        agg_df.loc[agg_df['Timestamp'] == '30/03/2025 01:00:00 - 30/03/2025 03:00:00', 'Timestamp'] = '30/03/2025 01:00:00 - 30/03/2025 02:00:00'\n",
    "\n",
    "        agg_df.loc[agg_df['Timestamp'] == '27/10/2024 02:00:00 - 27/10/2024 02:00:00', 'Timestamp'] = '27/10/2024 02:00:00 - 27/10/2024 03:00:00'\n",
    "        agg_df.loc[agg_df['Timestamp'] == '29/10/2023 02:00:00 - 29/10/2023 02:00:00', 'Timestamp'] = '29/10/2023 02:00:00 - 29/10/2023 03:00:00'\n",
    "\n",
    "        \n",
    "        new_timestamps = [\n",
    "        ('26/03/2023 01:00:00 - 26/03/2023 02:00:00', '26/03/2023 02:00:00 - 26/03/2023 03:00:00'),\n",
    "         ('31/03/2024 01:00:00 - 31/03/2024 02:00:00', '31/03/2024 02:00:00 - 31/03/2024 03:00:00'),\n",
    "        ('30/03/2025 01:00:00 - 30/03/2025 02:00:00', '30/03/2025 02:00:00 - 30/03/2025 03:00:00'),\n",
    "        ]\n",
    "        new_rows = []\n",
    "        for old_ts, new_ts in new_timestamps:\n",
    "            match = agg_df[agg_df['Timestamp'] == old_ts]\n",
    "            if not match.empty:\n",
    "                new_row = match.copy()\n",
    "                new_row['Timestamp'] = new_ts\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "        if new_rows:\n",
    "            agg_df = pd.concat([agg_df] + new_rows, ignore_index=True)\n",
    "\n",
    "        agg_df = agg_df.groupby(\"Timestamp\", as_index=False).agg({\n",
    "            \"Actual_Load\": \"mean\",\n",
    "            \"Forecasted_Load\": \"mean\"\n",
    "            })\n",
    "       \n",
    "        agg_df[[\"start\", \"end\"]] = agg_df[\"Timestamp\"].str.split(\" - \", expand=True)\n",
    "\n",
    "        agg_df[\"start_datetime\"] = pd.to_datetime(agg_df[\"start\"], format=\"%d/%m/%Y  %H:%M:%S\")\n",
    "        agg_df[\"end_datetime\"] = pd.to_datetime(agg_df[\"end\"], format=\"%d/%m/%Y  %H:%M:%S\")\n",
    "\n",
    "    \n",
    "        agg_df = agg_df.sort_values(by='start_datetime')\n",
    "\n",
    "        return agg_df[[\n",
    "            \"start_datetime\", \"end_datetime\",\n",
    "             \"Actual_Load\", \"Forecasted_Load\"\n",
    "        ]]\n",
    "    \n",
    "    dfs = [load_and_process_excel(os.path.join(folder_path, f)) for f in loadfile_names]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "comb_prices = price_data()\n",
    "comb_load = load_data()\n",
    "comb_prices.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Combined_Prices.csv\", index=False)\n",
    "comb_load.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Combined_Load_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b55ec6",
   "metadata": {},
   "source": [
    "Use API from NED.nl to load in the Solar and Wind data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "API_KEY = ''\n",
    "BASE_URL = \"https://api.ned.nl/v1/utilizations\"\n",
    "HEADERS = {\n",
    "    'X-AUTH-TOKEN': API_KEY,\n",
    "    'accept': 'text/csv' \n",
    "}\n",
    "\n",
    "START_DATE = datetime(2025, 1, 1)\n",
    "END_DATE = datetime(2025, 5, 17)\n",
    "STEP = timedelta(days=3)  \n",
    "\n",
    "OUTPUT_PATH = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/WindOffShore_2025.csv\"\n",
    "\n",
    "header_written = False\n",
    "\n",
    "current_start = START_DATE\n",
    "while current_start < END_DATE:\n",
    "    current_end = min(current_start + STEP, END_DATE)\n",
    "\n",
    "    start_str = current_start.strftime(\"%Y-%m-%d\")\n",
    "    end_str = current_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(f\"ðŸ“¡ Fetching from {start_str} to {end_str}...\")\n",
    "\n",
    "    params = {\n",
    "        'point': 0,\n",
    "        'type': 17,\n",
    "        'granularity': 5,\n",
    "        'granularitytimezone': 1,\n",
    "        'classification': 2,\n",
    "        'activity': 1,\n",
    "        'validfrom[after]': start_str,\n",
    "        'validfrom[strictly_before]': end_str\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()\n",
    "        if not lines:\n",
    "            print(f\" No data for {start_str} to {end_str}\")\n",
    "        else:\n",
    "            with open(OUTPUT_PATH, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                reader = csv.reader(lines)\n",
    "\n",
    "                for i, row in enumerate(reader):\n",
    "                    if i == 0 and not header_written:\n",
    "                        row = [col.replace(\"validfrom\", \"validfrom (UTC)\").replace(\"validto\", \"validto (UTC)\").replace(\"volume\", \"volume (kWh)\") for col in row]\n",
    "                        writer.writerow(row)\n",
    "                        header_written = True\n",
    "                    elif i > 0:\n",
    "                        writer.writerow(row)\n",
    "    else:\n",
    "        print(f\" Failed for {start_str} to {end_str}: {response.status_code} {response.text}\")\n",
    "\n",
    "    current_start = current_end\n",
    "\n",
    "print(f\" Data retrieved and saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc95102",
   "metadata": {},
   "source": [
    "Combine 2023, 2024 and 2025 Solar and Wind data into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def load_exo(name):\n",
    "    folder_path = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data\"\n",
    "    if name == \"Solar\":\n",
    "        file_names = [\n",
    "            \"Solar_2023.csv\",\n",
    "            \"Solar_2024.csv\",\n",
    "            \"Solar_2025.csv\"\n",
    "        ]\n",
    "    elif name == \"WindShore\":\n",
    "        file_names = [\n",
    "            \"WindShore_2023.csv\",\n",
    "            \"WindShore_2024.csv\",\n",
    "            \"WindShore_2025.csv\"\n",
    "        ]\n",
    "    elif name == \"WindOffShore\":\n",
    "        file_names = [\n",
    "            \"WindOffShore_2023.csv\",\n",
    "            \"WindOffShore_2024.csv\",\n",
    "            \"WindOffShore_2025.csv\"\n",
    "        ]\n",
    "\n",
    "    def load_and_process_excel(file_path, name):\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            parse_dates=['validfrom (UTC)', 'validto (UTC)']\n",
    "        )\n",
    "\n",
    "\n",
    "        df = df.rename(columns={\n",
    "            'validfrom (UTC)': 'start_datetime',\n",
    "            'validto (UTC)'  : 'end_datetime',\n",
    "            'volume (kWh)'   : name  \n",
    "        })\n",
    "        if df[\"start_datetime\"].dt.tz is None:\n",
    "            df[\"start_datetime\"] = (\n",
    "                df[\"start_datetime\"]\n",
    "                .dt.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "        else:\n",
    "            df[\"start_datetime\"] = (\n",
    "                df[\"start_datetime\"]\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "            \n",
    "        if df[\"end_datetime\"].dt.tz is None:\n",
    "            df[\"end_datetime\"] = (\n",
    "                df[\"end_datetime\"]\n",
    "                .dt.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "        else:\n",
    "            df[\"end_datetime\"] = (\n",
    "                df[\"end_datetime\"]\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "        \n",
    "        end_of_april = pd.Timestamp(\"2025-04-30 23:00:00\")\n",
    "        df = df[df[\"start_datetime\"] <= end_of_april]\n",
    "\n",
    "        df[\"Timestamp\"] = df[\"start_datetime\"].dt.strftime(\"%d/%m/%Y %H:%M:%S\") + \" - \" + df[\"end_datetime\"].dt.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        df.loc[df['Timestamp'] == '26/03/2023 01:00:00 - 26/03/2023 03:00:00', 'Timestamp'] = '26/03/2023 01:00:00 - 26/03/2023 02:00:00'\n",
    "        df.loc[df['Timestamp'] == '31/03/2024 01:00:00 - 31/03/2024 03:00:00', 'Timestamp'] = '31/03/2024 01:00:00 - 31/03/2024 02:00:00'\n",
    "        df.loc[df['Timestamp'] == '30/03/2025 01:00:00 - 30/03/2025 03:00:00', 'Timestamp'] = '30/03/2025 01:00:00 - 30/03/2025 02:00:00'\n",
    "\n",
    "        df.loc[df['Timestamp'] == '27/10/2024 02:00:00 - 27/10/2024 02:00:00', 'Timestamp'] = '27/10/2024 02:00:00 - 27/10/2024 03:00:00'\n",
    "        df.loc[df['Timestamp'] == '29/10/2023 02:00:00 - 29/10/2023 02:00:00', 'Timestamp'] = '29/10/2023 02:00:00 - 29/10/2023 03:00:00'\n",
    "\n",
    "        \n",
    "        new_timestamps = [\n",
    "        ('26/03/2023 01:00:00 - 26/03/2023 02:00:00', '26/03/2023 02:00:00 - 26/03/2023 03:00:00'),\n",
    "         ('31/03/2024 01:00:00 - 31/03/2024 02:00:00', '31/03/2024 02:00:00 - 31/03/2024 03:00:00'),\n",
    "        ('30/03/2025 01:00:00 - 30/03/2025 02:00:00', '30/03/2025 02:00:00 - 30/03/2025 03:00:00'),\n",
    "        ]\n",
    "\n",
    "        new_rows = []\n",
    "        for old_ts, new_ts in new_timestamps:\n",
    "            match = df[df['Timestamp'] == old_ts]\n",
    "            if not match.empty:\n",
    "                new_row = match.copy()\n",
    "                new_row['Timestamp'] = new_ts\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "        if new_rows:\n",
    "            df = pd.concat([df] + new_rows, ignore_index=True)\n",
    "\n",
    "        df = df.groupby(\"Timestamp\", as_index=False).agg({\n",
    "            name: \"mean\"\n",
    "            })\n",
    "       \n",
    "        df[[\"start\", \"end\"]] = df[\"Timestamp\"].str.split(\" - \", expand=True)\n",
    "\n",
    "        df[\"start_datetime\"] = pd.to_datetime(df[\"start\"], format=\"%d/%m/%Y  %H:%M:%S\")\n",
    "        df[\"end_datetime\"] = pd.to_datetime(df[\"end\"], format=\"%d/%m/%Y  %H:%M:%S\")\n",
    "\n",
    "        df = df.sort_values(by='start_datetime')\n",
    "\n",
    "        return df[['start_datetime', 'end_datetime', name]]\n",
    "\n",
    "    dfs = [load_and_process_excel(os.path.join(folder_path, f), name) for f in file_names]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "Solar = load_exo(\"Solar\")\n",
    "Solar.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Combined_Solar.csv\", index=False)\n",
    "\n",
    "WindShore = load_exo(\"WindShore\")  \n",
    "WindShore.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Combined_WindShore.csv\", index=False)  \n",
    "\n",
    "WindOffShore = load_exo(\"WindOffShore\")\n",
    "WindOffShore.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Combined_WindOffShore.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ad633",
   "metadata": {},
   "source": [
    "Combine the Price, Load, Solar and Wind datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da951ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data\"\n",
    "files = {\n",
    "    \"Prices\": \"Combined_Prices.csv\",\n",
    "    \"Load\": \"Combined_Load_new.csv\",\n",
    "    \"Solar\": \"Combined_Solar.csv\",\n",
    "    \"WindShore\": \"Combined_WindShore.csv\",\n",
    "    \"WindOffShore\": \"Combined_WindOffShore.csv\"\n",
    "}\n",
    "\n",
    "def load_csv(file_name):\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(folder_path, file_name),\n",
    "        parse_dates=['start_datetime', 'end_datetime']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_prices = load_csv(files[\"Prices\"])\n",
    "df_load = load_csv(files[\"Load\"])\n",
    "df_solar = load_csv(files[\"Solar\"])\n",
    "df_wind_shore = load_csv(files[\"WindShore\"])\n",
    "df_wind_offshore = load_csv(files[\"WindOffShore\"])\n",
    "\n",
    "data = df_prices \\\n",
    "    .merge(df_load, on=['start_datetime', 'end_datetime'], how='outer') \\\n",
    "    .merge(df_solar, on=['start_datetime', 'end_datetime'], how='outer') \\\n",
    "    .merge(df_wind_shore, on=['start_datetime', 'end_datetime'], how='outer') \\\n",
    "    .merge(df_wind_offshore, on=['start_datetime', 'end_datetime'], how='outer')\n",
    "\n",
    "\n",
    "data = data.sort_values(by='start_datetime').reset_index(drop=True)\n",
    "data['Solar'] = data['Solar']/1000\n",
    "data['WindShore'] = data['WindShore']/1000\n",
    "data['WindOffShore'] = data['WindOffShore']/1000\n",
    "data['Load'] = data['Actual_Load'] /4\n",
    "data = data.drop(\"Actual_Load\", axis='columns')\n",
    "data = data.drop(\"Forecasted_Load\", axis='columns')\n",
    "data.to_csv(os.path.join(folder_path, \"All_Combined_new.csv\"), index=False)\n",
    "\n",
    "print(\" Merged dataset saved as Combined_All_Features_new.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07b907",
   "metadata": {},
   "source": [
    "Add Import and Export to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Import_export_2023 = pd.read_csv(\n",
    "    \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Import_export_2023.csv\",\n",
    "    sep=';',\n",
    "    parse_dates=[\"start_datetime\", \"end_datetime\"], dayfirst=True\n",
    ")\n",
    "\n",
    "Import_export_2024 = pd.read_csv(\n",
    "    \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Import_export_2024.csv\",\n",
    "    sep=';',\n",
    "    parse_dates=[\"start_datetime\", \"end_datetime\"], dayfirst=True\n",
    ")\n",
    "\n",
    "Import_export_2025 = pd.read_csv(\n",
    "    \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Import_export_2025.csv\",\n",
    "    sep=';',\n",
    "    parse_dates=[\"start_datetime\", \"end_datetime\"], dayfirst=True\n",
    ")\n",
    "\n",
    "Import_export = pd.concat([Import_export_2023, Import_export_2024, Import_export_2025])\n",
    "Import_export = Import_export.replace(\"n/e\", pd.NA).dropna()\n",
    "Import_export['Physical Flow (MW)'] = pd.to_numeric(Import_export['Physical Flow (MW)'], errors='coerce')\n",
    "Import_export = Import_export.dropna()\n",
    "\n",
    "\n",
    "Export = Import_export.groupby(['start_datetime', 'Out Area']).agg({\n",
    "    'Physical Flow (MW)': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "Import = Import_export.groupby(['start_datetime', 'In Area']).agg({\n",
    "    'Physical Flow (MW)': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "Export = Export[Export['Out Area'] == 'NL'].drop(columns='Out Area')\n",
    "Import = Import[Import['In Area'] == 'NL'].drop(columns= 'In Area')\n",
    "Export.columns = ['start_datetime', 'Export']\n",
    "Import.columns = ['start_datetime', 'Import']\n",
    "Import_export = Export.merge(Import, how='outer', on = 'start_datetime')\n",
    "Import_export['Net_Import'] = (Import_export['Import'] - Import_export['Export']).round(2)\n",
    "Import_export = Import_export.drop(['Import', 'Export'], axis='columns')\n",
    "\n",
    "df = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv', parse_dates=['start_datetime', 'end_datetime'])\n",
    "df_added = df.merge(Import_export, how='left', on='start_datetime')\n",
    "df_added = df_added.fillna(0)\n",
    "\n",
    "df_added.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c734e89",
   "metadata": {},
   "source": [
    "Add EUA, API2 Coal and TTF Gas Prices to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc60459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EUA_price = pd.read_excel(\n",
    "    \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/EUA_price.xlsx\", parse_dates=[\"Exchange Date\"]\n",
    ")\n",
    "\n",
    "Coal_price = pd.read_csv(\n",
    "    \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Coal_price.csv\",\n",
    "    parse_dates=[\"Date\"], dayfirst=False\n",
    ")\n",
    "\n",
    "Gas_price = pd.read_csv(\n",
    "    \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/Gas_price.csv\",\n",
    "    parse_dates=[\"Date\"], dayfirst=False\n",
    ")\n",
    "\n",
    "EUA_price['Date'] = EUA_price['Exchange Date'].dt.date\n",
    "Coal_price['Date'] = Coal_price['Date'].dt.date\n",
    "Gas_price['Date'] = Gas_price['Date'].dt.date\n",
    "\n",
    "EUA_price['EUA_Price'] = EUA_price['Trade Price']\n",
    "Coal_price['Coal_Price'] = (Coal_price['Price'] * 0.86).round(2)\n",
    "Gas_price['Gas_Price'] = Gas_price['Price']\n",
    "\n",
    "Coal_price = Coal_price.drop(['Open', 'High', 'Low', 'Vol.', 'Change %', 'Price'], axis = 'columns')\n",
    "Gas_price = Gas_price.drop(['Open', 'High', 'Low', 'Vol.', 'Change %', 'Price'], axis = 'columns')\n",
    "EUA_price = EUA_price.drop(['Exchange Date', 'Trade Price'], axis='columns')\n",
    "\n",
    "\n",
    "all_combined_new = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv', parse_dates=['start_datetime', 'end_datetime'])\n",
    "all_combined_new['Date'] = all_combined_new['start_datetime'].dt.date\n",
    "\n",
    "all_combined_new = all_combined_new.merge(EUA_price, how='left', on='Date')\n",
    "all_combined_new = all_combined_new.merge(Coal_price, how='left', on='Date')\n",
    "all_combined_new = all_combined_new.merge(Gas_price, how='left', on='Date')\n",
    "\n",
    "all_combined_new = all_combined_new.fillna(method='ffill')\n",
    "all_combined_new = all_combined_new.fillna(method='bfill')\n",
    "all_combined_new = all_combined_new.drop('Date', axis='columns')\n",
    "\n",
    "all_combined_new.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f2a49",
   "metadata": {},
   "source": [
    "Determine which features are important for price forecasting through correlation, VIF and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33565149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_combined_new = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv', parse_dates=['start_datetime', 'end_datetime'])\n",
    "all_combined_new.dropna(inplace=True)\n",
    "\n",
    "train_df = all_combined_new[(all_combined_new['start_datetime'].dt.year == 2023)]\n",
    "for lag in [72, 168]:\n",
    "    train_df[f'Price_lag_{lag}'] = train_df['Price'].shift(lag)\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "feature_cols = ['Solar', 'WindShore', 'WindOffShore','Load', 'Net_Import','EUA_Price', 'Coal_Price', 'Gas_Price', 'Price_lag_72', 'Price_lag_168']\n",
    "\n",
    "correlation_matrix = train_df.corr(numeric_only=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.savefig(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Output/Plot/correlation_matrix.png\", format=\"png\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X_train = train_df[feature_cols] \n",
    "y_train = train_df['Price']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "lasso = LassoCV(cv=10).fit(X_train, y_train)\n",
    "print(lasso)\n",
    "\n",
    "selected = [feature for coef, feature in zip(lasso.coef_, feature_cols) if coef != 0]\n",
    "print(\"Selected features:\", selected)\n",
    "\n",
    "coef = pd.Series(lasso.coef_, index=X_train.columns)\n",
    "intercept = lasso.intercept_\n",
    "alpha = lasso.alpha_\n",
    "\n",
    "print(\"\\nLasso Regression Results\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Chosen alpha (regularization strength): {alpha:.6f}\")\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coef)\n",
    "\n",
    "\n",
    "X = add_constant(train_df[feature_cols])\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"Feature\"] = X.columns\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_df)\n",
    "\n",
    "newcolumns = ['Solar', 'WindShore', 'WindOffShore','Load', 'Net_Import', 'Coal_Price','Price_lag_72', 'Price_lag_168']\n",
    "X = add_constant(train_df[newcolumns])\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"Feature\"] = X.columns\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(vif_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179d14a",
   "metadata": {},
   "source": [
    "Generate summary statistics for the chosen variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/All_Combined_new.csv', parse_dates=['start_datetime', 'end_datetime'])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "train_df = df[(df['start_datetime'].dt.year == 2023)]\n",
    "val_df = df[((df['start_datetime'].dt.year == 2024)& (df['start_datetime'].dt.month <= 6))]\n",
    "test_df  = df[((df['start_datetime'].dt.year == 2024) & (df['start_datetime'].dt.month >= 7)) | (df['start_datetime'].dt.year == 2025)]\n",
    "feature_cols = ['Load', 'Solar', 'WindShore', 'WindOffShore', 'Net_Import', 'Coal_Price']\n",
    "\n",
    "\n",
    "summary_stats_train = train_df.describe()\n",
    "print(summary_stats_train)\n",
    "\n",
    "summary_stats_val = val_df.describe()\n",
    "print(summary_stats_val)\n",
    "\n",
    "summary_start_test = test_df.describe()\n",
    "print(summary_start_test)\n",
    "\n",
    "summary_stats_all = df.describe()\n",
    "print(summary_stats_all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b223b",
   "metadata": {},
   "source": [
    "Use API from Ned.nl to get the Electricity prices for 2025 to determine the noise to add to our variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "API_KEY = ''\n",
    "BASE_URL = \"https://api.ned.nl/v1/utilizations\"\n",
    "HEADERS = {\n",
    "    'X-AUTH-TOKEN': API_KEY,\n",
    "    'accept': 'text/csv' \n",
    "}\n",
    "\n",
    "START_DATE = datetime(2025, 3, 1)\n",
    "END_DATE = datetime(2025, 6, 30)\n",
    "STEP = timedelta(days=3) \n",
    "\n",
    "\n",
    "OUTPUT_PATH = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/SolarMar_Jun.csv\"\n",
    "header_written = False\n",
    "\n",
    "current_start = START_DATE\n",
    "while current_start < END_DATE:\n",
    "    current_end = min(current_start + STEP, END_DATE)\n",
    "\n",
    "    start_str = current_start.strftime(\"%Y-%m-%d\")\n",
    "    end_str = current_end.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(f\"ðŸ“¡ Fetching from {start_str} to {end_str}...\")\n",
    "\n",
    "    params = {\n",
    "        'point': 0,\n",
    "        'type': 2,\n",
    "        'granularity': 5,\n",
    "        'granularitytimezone': 1,\n",
    "        'classification': 2,\n",
    "        'activity': 1,\n",
    "        'validfrom[after]': start_str,\n",
    "        'validfrom[strictly_before]': end_str\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        lines = response.text.splitlines()\n",
    "        if not lines:\n",
    "            print(f\" No data for {start_str} to {end_str}\")\n",
    "        else:\n",
    "            with open(OUTPUT_PATH, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                reader = csv.reader(lines)\n",
    "\n",
    "                for i, row in enumerate(reader):\n",
    "                    if i == 0 and not header_written:\n",
    "                        row = [col.replace(\"validfrom\", \"validfrom (UTC)\").replace(\"validto\", \"validto (UTC)\").replace(\"volume\", \"volume (kWh)\") for col in row]\n",
    "                        writer.writerow(row)\n",
    "                        header_written = True\n",
    "                    elif i > 0:\n",
    "                        writer.writerow(row)\n",
    "    else:\n",
    "        print(f\"Failed for {start_str} to {end_str}: {response.status_code} {response.text}\")\n",
    "\n",
    "    current_start = current_end\n",
    "\n",
    "print(f\" Data retrieved and saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cfb22",
   "metadata": {},
   "source": [
    "Determining forecast error for solar and wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def load_exo(name):\n",
    "    folder_path = \"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar\"\n",
    "    if name == \"Solar\":\n",
    "        file_names = [os.path.join(\"Zon\", f\"Zon_ ({i}).csv\") for i in range(1, 19)]\n",
    "        data_name = \"SolarMar_Jun.csv\"\n",
    "\n",
    "    elif name == \"WindShore\":\n",
    "        file_names = [os.path.join(\"Wind\", f\"Wind_ ({i}).csv\") for i in range(1, 19)]\n",
    "        data_name = \"WindMar_Jun.csv\"\n",
    "\n",
    "    elif name == \"WindOffShore\":\n",
    "        file_names = [os.path.join(\"Zeewind\", f\"Zeewind_ ({i}).csv\") for i in range(1, 19)]\n",
    "        data_name = \"WindOffMar_Jun.csv\"\n",
    "\n",
    "    data = pd.read_csv(\n",
    "        os.path.join(folder_path, data_name), sep=',',\n",
    "        parse_dates=['validfrom (UTC)', 'validto (UTC)']\n",
    "        )\n",
    "    \n",
    "    data = data.rename(columns={\n",
    "            'validfrom (UTC)': 'start_datetime',\n",
    "            'validto (UTC)'  : 'end_datetime',\n",
    "            'volume (kWh)'   : f'{name}_actual'  \n",
    "        })\n",
    "    \n",
    "    if data[\"start_datetime\"].dt.tz is None:\n",
    "        data[\"start_datetime\"] = (\n",
    "                data[\"start_datetime\"]\n",
    "                .dt.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "    else:\n",
    "        data[\"start_datetime\"] = (\n",
    "                data[\"start_datetime\"]\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "                \n",
    "        \n",
    "    if data[\"end_datetime\"].dt.tz is None:\n",
    "            data[\"end_datetime\"] = (\n",
    "                data[\"end_datetime\"]\n",
    "                .dt.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "    else:\n",
    "            data[\"end_datetime\"] = (\n",
    "                data[\"end_datetime\"]\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "    \n",
    "    data = data[['start_datetime', 'end_datetime', f'{name}_actual']]\n",
    "\n",
    "    def load_and_process_excel(file_path, name, data):\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            parse_dates=['validfrom (UTC)', 'validto (UTC)']\n",
    "        )\n",
    "\n",
    "        df = df.rename(columns={\n",
    "            'validfrom (UTC)': 'start_datetime',\n",
    "            'validto (UTC)'  : 'end_datetime',\n",
    "            'volume (kWh)'   : name   \n",
    "        })\n",
    "    \n",
    "        if df[\"start_datetime\"].dt.tz is None:\n",
    "            df[\"start_datetime\"] = (\n",
    "                df[\"start_datetime\"]\n",
    "                .dt.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "        else:\n",
    "            df[\"start_datetime\"] = (\n",
    "                df[\"start_datetime\"]\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "            \n",
    "        if df[\"end_datetime\"].dt.tz is None:\n",
    "            df[\"end_datetime\"] = (\n",
    "                df[\"end_datetime\"]\n",
    "                .dt.tz_localize(\"UTC\", ambiguous=\"infer\")\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "        else:\n",
    "            df[\"end_datetime\"] = (\n",
    "                df[\"end_datetime\"]\n",
    "                .dt.tz_convert(\"Europe/Amsterdam\")\n",
    "                .dt.tz_localize(None))\n",
    "        \n",
    "        df = df[['start_datetime', 'end_datetime', name]]\n",
    "        \n",
    "        full = df.merge(data, on = [\"start_datetime\", \"end_datetime\"], how = 'left', validate='one_to_one', sort=True)\n",
    "        difference = full[name] - full[f'{name}_actual']\n",
    "\n",
    "        return difference, full\n",
    "\n",
    "    result = [load_and_process_excel(os.path.join(folder_path, f), name, data) for f in file_names]\n",
    "    diffs, full = zip(*result)\n",
    "    diffs_df = pd.concat(diffs, axis=1)\n",
    "    full_df = pd.concat(full, axis=1)\n",
    "    actuals_list = [f[[f\"{name}_actual\"]] for f in full]\n",
    "    actuals_df   = pd.concat(actuals_list, axis=1) \n",
    "    pct_error_df = diffs_df.div(actuals_df.values)  \n",
    "    \n",
    "    return pct_error_df\n",
    "\n",
    "def trimmed_row_mean_one_each(row):\n",
    "    values = row.dropna().values\n",
    "    values = values[np.isfinite(values)]\n",
    "\n",
    "    if len(values) <= 4:\n",
    "        return np.nan\n",
    "\n",
    "    sorted_values = np.sort(values)\n",
    "    trimmed = sorted_values[2:-2]\n",
    "\n",
    "    return trimmed.mean() if len(trimmed) > 0 else np.nan\n",
    "\n",
    "def trimmed_row_mean(df):\n",
    "    return df.apply(trimmed_row_mean_one_each, axis=1)\n",
    "    \n",
    "def every_24_rows(s):\n",
    "    return s.groupby(s.index // 24).mean(), s.groupby(s.index // 24).std(ddof=1)\n",
    "\n",
    "\n",
    "Solar_dif = load_exo(\"Solar\")\n",
    "Solar_mu, Solar_Sigma = every_24_rows(trimmed_row_mean(Solar_dif))\n",
    "Solar_diff = pd.concat([Solar_mu, Solar_Sigma], axis=1)\n",
    "Solar_diff.columns = ['Solar_mu', 'Solar_sigma']\n",
    "Solar_diff.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Solar_error.csv\", index=False)\n",
    "\n",
    "WindShore = load_exo(\"WindShore\")  \n",
    "Windshore_mu, Winshore_Sigma= every_24_rows(trimmed_row_mean(WindShore))\n",
    "WindShore_diff = pd.concat([Windshore_mu, Winshore_Sigma], axis=1)\n",
    "WindShore_diff.columns = ['WindShore_mu', 'WindShore_sigma']\n",
    "WindShore_diff.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindShore_error.csv\", index=False)\n",
    "\n",
    "WindOffShore = load_exo(\"WindOffShore\")\n",
    "WindOffshore_mu, WindOffShore_Sigma = every_24_rows(trimmed_row_mean(WindOffShore))\n",
    "WindOffShore_diff = pd.concat([WindOffshore_mu, WindOffShore_Sigma], axis=1)\n",
    "WindOffShore_diff.columns = ['WindOffShore_mu', 'WindOffShore_sigma']\n",
    "WindOffShore_diff.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/WindOffShore_error.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57afbc",
   "metadata": {},
   "source": [
    "Determining forecast error for load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35daf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Load_Forecast_2023.csv\", sep=';', parse_dates=['start_datetime', 'end_datetime'])\n",
    "\n",
    "\n",
    "df['start_datetime'] = pd.to_datetime(\n",
    "    df['start_datetime'],\n",
    "    dayfirst=True,\n",
    "    errors='raise'   \n",
    ")\n",
    "df['end_datetime'] = pd.to_datetime(\n",
    "    df['end_datetime'],\n",
    "    dayfirst=True,\n",
    "    errors='raise'\n",
    ")\n",
    "\n",
    "df['start_datetime'] = df['start_datetime'].dt.floor('H')\n",
    "\n",
    "hourly = df.groupby('start_datetime').agg({\n",
    "    'Actual'   : 'sum',   \n",
    "    'Forecast' : 'sum',  \n",
    "}).reset_index()\n",
    "\n",
    "hourly['Actual'] = hourly['Actual']/4\n",
    "hourly['Forecast'] = hourly['Forecast']/4\n",
    "hourly['end_datetime'] = hourly['start_datetime'] + pd.Timedelta(hours=1)\n",
    "hourly['difference'] = hourly['Actual'] - hourly['Forecast']\n",
    "hourly['pct_error'] = hourly['difference'] / hourly['Actual'].replace(0, pd.NA)  \n",
    "mean_error = hourly['pct_error'].mean()\n",
    "std_error = hourly['pct_error'].std()\n",
    "load_error = pd.Series(\n",
    "    {'load_mu':    mean_error,\n",
    "     'load_sigma': std_error}\n",
    ")\n",
    "load_error.to_csv(\"C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data_wind_solar/Load_error.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c0986",
   "metadata": {},
   "source": [
    "Generate the exact dates in the train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff316b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "val_dates = pd.date_range(start='2024-01-01', end='2024-06-30', freq='D')\n",
    "test_dates = pd.date_range(start='2024-07-01', end='2025-04-30', freq='D')\n",
    "\n",
    "train_dates_df = pd.DataFrame(train_dates, columns=['date'])\n",
    "val_dates_df = pd.DataFrame(val_dates, columns=['date'])\n",
    "test_dates_df = pd.DataFrame(test_dates, columns=['date'])\n",
    "\n",
    "train_dates_final = train_dates_df.iloc[7:-7]\n",
    "val_dates_final = val_dates_df.iloc[7:-7]\n",
    "test_dates_final = test_dates_df.iloc[7:-7]\n",
    "\n",
    "train_dates_final.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/train_dates.csv', index=False)\n",
    "val_dates_final.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/val_dates.csv', index=False)        \n",
    "test_dates_final.to_csv('C:/Users/daniq/iCloudDrive/Erasmus University Rotterdam/Master/Thesis/Data/test_dates.csv', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
